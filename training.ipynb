{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_working_directory = \"C:\\\\Users\\\\ianja\\\\REPOS\\\\Pepperpot\\\\\"\n",
    "os.chdir(new_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGMENT A DIR BY PARAM VARIATION\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "class ParameterValues:\n",
    "    def __init__(self, epsnx, alfax, betax, epsny, alfay, betay, epsnz, alfaz, betaz):\n",
    "        self.epsnx = epsnx\n",
    "        self.alfax = alfax\n",
    "        self.betax = betax\n",
    "        self.epsny = epsny\n",
    "        self.alfay = alfay\n",
    "        self.betay = betay\n",
    "        self.epsnz = epsnz\n",
    "        self.alfaz = alfaz\n",
    "        self.betaz = betaz\n",
    "\n",
    "# Specify the local directory containing the images\n",
    "local_directory = 'local-images-before-after-propagation-bw-shorter'\n",
    "\n",
    "# Check the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if the local_directory exists in the current working directory\n",
    "if not os.path.exists(local_directory):\n",
    "    print(f\"The directory '{local_directory}' does not exist in the current working directory.\")\n",
    "else:\n",
    "    print(f\"The directory '{local_directory}' exists.\")\n",
    "\n",
    "# Create the target directories for organizing the images\n",
    "target_directory_2param = os.path.join(local_directory, '2-param')\n",
    "target_directory_3param = os.path.join(local_directory, '3-param')\n",
    "target_directory_6param = os.path.join(local_directory, '6-param')\n",
    "\n",
    "# Create the target directories if they don't exist\n",
    "os.makedirs(target_directory_2param, exist_ok=True)\n",
    "os.makedirs(target_directory_3param, exist_ok=True)\n",
    "os.makedirs(target_directory_6param, exist_ok=True)\n",
    "\n",
    "# Regular expression pattern for extracting parameters from filename\n",
    "pattern = r\"epsnx([\\d.-]+)_alfax([\\d.-]+)_betax([\\d.-]+)_epsny([\\d.-]+)_alfay([\\d.-]+)_betay([\\d.-]+)_epsnz([\\d.-]+)_alfaz([\\d.-]+)_betaz([\\d.-]+)_*[\\d]*\\.png\"\n",
    "\n",
    "# Iterate over the images in the local directory\n",
    "for filename in os.listdir(local_directory):\n",
    "    if filename.endswith('.png'):\n",
    "        source_path = os.path.join(local_directory, filename)\n",
    "\n",
    "        # Extract parameters from filename using regex\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            params = ParameterValues(*match.groups())\n",
    "            varied_params = sum(param != '0.0' for param in vars(params).values())\n",
    "\n",
    "            # Move the file to the corresponding target directory based on the number of varied parameters\n",
    "            if filename.startswith(\"epsnx0.1_\"):\n",
    "                target_path = os.path.join(target_directory_2param, filename)\n",
    "            elif params.alfay == \"-0.55\" and params.betay == \"170.0\":\n",
    "                target_path = os.path.join(target_directory_3param, filename)\n",
    "            else:\n",
    "                target_path = os.path.join(target_directory_6param, filename)\n",
    "\n",
    "            shutil.move(source_path, target_path)\n",
    "\n",
    "print(\"Image organization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST/TRAIN SPLIT\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "original_folders = [\"local-images-50cm-propagation-bw-200bin-cropped/2-param/\", \"local-images-50cm-propagation-bw-200bin-cropped/3-param/\", \"local-images-10cm-propagation-bw-200bin-cropped\"]\n",
    "train_folder = \"./train\"\n",
    "test_folder = \"./test\"\n",
    "\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for folder in original_folders:\n",
    "    file_list.extend([os.path.join(folder, file) for file in os.listdir(folder)])\n",
    "    print(len(file_list))\n",
    "\n",
    "random.shuffle(file_list)\n",
    "\n",
    "train_size = int(len(file_list) * train_ratio)\n",
    "\n",
    "train_files = file_list[:train_size]\n",
    "test_files = file_list[train_size:]\n",
    "\n",
    "for src_path in train_files:\n",
    "    file = os.path.basename(src_path)\n",
    "    dst_path = os.path.join(train_folder, file)\n",
    "\n",
    "    # Check if a file with the same name already exists in the target directory\n",
    "    if os.path.exists(dst_path):\n",
    "        # Split filename into name and extension\n",
    "        base, ext = os.path.splitext(file)\n",
    "        # Append a unique identifier to the filename\n",
    "        new_filename = f\"{base}_{random.randint(1000,9999)}{ext}\"\n",
    "        dst_path = os.path.join(train_folder, new_filename)\n",
    "\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "for src_path in test_files:\n",
    "    file = os.path.basename(src_path)\n",
    "    dst_path = os.path.join(test_folder, file)\n",
    "\n",
    "    # Check if a file with the same name already exists in the target directory\n",
    "    if os.path.exists(dst_path):\n",
    "        # Split filename into name and extension\n",
    "        base, ext = os.path.splitext(file)\n",
    "        # Append a unique identifier to the filename\n",
    "        new_filename = f\"{base}_{random.randint(1000,9999)}{ext}\"\n",
    "        dst_path = os.path.join(test_folder, new_filename)\n",
    "\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Dataset split completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# force clear cache and memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "# ensure determinism\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_PARAM SIMPLE MODEL\n",
    "NAME = \"3param\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 147 * 147, 256)  # Adjust this if needed\n",
    "        self.fc2 = nn.Linear(256, 3)  # Now predicting three variables\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.pattern = r\"epsnx([\\d.-]+)_alfax([\\d.-]+)_betax([\\d.-]+)_epsny([\\d.-]+)_alfay([\\d.-]+)_betay([\\d.-]+)_epsnz([\\d.-]+)_alfaz([\\d.-]+)_betaz([\\d.-]+)\\.png\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))  # Normalizing single channel\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_files[index]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (294, 294))\n",
    "        image = self.transform(image)\n",
    "        matches = re.search(self.pattern, image_name)\n",
    "        if matches is None:\n",
    "            print(f\"No match found for file: {image_name}\")\n",
    "            return None\n",
    "        variables = [float(matches.group(i)) for i in range(1, 10) if matches.group(i)]\n",
    "        epsnx = variables[0]  # epsnx value\n",
    "        alfa_x = variables[1]  # alfax value\n",
    "        beta_x = variables[2]  # betax value\n",
    "        return image, torch.tensor([epsnx, alfa_x, beta_x])  # Returns epsnx, alfa x and betax as the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "root_dir = \"path_to_root_dir\"\n",
    "dataset = CustomDataset(root_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "saved_models = deque(maxlen=5)\n",
    "saved_losses = deque(maxlen=5)\n",
    "\n",
    "total_steps = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    if len(saved_models) < 5 or loss.item() < max(saved_losses):\n",
    "        if len(saved_models) == 5:\n",
    "            worst_model = saved_models[saved_losses.index(max(saved_losses))]\n",
    "            os.remove(worst_model)\n",
    "            saved_models.remove(worst_model)\n",
    "            saved_losses.remove(max(saved_losses))\n",
    "\n",
    "        filename = f\"cnn_{NAME}_bs{batch_size}_lr{learning_rate}_e{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        saved_models.append(filename)\n",
    "        saved_losses.append(loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, num_variables):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    total_mae = torch.zeros(num_variables, device=device)\n",
    "    total_mape = torch.zeros(num_variables, device=device)\n",
    "    total_smape = torch.zeros(num_variables, device=device)\n",
    "    total_mse = torch.zeros(num_variables, device=device)\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            absolute_error = torch.abs(outputs - labels)\n",
    "            total_mae += absolute_error.sum(dim=0)\n",
    "\n",
    "            non_zero_mask = torch.abs(labels) > 1e-8\n",
    "            percentage_error = (absolute_error / torch.abs(labels)) * 100\n",
    "            total_mape += (percentage_error * non_zero_mask).sum(dim=0)\n",
    "\n",
    "            smape = 200.0 * torch.abs(outputs - labels) / (torch.abs(outputs) + torch.abs(labels) + torch.finfo(torch.float32).eps)\n",
    "            total_smape += smape.sum(dim=0)\n",
    "            \n",
    "            mse = (outputs - labels) ** 2\n",
    "            total_mse += mse.sum(dim=0)\n",
    "\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    mae = total_mae / total_count\n",
    "    mape = total_mape / total_count\n",
    "    smape = total_smape / total_count\n",
    "    rmse = torch.sqrt(total_mse / total_count)\n",
    "\n",
    "    return mae.cpu().numpy(), mape.cpu().numpy(), smape.cpu().numpy(), rmse.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: .\\cnn_2param-bw-sgwd_bs4_lr0.001_e200_6752.pth, Aggregate Score: 17.2686\n",
      "Mean Absolute Error: [ 0.4459879 17.1889   ]\n",
      "Mean Absolute Percentage Error: [51.482895   7.5551586]\n",
      "Symmetric Mean Absolute Percentage Error: [28.398268   7.3972483]\n",
      "Root Mean Square Error: [ 0.6025582 25.077765 ]\n",
      "\n",
      "Model: .\\cnn_2param-bw-extraprop_bs4_lr0.001_e200_3376.pth, Aggregate Score: 80.7094\n",
      "Mean Absolute Error: [  1.4378942 131.74649  ]\n",
      "Mean Absolute Percentage Error: [94.070366 80.48501 ]\n",
      "Symmetric Mean Absolute Percentage Error: [91.236885 35.74836 ]\n",
      "Root Mean Square Error: [  2.205542 208.74432 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN MODEL EVALUATION\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "# Load models and evaluate their performance\n",
    "model_dir = './'  # Specify ysour directory where models are saved\n",
    "test_dataset = CustomDataset('test')  # Specify your test dataset\n",
    "\n",
    "model_performance = []\n",
    "\n",
    "# Load each model and evaluate it\n",
    "for model_file in glob.glob(model_dir + '/*.pth'):\n",
    "    # Load model\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluate model\n",
    "    mae, mape, smape, rmse = evaluate_model(model, test_dataset, 2)\n",
    "    aggregate_score = np.mean([mae, mape, smape, rmse])\n",
    "    model_performance.append((model_file, mae, mape, smape, rmse, aggregate_score))\n",
    "\n",
    "# Sort models based on aggregate score\n",
    "model_performance.sort(key=lambda x: x[-1])\n",
    "\n",
    "# Print the performance of each model\n",
    "for model_info in model_performance:\n",
    "    model_file, mae, mape, smape, rmse, aggregate_score = model_info\n",
    "    print(f\"Model: {model_file}, Aggregate Score: {aggregate_score:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error: {smape}\")\n",
    "    print(f\"Root Mean Square Error: {rmse}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
