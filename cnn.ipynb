{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def partition_images(base_dir, train_dir, test_dir, train_ratio):\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    all_images = os.listdir(base_dir)\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    train_count = int(len(all_images) * train_ratio)\n",
    "    train_images = all_images[:train_count]\n",
    "    test_images = all_images[train_count:]\n",
    "\n",
    "    for image in train_images:\n",
    "        shutil.copyfile(os.path.join(base_dir, image), os.path.join(train_dir, image))\n",
    "\n",
    "    for image in test_images:\n",
    "        shutil.copyfile(os.path.join(base_dir, image), os.path.join(test_dir, image))\n",
    "\n",
    "partition_images('images', 'train', 'test', 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [1/140], Loss: 154.0125\n",
      "Epoch [1/25], Step [2/140], Loss: 254.8400\n",
      "Epoch [1/25], Step [3/140], Loss: 119.3179\n",
      "Epoch [1/25], Step [4/140], Loss: 113.7042\n",
      "Epoch [1/25], Step [5/140], Loss: 84.7923\n",
      "Epoch [1/25], Step [6/140], Loss: 137.2838\n",
      "Epoch [1/25], Step [7/140], Loss: 105.3344\n",
      "Epoch [1/25], Step [8/140], Loss: 77.9037\n",
      "Epoch [1/25], Step [9/140], Loss: 78.1877\n",
      "Epoch [1/25], Step [10/140], Loss: 80.2626\n",
      "Epoch [1/25], Step [11/140], Loss: 68.7683\n",
      "Epoch [1/25], Step [12/140], Loss: 51.0361\n",
      "Epoch [1/25], Step [13/140], Loss: 38.5167\n",
      "Epoch [1/25], Step [14/140], Loss: 46.8061\n",
      "Epoch [1/25], Step [15/140], Loss: 38.3636\n",
      "Epoch [1/25], Step [16/140], Loss: 78.6200\n",
      "Epoch [1/25], Step [17/140], Loss: 98.8882\n",
      "Epoch [1/25], Step [18/140], Loss: 50.7807\n",
      "Epoch [1/25], Step [19/140], Loss: 80.1082\n",
      "Epoch [1/25], Step [20/140], Loss: 33.5948\n",
      "Epoch [1/25], Step [21/140], Loss: 45.5939\n",
      "Epoch [1/25], Step [22/140], Loss: 73.5188\n",
      "Epoch [1/25], Step [23/140], Loss: 78.4745\n",
      "Epoch [1/25], Step [24/140], Loss: 51.9606\n",
      "Epoch [1/25], Step [25/140], Loss: 45.8267\n",
      "Epoch [1/25], Step [26/140], Loss: 83.4558\n",
      "Epoch [1/25], Step [27/140], Loss: 58.2908\n",
      "Epoch [1/25], Step [28/140], Loss: 55.6764\n",
      "Epoch [1/25], Step [29/140], Loss: 29.5322\n",
      "Epoch [1/25], Step [30/140], Loss: 96.6473\n",
      "Epoch [1/25], Step [31/140], Loss: 83.7292\n",
      "Epoch [1/25], Step [32/140], Loss: 79.6267\n",
      "Epoch [1/25], Step [33/140], Loss: 64.5928\n",
      "Epoch [1/25], Step [34/140], Loss: 62.9499\n",
      "Epoch [1/25], Step [35/140], Loss: 36.5409\n",
      "Epoch [1/25], Step [36/140], Loss: 32.8496\n",
      "Epoch [1/25], Step [37/140], Loss: 39.0205\n",
      "Epoch [1/25], Step [38/140], Loss: 27.7925\n",
      "Epoch [1/25], Step [39/140], Loss: 66.7406\n",
      "Epoch [1/25], Step [40/140], Loss: 81.5067\n",
      "Epoch [1/25], Step [41/140], Loss: 45.5008\n",
      "Epoch [1/25], Step [42/140], Loss: 31.6624\n",
      "Epoch [1/25], Step [43/140], Loss: 87.2162\n",
      "Epoch [1/25], Step [44/140], Loss: 111.1958\n",
      "Epoch [1/25], Step [45/140], Loss: 63.9606\n",
      "Epoch [1/25], Step [46/140], Loss: 53.3104\n",
      "Epoch [1/25], Step [47/140], Loss: 48.1745\n",
      "Epoch [1/25], Step [48/140], Loss: 60.4727\n",
      "Epoch [1/25], Step [49/140], Loss: 75.4698\n",
      "Epoch [1/25], Step [50/140], Loss: 55.8182\n",
      "Epoch [1/25], Step [51/140], Loss: 57.2293\n",
      "Epoch [1/25], Step [52/140], Loss: 94.4834\n",
      "Epoch [1/25], Step [53/140], Loss: 60.1180\n",
      "Epoch [1/25], Step [54/140], Loss: 54.0993\n",
      "Epoch [1/25], Step [55/140], Loss: 54.7184\n",
      "Epoch [1/25], Step [56/140], Loss: 52.2162\n",
      "Epoch [1/25], Step [57/140], Loss: 40.2342\n",
      "Epoch [1/25], Step [58/140], Loss: 57.5496\n",
      "Epoch [1/25], Step [59/140], Loss: 29.9823\n",
      "Epoch [1/25], Step [60/140], Loss: 37.1297\n",
      "Epoch [1/25], Step [61/140], Loss: 35.5262\n",
      "Epoch [1/25], Step [62/140], Loss: 17.1665\n",
      "Epoch [1/25], Step [63/140], Loss: 70.4603\n",
      "Epoch [1/25], Step [64/140], Loss: 86.5166\n",
      "Epoch [1/25], Step [65/140], Loss: 39.9870\n",
      "Epoch [1/25], Step [66/140], Loss: 77.8193\n",
      "Epoch [1/25], Step [67/140], Loss: 64.9392\n",
      "Epoch [1/25], Step [68/140], Loss: 51.3705\n",
      "Epoch [1/25], Step [69/140], Loss: 64.7279\n",
      "Epoch [1/25], Step [70/140], Loss: 61.0570\n",
      "Epoch [1/25], Step [71/140], Loss: 27.5759\n",
      "Epoch [1/25], Step [72/140], Loss: 35.6174\n",
      "Epoch [1/25], Step [73/140], Loss: 45.1403\n",
      "Epoch [1/25], Step [74/140], Loss: 35.8637\n",
      "Epoch [1/25], Step [75/140], Loss: 60.6117\n",
      "Epoch [1/25], Step [76/140], Loss: 62.4396\n",
      "Epoch [1/25], Step [77/140], Loss: 45.6876\n",
      "Epoch [1/25], Step [78/140], Loss: 96.8311\n",
      "Epoch [1/25], Step [79/140], Loss: 51.0740\n",
      "Epoch [1/25], Step [80/140], Loss: 61.7946\n",
      "Epoch [1/25], Step [81/140], Loss: 48.0511\n",
      "Epoch [1/25], Step [82/140], Loss: 73.8142\n",
      "Epoch [1/25], Step [83/140], Loss: 116.0852\n",
      "Epoch [1/25], Step [84/140], Loss: 60.2225\n",
      "Epoch [1/25], Step [85/140], Loss: 27.2905\n",
      "Epoch [1/25], Step [86/140], Loss: 81.5176\n",
      "Epoch [1/25], Step [87/140], Loss: 71.8559\n",
      "Epoch [1/25], Step [88/140], Loss: 22.7934\n",
      "Epoch [1/25], Step [89/140], Loss: 82.8973\n",
      "Epoch [1/25], Step [90/140], Loss: 54.5877\n",
      "Epoch [1/25], Step [91/140], Loss: 77.1460\n",
      "Epoch [1/25], Step [92/140], Loss: 66.7906\n",
      "Epoch [1/25], Step [93/140], Loss: 47.8557\n",
      "Epoch [1/25], Step [94/140], Loss: 37.5763\n",
      "Epoch [1/25], Step [95/140], Loss: 75.3263\n",
      "Epoch [1/25], Step [96/140], Loss: 67.2001\n",
      "Epoch [1/25], Step [97/140], Loss: 45.6616\n",
      "Epoch [1/25], Step [98/140], Loss: 61.0265\n",
      "Epoch [1/25], Step [99/140], Loss: 82.8775\n",
      "Epoch [1/25], Step [100/140], Loss: 71.7222\n",
      "Epoch [1/25], Step [101/140], Loss: 76.3795\n",
      "Epoch [1/25], Step [102/140], Loss: 62.8330\n",
      "Epoch [1/25], Step [103/140], Loss: 78.6871\n",
      "Epoch [1/25], Step [104/140], Loss: 65.6464\n",
      "Epoch [1/25], Step [105/140], Loss: 90.7753\n",
      "Epoch [1/25], Step [106/140], Loss: 49.5050\n",
      "Epoch [1/25], Step [107/140], Loss: 101.6920\n",
      "Epoch [1/25], Step [108/140], Loss: 62.8169\n",
      "Epoch [1/25], Step [109/140], Loss: 61.6226\n",
      "Epoch [1/25], Step [110/140], Loss: 70.8439\n",
      "Epoch [1/25], Step [111/140], Loss: 48.5517\n",
      "Epoch [1/25], Step [112/140], Loss: 39.2014\n",
      "Epoch [1/25], Step [113/140], Loss: 56.4612\n",
      "Epoch [1/25], Step [114/140], Loss: 63.7108\n",
      "Epoch [1/25], Step [115/140], Loss: 81.5900\n",
      "Epoch [1/25], Step [116/140], Loss: 60.0445\n",
      "Epoch [1/25], Step [117/140], Loss: 82.3112\n",
      "Epoch [1/25], Step [118/140], Loss: 67.8752\n",
      "Epoch [1/25], Step [119/140], Loss: 54.2778\n",
      "Epoch [1/25], Step [120/140], Loss: 67.8851\n",
      "Epoch [1/25], Step [121/140], Loss: 36.8601\n",
      "Epoch [1/25], Step [122/140], Loss: 42.5162\n",
      "Epoch [1/25], Step [123/140], Loss: 39.7039\n",
      "Epoch [1/25], Step [124/140], Loss: 41.8856\n",
      "Epoch [1/25], Step [125/140], Loss: 94.0975\n",
      "Epoch [1/25], Step [126/140], Loss: 51.6792\n",
      "Epoch [1/25], Step [127/140], Loss: 56.7857\n",
      "Epoch [1/25], Step [128/140], Loss: 98.4884\n",
      "Epoch [1/25], Step [129/140], Loss: 37.9815\n",
      "Epoch [1/25], Step [130/140], Loss: 65.8554\n",
      "Epoch [1/25], Step [131/140], Loss: 67.5801\n",
      "Epoch [1/25], Step [132/140], Loss: 89.3301\n",
      "Epoch [1/25], Step [133/140], Loss: 90.7522\n",
      "Epoch [1/25], Step [134/140], Loss: 45.5246\n",
      "Epoch [1/25], Step [135/140], Loss: 58.0365\n",
      "Epoch [1/25], Step [136/140], Loss: 58.9831\n",
      "Epoch [1/25], Step [137/140], Loss: 73.8809\n",
      "Epoch [1/25], Step [138/140], Loss: 35.2217\n",
      "Epoch [1/25], Step [139/140], Loss: 31.7344\n",
      "Epoch [1/25], Step [140/140], Loss: 69.0922\n",
      "Epoch [2/25], Step [1/140], Loss: 52.6106\n",
      "Epoch [2/25], Step [2/140], Loss: 47.5184\n",
      "Epoch [2/25], Step [3/140], Loss: 27.9757\n",
      "Epoch [2/25], Step [4/140], Loss: 63.3658\n",
      "Epoch [2/25], Step [5/140], Loss: 97.1768\n",
      "Epoch [2/25], Step [6/140], Loss: 48.4158\n",
      "Epoch [2/25], Step [7/140], Loss: 59.4364\n",
      "Epoch [2/25], Step [8/140], Loss: 71.6218\n",
      "Epoch [2/25], Step [9/140], Loss: 67.6369\n",
      "Epoch [2/25], Step [10/140], Loss: 117.4869\n",
      "Epoch [2/25], Step [11/140], Loss: 93.6259\n",
      "Epoch [2/25], Step [12/140], Loss: 45.3789\n",
      "Epoch [2/25], Step [13/140], Loss: 43.9698\n",
      "Epoch [2/25], Step [14/140], Loss: 40.9977\n",
      "Epoch [2/25], Step [15/140], Loss: 61.6869\n",
      "Epoch [2/25], Step [16/140], Loss: 60.5414\n",
      "Epoch [2/25], Step [17/140], Loss: 34.7379\n",
      "Epoch [2/25], Step [18/140], Loss: 76.2726\n",
      "Epoch [2/25], Step [19/140], Loss: 43.4260\n",
      "Epoch [2/25], Step [20/140], Loss: 81.4015\n",
      "Epoch [2/25], Step [21/140], Loss: 72.7329\n",
      "Epoch [2/25], Step [22/140], Loss: 44.7193\n",
      "Epoch [2/25], Step [23/140], Loss: 42.6322\n",
      "Epoch [2/25], Step [24/140], Loss: 44.9141\n",
      "Epoch [2/25], Step [25/140], Loss: 62.7731\n",
      "Epoch [2/25], Step [26/140], Loss: 36.9353\n",
      "Epoch [2/25], Step [27/140], Loss: 59.2117\n",
      "Epoch [2/25], Step [28/140], Loss: 38.1643\n",
      "Epoch [2/25], Step [29/140], Loss: 72.0476\n",
      "Epoch [2/25], Step [30/140], Loss: 82.0573\n",
      "Epoch [2/25], Step [31/140], Loss: 86.0903\n",
      "Epoch [2/25], Step [32/140], Loss: 65.8943\n",
      "Epoch [2/25], Step [33/140], Loss: 61.5377\n",
      "Epoch [2/25], Step [34/140], Loss: 41.3748\n",
      "Epoch [2/25], Step [35/140], Loss: 77.5156\n",
      "Epoch [2/25], Step [36/140], Loss: 42.6669\n",
      "Epoch [2/25], Step [37/140], Loss: 58.2890\n",
      "Epoch [2/25], Step [38/140], Loss: 37.2496\n",
      "Epoch [2/25], Step [39/140], Loss: 59.4921\n",
      "Epoch [2/25], Step [40/140], Loss: 61.0785\n",
      "Epoch [2/25], Step [41/140], Loss: 55.1357\n",
      "Epoch [2/25], Step [42/140], Loss: 28.4612\n",
      "Epoch [2/25], Step [43/140], Loss: 86.5806\n",
      "Epoch [2/25], Step [44/140], Loss: 60.7636\n",
      "Epoch [2/25], Step [45/140], Loss: 50.3359\n",
      "Epoch [2/25], Step [46/140], Loss: 63.9908\n",
      "Epoch [2/25], Step [47/140], Loss: 69.5949\n",
      "Epoch [2/25], Step [48/140], Loss: 58.6771\n",
      "Epoch [2/25], Step [49/140], Loss: 81.3441\n",
      "Epoch [2/25], Step [50/140], Loss: 33.0767\n",
      "Epoch [2/25], Step [51/140], Loss: 56.2635\n",
      "Epoch [2/25], Step [52/140], Loss: 61.9539\n",
      "Epoch [2/25], Step [53/140], Loss: 50.8056\n",
      "Epoch [2/25], Step [54/140], Loss: 54.2687\n",
      "Epoch [2/25], Step [55/140], Loss: 22.5212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m total_steps \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[0;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     90\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     91\u001b[0m         labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[24], line 66\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     64\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_path)\n\u001b[0;32m     65\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 66\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     67\u001b[0m matches \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern, image_name)\n\u001b[0;32m     68\u001b[0m variables \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(matches\u001b[39m.\u001b[39mgroup(i)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m) \u001b[39mif\u001b[39;00m matches\u001b[39m.\u001b[39mgroup(i)]\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:917\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    913\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 917\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m    919\u001b[0m dtype \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    920\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "# ensure determinism\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 147 * 147, 256)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(256, 2) # predicting two variables (alfa x and betax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.pattern = r\"epsnx([\\d.-]+)_alfax([\\d.-]+)_betax([\\d.-]+)_epsny([\\d.-]+)_alfay([\\d.-]+)_betay([\\d.-]+)_epsnz([\\d.-]+)_alfaz([\\d.-]+)_betaz([\\d.-]+)\\.png\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_files[index]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "        matches = re.search(self.pattern, image_name)\n",
    "        variables = [float(matches.group(i)) for i in range(1, 10) if matches.group(i)]\n",
    "        alfa_x = variables[1] # alfax value\n",
    "        beta_x = variables[2] # betax value\n",
    "        return image, torch.tensor([alfa_x, beta_x]) # returns alfa x and betax as the labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 250\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.L1Loss() # MAE\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "image_dir = \"train\"\n",
    "dataset = CustomDataset(image_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_steps = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # if (i + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "filename = f\"cnn_model_bs{batch_size}_lr{learning_rate}_e{num_epochs}.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    }, filename)\n",
    "\n",
    "torch.save(model.state_dict(), filename)\n",
    "\n",
    "# checkpoint = torch.load(filename)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# num_epochs = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for prediction 1:\n",
      "Mean Absolute Error: 1.4102\n",
      "Mean Absolute Percentage Error: nan%\n",
      "Symmetric Mean Absolute Percentage Error: 85.29%\n",
      "Root Mean Square Error: 1.6444\n",
      "\n",
      "Metrics for prediction 2:\n",
      "Mean Absolute Error: 90.2841\n",
      "Mean Absolute Percentage Error: 37.10%\n",
      "Symmetric Mean Absolute Percentage Error: 40.21%\n",
      "Root Mean Square Error: 111.6917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_dir):\n",
    "    model.eval()\n",
    "    test_dataset = CustomDataset(test_dir)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    total_mae = torch.zeros(2, device=device)   # Added device argument\n",
    "    total_mape = torch.zeros(2, device=device)  # Added device argument\n",
    "    total_smape = torch.zeros(2, device=device) # Added device argument\n",
    "    total_mse = torch.zeros(2, device=device)   # Added device argument\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            absolute_error = torch.abs(outputs - labels)  # Absolute error\n",
    "            total_mae += absolute_error.sum(dim=0)\n",
    "\n",
    "            non_zero_mask = torch.abs(labels) > 1e-8  # Only calculate percentage for non-zero labels\n",
    "            percentage_error = (absolute_error / torch.abs(labels)) * 100  # Percentage error\n",
    "            total_mape += (percentage_error * non_zero_mask).sum(dim=0)\n",
    "\n",
    "            smape = 200.0 * torch.abs(outputs - labels) / (torch.abs(outputs) + torch.abs(labels) + torch.finfo(torch.float32).eps)\n",
    "            total_smape += smape.sum(dim=0)\n",
    "            \n",
    "            mse = (outputs - labels) ** 2  # Mean Square Error\n",
    "            total_mse += mse.sum(dim=0)\n",
    "\n",
    "            total_count += labels.size(0)  # number of samples\n",
    "\n",
    "    mae = total_mae / total_count\n",
    "    mape = total_mape / total_count\n",
    "    smape = total_smape / total_count\n",
    "    rmse = torch.sqrt(total_mse / total_count)\n",
    "\n",
    "    return mae.cpu().numpy(), mape.cpu().numpy(), smape.cpu().numpy(), rmse.cpu().numpy()  # Moved tensors to CPU and converted to numpy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"models/newcnn_model_500images_b8_e25.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "mae, mape, smape, rmse = evaluate_model(model, \"test\")\n",
    "for i in range(2):\n",
    "    print(f\"Metrics for prediction {i+1}:\")\n",
    "    print(f\"Mean Absolute Error: {mae[i]:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape[i]:.2f}%\")\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error: {smape[i]:.2f}%\")\n",
    "    print(f\"Root Mean Square Error: {rmse[i]:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted variables: [0.011563804000616074, 281.4032897949219]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"cnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# using cv2 is gross but i like the transforms more\n",
    "image_path = \"test\\epsnx0.10_alfax-0.00_betax282.89_epsny0.10_alfay-0.55_betay170.00_epsnz5.00_alfaz0.10_betaz10.00.png\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = transforms.ToTensor()(image)\n",
    "image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_variables = output.squeeze().tolist()\n",
    "\n",
    "print(\"Predicted variables:\", predicted_variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
