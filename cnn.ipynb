{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# def partition_images(base_dir, train_dir, test_dir, train_ratio):\n",
    "#     os.makedirs(train_dir, exist_ok=True)\n",
    "#     os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "#     all_images = os.listdir(base_dir)\n",
    "#     random.shuffle(all_images)\n",
    "\n",
    "#     train_count = int(len(all_images) * train_ratio)\n",
    "#     train_images = all_images[:train_count]\n",
    "#     test_images = all_images[train_count:]\n",
    "\n",
    "#     for image in train_images:\n",
    "#         shutil.copyfile(os.path.join(base_dir, image), os.path.join(train_dir, image))\n",
    "\n",
    "#     for image in test_images:\n",
    "#         shutil.copyfile(os.path.join(base_dir, image), os.path.join(test_dir, image))\n",
    "\n",
    "# partition_images('images', 'train', 'test', 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.dropout = nn.Dropout(0.5)  # Added dropout layer\n",
    "#         self.fc1 = nn.Linear(64 * 73 * 73, 256)  # Adjusted input size\n",
    "#         self.fc2 = nn.Linear(256, 2)  # predicting two variables (alfa x and betax)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.dropout(x)  # Apply dropout after maxpool\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.dropout(x)  # Apply dropout after maxpool\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "    # def __init__(self):\n",
    "    #     super(CNN, self).__init__()\n",
    "    #     self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "    #     self.relu = nn.ReLU()\n",
    "    #     self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    #     self.fc1 = nn.Linear(16 * 147 * 147, 256)\n",
    "    #     self.fc2 = nn.Linear(256, 2) # predicting two variables (alfa x and betax)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.conv1(x)\n",
    "    #     x = self.relu(x)\n",
    "    #     x = self.maxpool(x)\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "    #     x = self.fc1(x)\n",
    "    #     x = self.relu(x)\n",
    "    #     x = self.fc2(x)\n",
    "    #     return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "# ensure determinism\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 147 * 147, 256)\n",
    "        self.fc2 = nn.Linear(256, 2) # predicting two variables (alfa x and betax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files):\n",
    "        self.image_files = image_files\n",
    "        self.pattern = r\"epsnx([\\d.-]+)_alfax([\\d.-]+)_betax([\\d.-]+)_epsny([\\d.-]+)_alfay([\\d.-]+)_betay([\\d.-]+)_epsnz([\\d.-]+)_alfaz([\\d.-]+)_betaz([\\d.-]+)\\.png\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_files[index]\n",
    "        image_name = os.path.basename(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize the image to 295x295\n",
    "        image = cv2.resize(image, (295, 295))\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        matches = re.search(self.pattern, image_name)\n",
    "        variables = [float(matches.group(i)) for i in range(1, 10) if matches.group(i)]\n",
    "        alfa_x = variables[1] # alfax value\n",
    "        beta_x = variables[2] # betax value\n",
    "        return image, torch.tensor([alfa_x, beta_x]) # returns alfa x and betax as the labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# checkpoint = torch.load(filename)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# num_epochs = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch [1/400], Train Loss: 8656.7930\n",
      "Epoch [2/400], Train Loss: 12374.5098\n",
      "Epoch [3/400], Train Loss: 1620.7599\n",
      "Epoch [4/400], Train Loss: 12022.5010\n",
      "Epoch [5/400], Train Loss: 4509.9951\n",
      "Epoch [6/400], Train Loss: 2474.6177\n",
      "Epoch [7/400], Train Loss: 925.8811\n",
      "Epoch [8/400], Train Loss: 623.4400\n",
      "Epoch [9/400], Train Loss: 1542.4626\n",
      "Epoch [10/400], Train Loss: 1248.3260\n",
      "Epoch [11/400], Train Loss: 3291.0969\n",
      "Epoch [12/400], Train Loss: 5722.4170\n",
      "Epoch [13/400], Train Loss: 139.1386\n",
      "Epoch [14/400], Train Loss: 420.1382\n",
      "Epoch [15/400], Train Loss: 381.6141\n",
      "Epoch [16/400], Train Loss: 1792.1631\n",
      "Epoch [17/400], Train Loss: 255.5192\n",
      "Epoch [18/400], Train Loss: 470.4072\n",
      "Epoch [19/400], Train Loss: 3114.0107\n",
      "Epoch [20/400], Train Loss: 1279.9836\n",
      "Epoch [21/400], Train Loss: 635.3932\n",
      "Epoch [22/400], Train Loss: 1061.4446\n",
      "Epoch [23/400], Train Loss: 1606.2670\n",
      "Epoch [24/400], Train Loss: 37.4555\n",
      "Epoch [25/400], Train Loss: 191.5238\n",
      "Epoch [26/400], Train Loss: 154.2763\n",
      "Epoch [27/400], Train Loss: 278.3380\n",
      "Epoch [28/400], Train Loss: 1708.0249\n",
      "Epoch [29/400], Train Loss: 473.5228\n",
      "Epoch [30/400], Train Loss: 167.7509\n",
      "Epoch [31/400], Train Loss: 1648.5007\n",
      "Epoch [32/400], Train Loss: 376.3340\n",
      "Epoch [33/400], Train Loss: 184.8878\n",
      "Epoch [34/400], Train Loss: 272.1324\n",
      "Epoch [35/400], Train Loss: 470.6164\n",
      "Epoch [36/400], Train Loss: 236.5148\n",
      "Epoch [37/400], Train Loss: 207.6817\n",
      "Epoch [38/400], Train Loss: 359.1866\n",
      "Epoch [39/400], Train Loss: 64.6675\n",
      "Epoch [40/400], Train Loss: 130.2231\n",
      "Epoch [41/400], Train Loss: 424.5363\n",
      "Epoch [42/400], Train Loss: 1481.6130\n",
      "Epoch [43/400], Train Loss: 1584.9655\n",
      "Epoch [44/400], Train Loss: 477.1028\n",
      "Epoch [45/400], Train Loss: 30.5289\n",
      "Epoch [46/400], Train Loss: 160.0523\n",
      "Epoch [47/400], Train Loss: 231.5442\n",
      "Epoch [48/400], Train Loss: 466.9778\n",
      "Epoch [49/400], Train Loss: 485.9273\n",
      "Epoch [50/400], Train Loss: 367.0974\n",
      "Epoch [51/400], Train Loss: 219.3376\n",
      "Epoch [52/400], Train Loss: 167.2077\n",
      "Epoch [53/400], Train Loss: 832.3851\n",
      "Epoch [54/400], Train Loss: 376.5239\n",
      "Epoch [55/400], Train Loss: 771.2620\n",
      "Epoch [56/400], Train Loss: 95.0302\n",
      "Epoch [57/400], Train Loss: 128.0712\n",
      "Epoch [58/400], Train Loss: 177.4020\n",
      "Epoch [59/400], Train Loss: 138.4655\n",
      "Epoch [60/400], Train Loss: 27.0372\n",
      "Epoch [61/400], Train Loss: 235.4395\n",
      "Epoch [62/400], Train Loss: 147.5724\n",
      "Epoch [63/400], Train Loss: 478.8437\n",
      "Epoch [64/400], Train Loss: 187.5047\n",
      "Epoch [65/400], Train Loss: 71.2363\n",
      "Epoch [66/400], Train Loss: 820.3799\n",
      "Epoch [67/400], Train Loss: 335.5586\n",
      "Epoch [68/400], Train Loss: 126.2165\n",
      "Epoch [69/400], Train Loss: 311.3577\n",
      "Epoch [70/400], Train Loss: 68.7422\n",
      "Epoch [71/400], Train Loss: 365.5421\n",
      "Epoch [72/400], Train Loss: 254.4496\n",
      "Epoch [73/400], Train Loss: 389.8507\n",
      "Epoch [74/400], Train Loss: 943.2186\n",
      "Epoch [75/400], Train Loss: 240.6726\n",
      "Epoch [76/400], Train Loss: 364.1372\n",
      "Epoch [77/400], Train Loss: 371.6628\n",
      "Epoch [78/400], Train Loss: 16.4620\n",
      "Epoch [79/400], Train Loss: 76.9161\n",
      "Epoch [80/400], Train Loss: 105.9373\n",
      "Epoch [81/400], Train Loss: 668.6054\n",
      "Epoch [82/400], Train Loss: 30.8644\n",
      "Epoch [83/400], Train Loss: 228.8413\n",
      "Epoch [84/400], Train Loss: 151.9288\n",
      "Epoch [85/400], Train Loss: 127.1872\n",
      "Epoch [86/400], Train Loss: 180.6134\n",
      "Epoch [87/400], Train Loss: 1173.3871\n",
      "Epoch [88/400], Train Loss: 69.7768\n",
      "Epoch [89/400], Train Loss: 31.2999\n",
      "Epoch [90/400], Train Loss: 34.2838\n",
      "Epoch [91/400], Train Loss: 701.5020\n",
      "Epoch [92/400], Train Loss: 95.8776\n",
      "Epoch [93/400], Train Loss: 175.9399\n",
      "Epoch [94/400], Train Loss: 561.4495\n",
      "Epoch [95/400], Train Loss: 282.1614\n",
      "Epoch [96/400], Train Loss: 66.7032\n",
      "Epoch [97/400], Train Loss: 264.2216\n",
      "Epoch [98/400], Train Loss: 60.2975\n",
      "Epoch [99/400], Train Loss: 276.7339\n",
      "Epoch [100/400], Train Loss: 256.0278\n",
      "Epoch [101/400], Train Loss: 153.9557\n",
      "Epoch [102/400], Train Loss: 93.7451\n",
      "Epoch [103/400], Train Loss: 77.6545\n",
      "Epoch [104/400], Train Loss: 92.4739\n",
      "Epoch [105/400], Train Loss: 51.6663\n",
      "Epoch [106/400], Train Loss: 218.9749\n",
      "Epoch [107/400], Train Loss: 261.0148\n",
      "Epoch [108/400], Train Loss: 156.6857\n",
      "Epoch [109/400], Train Loss: 133.9496\n",
      "Epoch [110/400], Train Loss: 19.6955\n",
      "Epoch [111/400], Train Loss: 224.6773\n",
      "Epoch [112/400], Train Loss: 57.1622\n",
      "Epoch [113/400], Train Loss: 417.6970\n",
      "Epoch [114/400], Train Loss: 161.7936\n",
      "Epoch [115/400], Train Loss: 29.1335\n",
      "Epoch [116/400], Train Loss: 321.7959\n",
      "Epoch [117/400], Train Loss: 199.4137\n",
      "Epoch [118/400], Train Loss: 167.3884\n",
      "Epoch [119/400], Train Loss: 215.2012\n",
      "Epoch [120/400], Train Loss: 182.9746\n",
      "Epoch [121/400], Train Loss: 136.2896\n",
      "Epoch [122/400], Train Loss: 219.9964\n",
      "Epoch [123/400], Train Loss: 475.5374\n",
      "Epoch [124/400], Train Loss: 14.9637\n",
      "Epoch [125/400], Train Loss: 434.7441\n",
      "Epoch [126/400], Train Loss: 37.4288\n",
      "Epoch [127/400], Train Loss: 121.3159\n",
      "Epoch [128/400], Train Loss: 88.2156\n",
      "Epoch [129/400], Train Loss: 39.0484\n",
      "Epoch [130/400], Train Loss: 49.0893\n",
      "Epoch [131/400], Train Loss: 107.4289\n",
      "Epoch [132/400], Train Loss: 87.7475\n",
      "Epoch [133/400], Train Loss: 55.3725\n",
      "Epoch [134/400], Train Loss: 70.3209\n",
      "Epoch [135/400], Train Loss: 10.1409\n",
      "Epoch [136/400], Train Loss: 100.3568\n",
      "Epoch [137/400], Train Loss: 74.8187\n",
      "Epoch [138/400], Train Loss: 65.2945\n",
      "Epoch [139/400], Train Loss: 67.6849\n",
      "Epoch [140/400], Train Loss: 75.5563\n",
      "Epoch [141/400], Train Loss: 81.1998\n",
      "Epoch [142/400], Train Loss: 68.2682\n",
      "Epoch [143/400], Train Loss: 564.7211\n",
      "Epoch [144/400], Train Loss: 44.0294\n",
      "Epoch [145/400], Train Loss: 126.3703\n",
      "Epoch [146/400], Train Loss: 63.8111\n",
      "Epoch [147/400], Train Loss: 281.0477\n",
      "Epoch [148/400], Train Loss: 101.9683\n",
      "Epoch [149/400], Train Loss: 85.8795\n",
      "Epoch [150/400], Train Loss: 66.8597\n",
      "Epoch [151/400], Train Loss: 3.7281\n",
      "Epoch [152/400], Train Loss: 122.0966\n",
      "Epoch [153/400], Train Loss: 75.7738\n",
      "Epoch [154/400], Train Loss: 62.5589\n",
      "Epoch [155/400], Train Loss: 105.9106\n",
      "Epoch [156/400], Train Loss: 189.7168\n",
      "Epoch [157/400], Train Loss: 58.4858\n",
      "Epoch [158/400], Train Loss: 65.5869\n",
      "Epoch [159/400], Train Loss: 168.3990\n",
      "Epoch [160/400], Train Loss: 85.1430\n",
      "Epoch [161/400], Train Loss: 35.9343\n",
      "Epoch [162/400], Train Loss: 40.2004\n",
      "Epoch [163/400], Train Loss: 32.6947\n",
      "Epoch [164/400], Train Loss: 81.2930\n",
      "Epoch [165/400], Train Loss: 129.5309\n",
      "Epoch [166/400], Train Loss: 97.4623\n",
      "Epoch [167/400], Train Loss: 47.0110\n",
      "Epoch [168/400], Train Loss: 49.1122\n",
      "Epoch [169/400], Train Loss: 123.5698\n",
      "Epoch [170/400], Train Loss: 167.2563\n",
      "Epoch [171/400], Train Loss: 28.4448\n",
      "Epoch [172/400], Train Loss: 780.2200\n",
      "Epoch [173/400], Train Loss: 48.3827\n",
      "Epoch [174/400], Train Loss: 176.7348\n",
      "Epoch [175/400], Train Loss: 35.3587\n",
      "Epoch [176/400], Train Loss: 12.2052\n",
      "Epoch [177/400], Train Loss: 3.7804\n",
      "Epoch [178/400], Train Loss: 63.3022\n",
      "Epoch [179/400], Train Loss: 75.8250\n",
      "Epoch [180/400], Train Loss: 24.6228\n",
      "Epoch [181/400], Train Loss: 276.3903\n",
      "Epoch [182/400], Train Loss: 122.8928\n",
      "Epoch [183/400], Train Loss: 29.0261\n",
      "Epoch [184/400], Train Loss: 31.0908\n",
      "Epoch [185/400], Train Loss: 191.0605\n",
      "Epoch [186/400], Train Loss: 52.4965\n",
      "Epoch [187/400], Train Loss: 48.1994\n",
      "Epoch [188/400], Train Loss: 87.9849\n",
      "Epoch [189/400], Train Loss: 19.4947\n",
      "Epoch [190/400], Train Loss: 119.8974\n",
      "Epoch [191/400], Train Loss: 399.1864\n",
      "Epoch [192/400], Train Loss: 8.0531\n",
      "Epoch [193/400], Train Loss: 54.3216\n",
      "Epoch [194/400], Train Loss: 48.4334\n",
      "Epoch [195/400], Train Loss: 161.6674\n",
      "Epoch [196/400], Train Loss: 14.5450\n",
      "Epoch [197/400], Train Loss: 98.8427\n",
      "Epoch [198/400], Train Loss: 36.1712\n",
      "Epoch [199/400], Train Loss: 124.9419\n",
      "Epoch [200/400], Train Loss: 41.8129\n",
      "Epoch [201/400], Train Loss: 6.8805\n",
      "Epoch [202/400], Train Loss: 261.2011\n",
      "Epoch [203/400], Train Loss: 24.8380\n",
      "Epoch [204/400], Train Loss: 10.1427\n",
      "Epoch [205/400], Train Loss: 13.7772\n",
      "Epoch [206/400], Train Loss: 51.3560\n",
      "Epoch [207/400], Train Loss: 182.3168\n",
      "Epoch [208/400], Train Loss: 27.1847\n",
      "Epoch [209/400], Train Loss: 2.2718\n",
      "Epoch [210/400], Train Loss: 246.0239\n",
      "Epoch [211/400], Train Loss: 55.5551\n",
      "Epoch [212/400], Train Loss: 4.4189\n",
      "Epoch [213/400], Train Loss: 18.4249\n",
      "Epoch [214/400], Train Loss: 12.8671\n",
      "Epoch [215/400], Train Loss: 49.6378\n",
      "Epoch [216/400], Train Loss: 13.7018\n",
      "Epoch [217/400], Train Loss: 69.6232\n",
      "Epoch [218/400], Train Loss: 11.8086\n",
      "Epoch [219/400], Train Loss: 7.9525\n",
      "Epoch [220/400], Train Loss: 48.5049\n",
      "Epoch [221/400], Train Loss: 29.8944\n",
      "Epoch [222/400], Train Loss: 52.7878\n",
      "Epoch [223/400], Train Loss: 51.0564\n",
      "Epoch [224/400], Train Loss: 15.9504\n",
      "Epoch [225/400], Train Loss: 91.7395\n",
      "Epoch [226/400], Train Loss: 45.1207\n",
      "Epoch [227/400], Train Loss: 33.9869\n",
      "Epoch [228/400], Train Loss: 74.5005\n",
      "Epoch [229/400], Train Loss: 21.5466\n",
      "Epoch [230/400], Train Loss: 15.8167\n",
      "Epoch [231/400], Train Loss: 29.9370\n",
      "Epoch [232/400], Train Loss: 43.7679\n",
      "Epoch [233/400], Train Loss: 25.1021\n",
      "Epoch [234/400], Train Loss: 41.2528\n",
      "Epoch [235/400], Train Loss: 9.5630\n",
      "Epoch [236/400], Train Loss: 13.9248\n",
      "Epoch [237/400], Train Loss: 72.4836\n",
      "Epoch [238/400], Train Loss: 11.7499\n",
      "Epoch [239/400], Train Loss: 68.6780\n",
      "Epoch [240/400], Train Loss: 43.0494\n",
      "Epoch [241/400], Train Loss: 57.9835\n",
      "Epoch [242/400], Train Loss: 188.7205\n",
      "Epoch [243/400], Train Loss: 63.8930\n",
      "Epoch [244/400], Train Loss: 8.5859\n",
      "Epoch [245/400], Train Loss: 27.1109\n",
      "Epoch [246/400], Train Loss: 27.3944\n",
      "Epoch [247/400], Train Loss: 14.6064\n",
      "Epoch [248/400], Train Loss: 147.5757\n",
      "Epoch [249/400], Train Loss: 87.7020\n",
      "Epoch [250/400], Train Loss: 18.7406\n",
      "Epoch [251/400], Train Loss: 15.6961\n",
      "Epoch [252/400], Train Loss: 9.7396\n",
      "Epoch [253/400], Train Loss: 45.4627\n",
      "Epoch [254/400], Train Loss: 28.2100\n",
      "Epoch [255/400], Train Loss: 61.8840\n",
      "Epoch [256/400], Train Loss: 20.1949\n",
      "Epoch [257/400], Train Loss: 85.8755\n",
      "Epoch [258/400], Train Loss: 8.5975\n",
      "Epoch [259/400], Train Loss: 85.3534\n",
      "Epoch [260/400], Train Loss: 10.3366\n",
      "Epoch [261/400], Train Loss: 16.2482\n",
      "Epoch [262/400], Train Loss: 44.7756\n",
      "Epoch [263/400], Train Loss: 51.7343\n",
      "Epoch [264/400], Train Loss: 16.1257\n",
      "Epoch [265/400], Train Loss: 34.4094\n",
      "Epoch [266/400], Train Loss: 54.3394\n",
      "Epoch [267/400], Train Loss: 86.5452\n",
      "Epoch [268/400], Train Loss: 14.7890\n",
      "Epoch [269/400], Train Loss: 8.8591\n",
      "Epoch [270/400], Train Loss: 22.5441\n",
      "Epoch [271/400], Train Loss: 15.7740\n",
      "Epoch [272/400], Train Loss: 40.8140\n",
      "Epoch [273/400], Train Loss: 74.4529\n",
      "Epoch [274/400], Train Loss: 32.3083\n",
      "Epoch [275/400], Train Loss: 9.3025\n",
      "Epoch [276/400], Train Loss: 3.4546\n",
      "Epoch [277/400], Train Loss: 5.8281\n",
      "Epoch [278/400], Train Loss: 12.5323\n",
      "Epoch [279/400], Train Loss: 22.7411\n",
      "Epoch [280/400], Train Loss: 23.1826\n",
      "Epoch [281/400], Train Loss: 148.1614\n",
      "Epoch [282/400], Train Loss: 77.7670\n",
      "Epoch [283/400], Train Loss: 27.5096\n",
      "Epoch [284/400], Train Loss: 11.8621\n",
      "Epoch [285/400], Train Loss: 50.7724\n",
      "Epoch [286/400], Train Loss: 11.6203\n",
      "Epoch [287/400], Train Loss: 24.3486\n",
      "Epoch [288/400], Train Loss: 44.3662\n",
      "Epoch [289/400], Train Loss: 34.7479\n",
      "Epoch [290/400], Train Loss: 146.9017\n",
      "Epoch [291/400], Train Loss: 25.3145\n",
      "Epoch [292/400], Train Loss: 46.1294\n",
      "Epoch [293/400], Train Loss: 16.2875\n",
      "Epoch [294/400], Train Loss: 18.2968\n",
      "Epoch [295/400], Train Loss: 2.4328\n",
      "Epoch [296/400], Train Loss: 12.3081\n",
      "Epoch [297/400], Train Loss: 17.1544\n",
      "Epoch [298/400], Train Loss: 20.9191\n",
      "Epoch [299/400], Train Loss: 63.1118\n",
      "Epoch [300/400], Train Loss: 83.2233\n",
      "Epoch [301/400], Train Loss: 56.7624\n",
      "Epoch [302/400], Train Loss: 25.8165\n",
      "Epoch [303/400], Train Loss: 16.4733\n",
      "Epoch [304/400], Train Loss: 362.7162\n",
      "Epoch [305/400], Train Loss: 195.1656\n",
      "Epoch [306/400], Train Loss: 143.1096\n",
      "Epoch [307/400], Train Loss: 111.0233\n",
      "Epoch [308/400], Train Loss: 262.4720\n",
      "Epoch [309/400], Train Loss: 21.5581\n",
      "Epoch [310/400], Train Loss: 153.9493\n",
      "Epoch [311/400], Train Loss: 11.5209\n",
      "Epoch [312/400], Train Loss: 36.2844\n",
      "Epoch [313/400], Train Loss: 8.2485\n",
      "Epoch [314/400], Train Loss: 113.1579\n",
      "Epoch [315/400], Train Loss: 5.2118\n",
      "Epoch [316/400], Train Loss: 24.5944\n",
      "Epoch [317/400], Train Loss: 6.4801\n",
      "Epoch [318/400], Train Loss: 26.1198\n",
      "Epoch [319/400], Train Loss: 94.2025\n",
      "Epoch [320/400], Train Loss: 4.5604\n",
      "Epoch [321/400], Train Loss: 71.0635\n",
      "Epoch [322/400], Train Loss: 16.7322\n",
      "Epoch [323/400], Train Loss: 39.3852\n",
      "Epoch [324/400], Train Loss: 37.9159\n",
      "Epoch [325/400], Train Loss: 40.8919\n",
      "Epoch [326/400], Train Loss: 19.2474\n",
      "Epoch [327/400], Train Loss: 476.1601\n",
      "Epoch [328/400], Train Loss: 3.9689\n",
      "Epoch [329/400], Train Loss: 38.7929\n",
      "Epoch [330/400], Train Loss: 41.8147\n",
      "Epoch [331/400], Train Loss: 63.6074\n",
      "Epoch [332/400], Train Loss: 34.1699\n",
      "Epoch [333/400], Train Loss: 1.4976\n",
      "Epoch [334/400], Train Loss: 42.3778\n",
      "Epoch [335/400], Train Loss: 73.6041\n",
      "Epoch [336/400], Train Loss: 12.0335\n",
      "Epoch [337/400], Train Loss: 18.9519\n",
      "Epoch [338/400], Train Loss: 21.3177\n",
      "Epoch [339/400], Train Loss: 64.5428\n",
      "Epoch [340/400], Train Loss: 5.8766\n",
      "Epoch [341/400], Train Loss: 19.6405\n",
      "Epoch [342/400], Train Loss: 57.1623\n",
      "Epoch [343/400], Train Loss: 50.9259\n",
      "Epoch [344/400], Train Loss: 92.5254\n",
      "Epoch [345/400], Train Loss: 7.2773\n",
      "Epoch [346/400], Train Loss: 27.3186\n",
      "Epoch [347/400], Train Loss: 11.0439\n",
      "Epoch [348/400], Train Loss: 23.2262\n",
      "Epoch [349/400], Train Loss: 25.7306\n",
      "Epoch [350/400], Train Loss: 32.7587\n",
      "Epoch [351/400], Train Loss: 33.5962\n",
      "Epoch [352/400], Train Loss: 9.8883\n",
      "Epoch [353/400], Train Loss: 9.1956\n",
      "Epoch [354/400], Train Loss: 27.4423\n",
      "Epoch [355/400], Train Loss: 5.3855\n",
      "Epoch [356/400], Train Loss: 44.4190\n",
      "Epoch [357/400], Train Loss: 28.0520\n",
      "Epoch [358/400], Train Loss: 24.0549\n",
      "Epoch [359/400], Train Loss: 5.6256\n",
      "Epoch [360/400], Train Loss: 9.9809\n",
      "Epoch [361/400], Train Loss: 12.8559\n",
      "Epoch [362/400], Train Loss: 8.2839\n",
      "Epoch [363/400], Train Loss: 1.6876\n",
      "Epoch [364/400], Train Loss: 12.2522\n",
      "Epoch [365/400], Train Loss: 289.4234\n",
      "Epoch [366/400], Train Loss: 59.4136\n",
      "Epoch [367/400], Train Loss: 98.5551\n",
      "Epoch [368/400], Train Loss: 50.7591\n",
      "Epoch [369/400], Train Loss: 8.5715\n",
      "Epoch [370/400], Train Loss: 4.7127\n",
      "Epoch [371/400], Train Loss: 42.3249\n",
      "Epoch [372/400], Train Loss: 23.3680\n",
      "Epoch [373/400], Train Loss: 36.4983\n",
      "Epoch [374/400], Train Loss: 29.2044\n",
      "Epoch [375/400], Train Loss: 0.7041\n",
      "Epoch [376/400], Train Loss: 4.2761\n",
      "Epoch [377/400], Train Loss: 2.1560\n",
      "Epoch [378/400], Train Loss: 3.1232\n",
      "Epoch [379/400], Train Loss: 27.1659\n",
      "Epoch [380/400], Train Loss: 49.9255\n",
      "Epoch [381/400], Train Loss: 22.4238\n",
      "Epoch [382/400], Train Loss: 278.2876\n",
      "Epoch [383/400], Train Loss: 61.4486\n",
      "Epoch [384/400], Train Loss: 73.6745\n",
      "Epoch [385/400], Train Loss: 13.4041\n",
      "Epoch [386/400], Train Loss: 10.6674\n",
      "Epoch [387/400], Train Loss: 26.1068\n",
      "Epoch [388/400], Train Loss: 128.6895\n",
      "Epoch [389/400], Train Loss: 23.4414\n",
      "Epoch [390/400], Train Loss: 65.1461\n",
      "Epoch [391/400], Train Loss: 14.6397\n",
      "Epoch [392/400], Train Loss: 5.2255\n",
      "Epoch [393/400], Train Loss: 11.7906\n",
      "Epoch [394/400], Train Loss: 5.7208\n",
      "Epoch [395/400], Train Loss: 18.5848\n",
      "Epoch [396/400], Train Loss: 13.5252\n",
      "Epoch [397/400], Train Loss: 18.9170\n",
      "Epoch [398/400], Train Loss: 29.7922\n",
      "Epoch [399/400], Train Loss: 32.9801\n",
      "Epoch [400/400], Train Loss: 18.8252\n",
      "Test Loss: 985.0717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "image_dir = \"./hostinger-images-80bin/2-param/\"\n",
    "image_files = [os.path.join(image_dir, name) for name in sorted(os.listdir(image_dir))]\n",
    "total_size = len(image_files)\n",
    "train_prop = 0.85\n",
    "test_prop = 0.15\n",
    "train_size = int(train_prop * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "# Split the dataset, removed validation network\n",
    "train_files = image_files[:train_size]\n",
    "test_files = image_files[train_size:]\n",
    "\n",
    "train_dataset = CustomDataset(train_files)\n",
    "test_dataset = CustomDataset(test_files)\n",
    "\n",
    "# define your parameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [4],\n",
    "    'learning_rate': [0.001],\n",
    "    'num_epochs': [500]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in grid:\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "\n",
    "    filename = f\"cnn_model3_bs{batch_size}_lr{learning_rate}_e{num_epochs}.pth\"\n",
    "\n",
    "    # If the model file already exists, skip this iteration\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Model file {filename} already exists. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()  # set the model to training mode\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Test\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(test_dataloader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    filename = f\"cnn_model3_bs{batch_size}_lr{learning_rate}_e{num_epochs}.pth\"\n",
    "    torch.save(model.state_dict(), filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    total_mae = torch.zeros(2, device=device)\n",
    "    total_mape = torch.zeros(2, device=device)\n",
    "    total_smape = torch.zeros(2, device=device)\n",
    "    total_mse = torch.zeros(2, device=device)\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            absolute_error = torch.abs(outputs - labels)\n",
    "            total_mae += absolute_error.sum(dim=0)\n",
    "\n",
    "            non_zero_mask = torch.abs(labels) > 1e-8\n",
    "            percentage_error = (absolute_error / torch.abs(labels)) * 100\n",
    "            total_mape += (percentage_error * non_zero_mask).sum(dim=0)\n",
    "\n",
    "            smape = 200.0 * torch.abs(outputs - labels) / (torch.abs(outputs) + torch.abs(labels) + torch.finfo(torch.float32).eps)\n",
    "            total_smape += smape.sum(dim=0)\n",
    "            \n",
    "            mse = (outputs - labels) ** 2\n",
    "            total_mse += mse.sum(dim=0)\n",
    "\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    mae = total_mae / total_count\n",
    "    mape = total_mape / total_count\n",
    "    smape = total_smape / total_count\n",
    "    rmse = torch.sqrt(total_mse / total_count)\n",
    "\n",
    "    return mae.cpu().numpy(), mape.cpu().numpy(), smape.cpu().numpy(), rmse.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: .\\cnn_model3_bs4_lr0.001_e400.pth, Aggregate Score: 18.8496\n",
      "Metrics for prediction 1:\n",
      "Mean Absolute Error: 0.5984\n",
      "Mean Absolute Percentage Error: 13.66%\n",
      "Symmetric Mean Absolute Percentage Error: 14.91%\n",
      "Root Mean Square Error: 0.7416\n",
      "Metrics for prediction 2:\n",
      "Mean Absolute Error: 37.2149\n",
      "Mean Absolute Percentage Error: 17.74%\n",
      "Symmetric Mean Absolute Percentage Error: 21.64%\n",
      "Root Mean Square Error: 44.2813\n",
      "\n",
      "Model: .\\model-best-retest-shorter.pth, Aggregate Score: 120.5657\n",
      "Metrics for prediction 1:\n",
      "Mean Absolute Error: 2.1865\n",
      "Mean Absolute Percentage Error: 50.21%\n",
      "Symmetric Mean Absolute Percentage Error: 67.33%\n",
      "Root Mean Square Error: 2.2210\n",
      "Metrics for prediction 2:\n",
      "Mean Absolute Error: 268.2839\n",
      "Mean Absolute Percentage Error: 94.26%\n",
      "Symmetric Mean Absolute Percentage Error: 179.26%\n",
      "Root Mean Square Error: 300.7775\n",
      "\n",
      "Model: .\\current_best_model_b4_lr0.001_e500.pth, Aggregate Score: 124.5978\n",
      "Metrics for prediction 1:\n",
      "Mean Absolute Error: 2.7696\n",
      "Mean Absolute Percentage Error: 63.84%\n",
      "Symmetric Mean Absolute Percentage Error: 94.08%\n",
      "Root Mean Square Error: 2.7971\n",
      "Metrics for prediction 2:\n",
      "Mean Absolute Error: 265.6624\n",
      "Mean Absolute Percentage Error: 93.76%\n",
      "Symmetric Mean Absolute Percentage Error: 177.57%\n",
      "Root Mean Square Error: 296.3069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Path to your model directory\n",
    "model_dir = './'\n",
    "\n",
    "# Sort models based on performance\n",
    "model_performance = []\n",
    "\n",
    "# For each model file in the directory\n",
    "for model_file in glob.glob(model_dir + '/*.pth'):\n",
    "    # Load model state\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluate model\n",
    "    mae, mape, smape, rmse = evaluate_model(model, test_dataset)  # Pass test_dataset instead of \"test\"\n",
    "\n",
    "    # Calculate some form of aggregate score. Here, I use the mean of all metrics.\n",
    "    # Change this if you want a different form of ranking.\n",
    "    aggregate_score = np.mean([np.mean(mae), np.mean(mape), np.mean(smape), np.mean(rmse)])\n",
    "\n",
    "    # Save model's performance\n",
    "    model_performance.append((model_file, mae, mape, smape, rmse, aggregate_score))\n",
    "\n",
    "# Sort models based on the aggregate score\n",
    "model_performance.sort(key=lambda x: x[5])\n",
    "\n",
    "# Print model performance in order\n",
    "for model_info in model_performance:\n",
    "    model_file, mae, mape, smape, rmse, aggregate_score = model_info\n",
    "    print(f\"Model: {model_file}, Aggregate Score: {aggregate_score:.4f}\")\n",
    "    for i in range(2):\n",
    "        print(f\"Metrics for prediction {i+1}:\")\n",
    "        print(f\"Mean Absolute Error: {mae[i]:.4f}\")\n",
    "        print(f\"Mean Absolute Percentage Error: {mape[i]:.2f}%\")\n",
    "        print(f\"Symmetric Mean Absolute Percentage Error: {smape[i]:.2f}%\")\n",
    "        print(f\"Root Mean Square Error: {rmse[i]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Open the text document in write mode\n",
    "with open('model_performance_model_test.txt', 'w') as f:\n",
    "    # Print model performance in order\n",
    "    for model_info in model_performance:\n",
    "        model_file, mae, mape, smape, rmse, aggregate_score = model_info\n",
    "        f.write(f\"Model: {model_file}, Aggregate Score: {aggregate_score:.4f}\\n\")\n",
    "        for i in range(2):\n",
    "            f.write(f\"Metrics for prediction {i+1}:\\n\")\n",
    "            f.write(f\"Mean Absolute Error: {mae[i]:.4f}\\n\")\n",
    "            f.write(f\"Mean Absolute Percentage Error: {mape[i]:.2f}%\\n\")\n",
    "            f.write(f\"Symmetric Mean Absolute Percentage Error: {smape[i]:.2f}%\\n\")\n",
    "            f.write(f\"Root Mean Square Error: {rmse[i]:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cnn_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m CNN()\n\u001b[1;32m----> 5\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mcnn_model.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      8\u001b[0m \u001b[39m# using cv2 is gross but i like the transforms more\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\ianja\\.conda\\envs\\torch_gpu\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cnn_model.pth'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"cnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# using cv2 is gross but i like the transforms more\n",
    "image_path = \"test\\epsnx0.10_alfax-0.00_betax282.89_epsny0.10_alfay-0.55_betay170.00_epsnz5.00_alfaz0.10_betaz10.00.png\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = transforms.ToTensor()(image)\n",
    "image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_variables = output.squeeze().tolist()\n",
    "\n",
    "print(\"Predicted variables:\", predicted_variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
