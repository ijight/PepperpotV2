{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def partition_images(base_dir, train_dir, test_dir, train_ratio):\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    all_images = os.listdir(base_dir)\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    train_count = int(len(all_images) * train_ratio)\n",
    "    train_images = all_images[:train_count]\n",
    "    test_images = all_images[train_count:]\n",
    "\n",
    "    for image in train_images:\n",
    "        shutil.copyfile(os.path.join(base_dir, image), os.path.join(train_dir, image))\n",
    "\n",
    "    for image in test_images:\n",
    "        shutil.copyfile(os.path.join(base_dir, image), os.path.join(test_dir, image))\n",
    "\n",
    "partition_images('images', 'train', 'test', 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [1/26], Loss: 50644.3555\n",
      "Epoch [1/500], Step [2/26], Loss: 9929.5088\n",
      "Epoch [1/500], Step [3/26], Loss: 35850.0938\n",
      "Epoch [1/500], Step [4/26], Loss: 11513.4434\n",
      "Epoch [1/500], Step [5/26], Loss: 7076.7783\n",
      "Epoch [1/500], Step [6/26], Loss: 8970.6182\n",
      "Epoch [1/500], Step [7/26], Loss: 10255.7510\n",
      "Epoch [1/500], Step [8/26], Loss: 12498.0508\n",
      "Epoch [1/500], Step [9/26], Loss: 5950.2588\n",
      "Epoch [1/500], Step [10/26], Loss: 10170.3750\n",
      "Epoch [1/500], Step [11/26], Loss: 6176.5229\n",
      "Epoch [1/500], Step [12/26], Loss: 9368.8809\n",
      "Epoch [1/500], Step [13/26], Loss: 7273.3184\n",
      "Epoch [1/500], Step [14/26], Loss: 5982.3408\n",
      "Epoch [1/500], Step [15/26], Loss: 5375.0225\n",
      "Epoch [1/500], Step [16/26], Loss: 8632.0098\n",
      "Epoch [1/500], Step [17/26], Loss: 8417.8438\n",
      "Epoch [1/500], Step [18/26], Loss: 9049.4033\n",
      "Epoch [1/500], Step [19/26], Loss: 14023.5908\n",
      "Epoch [1/500], Step [20/26], Loss: 10278.7471\n",
      "Epoch [1/500], Step [21/26], Loss: 8028.3350\n",
      "Epoch [1/500], Step [22/26], Loss: 8640.2666\n",
      "Epoch [1/500], Step [23/26], Loss: 9130.6016\n",
      "Epoch [1/500], Step [24/26], Loss: 8033.0059\n",
      "Epoch [1/500], Step [25/26], Loss: 10894.9199\n",
      "Epoch [1/500], Step [26/26], Loss: 12158.6943\n",
      "Epoch [2/500], Step [1/26], Loss: 5247.0527\n",
      "Epoch [2/500], Step [2/26], Loss: 7195.4424\n",
      "Epoch [2/500], Step [3/26], Loss: 5216.9697\n",
      "Epoch [2/500], Step [4/26], Loss: 10457.6621\n",
      "Epoch [2/500], Step [5/26], Loss: 7367.9277\n",
      "Epoch [2/500], Step [6/26], Loss: 7363.1738\n",
      "Epoch [2/500], Step [7/26], Loss: 5963.1484\n",
      "Epoch [2/500], Step [8/26], Loss: 9340.0186\n",
      "Epoch [2/500], Step [9/26], Loss: 11064.9541\n",
      "Epoch [2/500], Step [10/26], Loss: 8736.4512\n",
      "Epoch [2/500], Step [11/26], Loss: 11323.0234\n",
      "Epoch [2/500], Step [12/26], Loss: 11306.0820\n",
      "Epoch [2/500], Step [13/26], Loss: 9919.7393\n",
      "Epoch [2/500], Step [14/26], Loss: 7098.0947\n",
      "Epoch [2/500], Step [15/26], Loss: 7704.9707\n",
      "Epoch [2/500], Step [16/26], Loss: 9421.9199\n",
      "Epoch [2/500], Step [17/26], Loss: 9582.4844\n",
      "Epoch [2/500], Step [18/26], Loss: 9923.6934\n",
      "Epoch [2/500], Step [19/26], Loss: 12838.9824\n",
      "Epoch [2/500], Step [20/26], Loss: 7346.8320\n",
      "Epoch [2/500], Step [21/26], Loss: 7870.1865\n",
      "Epoch [2/500], Step [22/26], Loss: 6461.9492\n",
      "Epoch [2/500], Step [23/26], Loss: 9886.1113\n",
      "Epoch [2/500], Step [24/26], Loss: 6902.2764\n",
      "Epoch [2/500], Step [25/26], Loss: 6059.3882\n",
      "Epoch [2/500], Step [26/26], Loss: 3507.0898\n",
      "Epoch [3/500], Step [1/26], Loss: 7957.1191\n",
      "Epoch [3/500], Step [2/26], Loss: 6417.2695\n",
      "Epoch [3/500], Step [3/26], Loss: 8305.9180\n",
      "Epoch [3/500], Step [4/26], Loss: 6632.3501\n",
      "Epoch [3/500], Step [5/26], Loss: 7748.9170\n",
      "Epoch [3/500], Step [6/26], Loss: 6328.5088\n",
      "Epoch [3/500], Step [7/26], Loss: 6872.0640\n",
      "Epoch [3/500], Step [8/26], Loss: 6274.1177\n",
      "Epoch [3/500], Step [9/26], Loss: 8208.9600\n",
      "Epoch [3/500], Step [10/26], Loss: 10493.3691\n",
      "Epoch [3/500], Step [11/26], Loss: 10795.3027\n",
      "Epoch [3/500], Step [12/26], Loss: 10103.3740\n",
      "Epoch [3/500], Step [13/26], Loss: 7334.1060\n",
      "Epoch [3/500], Step [14/26], Loss: 8267.6055\n",
      "Epoch [3/500], Step [15/26], Loss: 5616.9971\n",
      "Epoch [3/500], Step [16/26], Loss: 9889.2441\n",
      "Epoch [3/500], Step [17/26], Loss: 6833.7021\n",
      "Epoch [3/500], Step [18/26], Loss: 8348.0039\n",
      "Epoch [3/500], Step [19/26], Loss: 6586.3516\n",
      "Epoch [3/500], Step [20/26], Loss: 8928.8877\n",
      "Epoch [3/500], Step [21/26], Loss: 4908.0298\n",
      "Epoch [3/500], Step [22/26], Loss: 6573.3516\n",
      "Epoch [3/500], Step [23/26], Loss: 8744.1973\n",
      "Epoch [3/500], Step [24/26], Loss: 6750.3413\n",
      "Epoch [3/500], Step [25/26], Loss: 5250.6582\n",
      "Epoch [3/500], Step [26/26], Loss: 7769.1846\n",
      "Epoch [4/500], Step [1/26], Loss: 7729.5972\n",
      "Epoch [4/500], Step [2/26], Loss: 3756.4641\n",
      "Epoch [4/500], Step [3/26], Loss: 10779.1641\n",
      "Epoch [4/500], Step [4/26], Loss: 10263.4932\n",
      "Epoch [4/500], Step [5/26], Loss: 12621.6582\n",
      "Epoch [4/500], Step [6/26], Loss: 6955.2720\n",
      "Epoch [4/500], Step [7/26], Loss: 8741.7471\n",
      "Epoch [4/500], Step [8/26], Loss: 6733.8691\n",
      "Epoch [4/500], Step [9/26], Loss: 7455.9312\n",
      "Epoch [4/500], Step [10/26], Loss: 5906.8125\n",
      "Epoch [4/500], Step [11/26], Loss: 5706.4736\n",
      "Epoch [4/500], Step [12/26], Loss: 7789.6982\n",
      "Epoch [4/500], Step [13/26], Loss: 5369.4014\n",
      "Epoch [4/500], Step [14/26], Loss: 7651.2188\n",
      "Epoch [4/500], Step [15/26], Loss: 4623.9863\n",
      "Epoch [4/500], Step [16/26], Loss: 5639.3813\n",
      "Epoch [4/500], Step [17/26], Loss: 5801.6938\n",
      "Epoch [4/500], Step [18/26], Loss: 5189.7046\n",
      "Epoch [4/500], Step [19/26], Loss: 6995.1675\n",
      "Epoch [4/500], Step [20/26], Loss: 6799.8975\n",
      "Epoch [4/500], Step [21/26], Loss: 4100.9326\n",
      "Epoch [4/500], Step [22/26], Loss: 9564.6777\n",
      "Epoch [4/500], Step [23/26], Loss: 9093.3965\n",
      "Epoch [4/500], Step [24/26], Loss: 4732.9736\n",
      "Epoch [4/500], Step [25/26], Loss: 5381.8682\n",
      "Epoch [4/500], Step [26/26], Loss: 6728.5068\n",
      "Epoch [5/500], Step [1/26], Loss: 9522.2168\n",
      "Epoch [5/500], Step [2/26], Loss: 7284.9463\n",
      "Epoch [5/500], Step [3/26], Loss: 8794.3379\n",
      "Epoch [5/500], Step [4/26], Loss: 8336.9375\n",
      "Epoch [5/500], Step [5/26], Loss: 7054.2783\n",
      "Epoch [5/500], Step [6/26], Loss: 6295.2124\n",
      "Epoch [5/500], Step [7/26], Loss: 6637.3374\n",
      "Epoch [5/500], Step [8/26], Loss: 4000.4819\n",
      "Epoch [5/500], Step [9/26], Loss: 4395.0059\n",
      "Epoch [5/500], Step [10/26], Loss: 5425.0742\n",
      "Epoch [5/500], Step [11/26], Loss: 3349.1240\n",
      "Epoch [5/500], Step [12/26], Loss: 6932.8760\n",
      "Epoch [5/500], Step [13/26], Loss: 4612.4497\n",
      "Epoch [5/500], Step [14/26], Loss: 7505.6025\n",
      "Epoch [5/500], Step [15/26], Loss: 8345.2842\n",
      "Epoch [5/500], Step [16/26], Loss: 11191.2500\n",
      "Epoch [5/500], Step [17/26], Loss: 4523.6226\n",
      "Epoch [5/500], Step [18/26], Loss: 6040.8008\n",
      "Epoch [5/500], Step [19/26], Loss: 4694.7544\n",
      "Epoch [5/500], Step [20/26], Loss: 5111.7163\n",
      "Epoch [5/500], Step [21/26], Loss: 7984.1514\n",
      "Epoch [5/500], Step [22/26], Loss: 4358.5864\n",
      "Epoch [5/500], Step [23/26], Loss: 5607.1392\n",
      "Epoch [5/500], Step [24/26], Loss: 7105.4072\n",
      "Epoch [5/500], Step [25/26], Loss: 5694.2524\n",
      "Epoch [5/500], Step [26/26], Loss: 3859.5776\n",
      "Epoch [6/500], Step [1/26], Loss: 4175.2485\n",
      "Epoch [6/500], Step [2/26], Loss: 10870.6523\n",
      "Epoch [6/500], Step [3/26], Loss: 4864.2305\n",
      "Epoch [6/500], Step [4/26], Loss: 5728.7217\n",
      "Epoch [6/500], Step [5/26], Loss: 4976.3403\n",
      "Epoch [6/500], Step [6/26], Loss: 4876.0234\n",
      "Epoch [6/500], Step [7/26], Loss: 7184.9902\n",
      "Epoch [6/500], Step [8/26], Loss: 8722.6445\n",
      "Epoch [6/500], Step [9/26], Loss: 5151.1045\n",
      "Epoch [6/500], Step [10/26], Loss: 6926.7065\n",
      "Epoch [6/500], Step [11/26], Loss: 4501.9771\n",
      "Epoch [6/500], Step [12/26], Loss: 6820.4194\n",
      "Epoch [6/500], Step [13/26], Loss: 6267.3828\n",
      "Epoch [6/500], Step [14/26], Loss: 6461.3291\n",
      "Epoch [6/500], Step [15/26], Loss: 6047.6831\n",
      "Epoch [6/500], Step [16/26], Loss: 2517.5271\n",
      "Epoch [6/500], Step [17/26], Loss: 3505.8518\n",
      "Epoch [6/500], Step [18/26], Loss: 7121.1851\n",
      "Epoch [6/500], Step [19/26], Loss: 4396.9814\n",
      "Epoch [6/500], Step [20/26], Loss: 6457.0996\n",
      "Epoch [6/500], Step [21/26], Loss: 4908.0459\n",
      "Epoch [6/500], Step [22/26], Loss: 3865.4395\n",
      "Epoch [6/500], Step [23/26], Loss: 5321.4800\n",
      "Epoch [6/500], Step [24/26], Loss: 6677.6870\n",
      "Epoch [6/500], Step [25/26], Loss: 4770.6401\n",
      "Epoch [6/500], Step [26/26], Loss: 13957.1680\n",
      "Epoch [7/500], Step [1/26], Loss: 3459.9556\n",
      "Epoch [7/500], Step [2/26], Loss: 5600.2939\n",
      "Epoch [7/500], Step [3/26], Loss: 5090.3916\n",
      "Epoch [7/500], Step [4/26], Loss: 6182.0596\n",
      "Epoch [7/500], Step [5/26], Loss: 4866.5933\n",
      "Epoch [7/500], Step [6/26], Loss: 4857.6670\n",
      "Epoch [7/500], Step [7/26], Loss: 4251.1665\n",
      "Epoch [7/500], Step [8/26], Loss: 9208.5898\n",
      "Epoch [7/500], Step [9/26], Loss: 4145.9932\n",
      "Epoch [7/500], Step [10/26], Loss: 6652.9419\n",
      "Epoch [7/500], Step [11/26], Loss: 3924.1345\n",
      "Epoch [7/500], Step [12/26], Loss: 4231.7715\n",
      "Epoch [7/500], Step [13/26], Loss: 7602.0439\n",
      "Epoch [7/500], Step [14/26], Loss: 7957.8936\n",
      "Epoch [7/500], Step [15/26], Loss: 4805.3838\n",
      "Epoch [7/500], Step [16/26], Loss: 6517.8037\n",
      "Epoch [7/500], Step [17/26], Loss: 6172.4668\n",
      "Epoch [7/500], Step [18/26], Loss: 5980.0107\n",
      "Epoch [7/500], Step [19/26], Loss: 3260.2012\n",
      "Epoch [7/500], Step [20/26], Loss: 4903.0864\n",
      "Epoch [7/500], Step [21/26], Loss: 5117.8638\n",
      "Epoch [7/500], Step [22/26], Loss: 3807.0193\n",
      "Epoch [7/500], Step [23/26], Loss: 6854.7402\n",
      "Epoch [7/500], Step [24/26], Loss: 7051.1533\n",
      "Epoch [7/500], Step [25/26], Loss: 4059.8425\n",
      "Epoch [7/500], Step [26/26], Loss: 10497.7969\n",
      "Epoch [8/500], Step [1/26], Loss: 7170.0518\n",
      "Epoch [8/500], Step [2/26], Loss: 4125.9839\n",
      "Epoch [8/500], Step [3/26], Loss: 9349.7588\n",
      "Epoch [8/500], Step [4/26], Loss: 5604.8242\n",
      "Epoch [8/500], Step [5/26], Loss: 3930.7217\n",
      "Epoch [8/500], Step [6/26], Loss: 3221.4307\n",
      "Epoch [8/500], Step [7/26], Loss: 6338.2598\n",
      "Epoch [8/500], Step [8/26], Loss: 6502.9023\n",
      "Epoch [8/500], Step [9/26], Loss: 3525.4880\n",
      "Epoch [8/500], Step [10/26], Loss: 6005.7393\n",
      "Epoch [8/500], Step [11/26], Loss: 4396.3789\n",
      "Epoch [8/500], Step [12/26], Loss: 3772.6230\n",
      "Epoch [8/500], Step [13/26], Loss: 6861.7725\n",
      "Epoch [8/500], Step [14/26], Loss: 5813.7393\n",
      "Epoch [8/500], Step [15/26], Loss: 5982.2358\n",
      "Epoch [8/500], Step [16/26], Loss: 3922.1475\n",
      "Epoch [8/500], Step [17/26], Loss: 2721.9839\n",
      "Epoch [8/500], Step [18/26], Loss: 4812.5200\n",
      "Epoch [8/500], Step [19/26], Loss: 5039.4937\n",
      "Epoch [8/500], Step [20/26], Loss: 5780.9648\n",
      "Epoch [8/500], Step [21/26], Loss: 5925.0869\n",
      "Epoch [8/500], Step [22/26], Loss: 4580.5884\n",
      "Epoch [8/500], Step [23/26], Loss: 2081.8184\n",
      "Epoch [8/500], Step [24/26], Loss: 2577.3667\n",
      "Epoch [8/500], Step [25/26], Loss: 5001.7852\n",
      "Epoch [8/500], Step [26/26], Loss: 10563.3320\n",
      "Epoch [9/500], Step [1/26], Loss: 5364.5020\n",
      "Epoch [9/500], Step [2/26], Loss: 4795.4170\n",
      "Epoch [9/500], Step [3/26], Loss: 4060.2173\n",
      "Epoch [9/500], Step [4/26], Loss: 3569.8220\n",
      "Epoch [9/500], Step [5/26], Loss: 3936.3218\n",
      "Epoch [9/500], Step [6/26], Loss: 4329.1562\n",
      "Epoch [9/500], Step [7/26], Loss: 3562.1597\n",
      "Epoch [9/500], Step [8/26], Loss: 6540.5566\n",
      "Epoch [9/500], Step [9/26], Loss: 5490.4897\n",
      "Epoch [9/500], Step [10/26], Loss: 9342.1416\n",
      "Epoch [9/500], Step [11/26], Loss: 6044.1768\n",
      "Epoch [9/500], Step [12/26], Loss: 5197.4370\n",
      "Epoch [9/500], Step [13/26], Loss: 5150.5527\n",
      "Epoch [9/500], Step [14/26], Loss: 5308.6787\n",
      "Epoch [9/500], Step [15/26], Loss: 3939.2197\n",
      "Epoch [9/500], Step [16/26], Loss: 7508.6475\n",
      "Epoch [9/500], Step [17/26], Loss: 1956.6871\n",
      "Epoch [9/500], Step [18/26], Loss: 4409.1924\n",
      "Epoch [9/500], Step [19/26], Loss: 6041.7256\n",
      "Epoch [9/500], Step [20/26], Loss: 4469.8853\n",
      "Epoch [9/500], Step [21/26], Loss: 4500.5596\n",
      "Epoch [9/500], Step [22/26], Loss: 4111.2256\n",
      "Epoch [9/500], Step [23/26], Loss: 5374.4424\n",
      "Epoch [9/500], Step [24/26], Loss: 4717.3330\n",
      "Epoch [9/500], Step [25/26], Loss: 4613.5688\n",
      "Epoch [9/500], Step [26/26], Loss: 8790.9062\n",
      "Epoch [10/500], Step [1/26], Loss: 4994.9663\n",
      "Epoch [10/500], Step [2/26], Loss: 5416.1514\n",
      "Epoch [10/500], Step [3/26], Loss: 5375.8804\n",
      "Epoch [10/500], Step [4/26], Loss: 4806.1348\n",
      "Epoch [10/500], Step [5/26], Loss: 5416.1006\n",
      "Epoch [10/500], Step [6/26], Loss: 4657.8760\n",
      "Epoch [10/500], Step [7/26], Loss: 5460.5020\n",
      "Epoch [10/500], Step [8/26], Loss: 3833.5762\n",
      "Epoch [10/500], Step [9/26], Loss: 2954.4595\n",
      "Epoch [10/500], Step [10/26], Loss: 4544.0537\n",
      "Epoch [10/500], Step [11/26], Loss: 5806.7842\n",
      "Epoch [10/500], Step [12/26], Loss: 6516.5928\n",
      "Epoch [10/500], Step [13/26], Loss: 6493.8271\n",
      "Epoch [10/500], Step [14/26], Loss: 5299.6006\n",
      "Epoch [10/500], Step [15/26], Loss: 3185.4155\n",
      "Epoch [10/500], Step [16/26], Loss: 5907.1016\n",
      "Epoch [10/500], Step [17/26], Loss: 6139.3062\n",
      "Epoch [10/500], Step [18/26], Loss: 4094.2693\n",
      "Epoch [10/500], Step [19/26], Loss: 2015.9958\n",
      "Epoch [10/500], Step [20/26], Loss: 5067.4902\n",
      "Epoch [10/500], Step [21/26], Loss: 4214.8574\n",
      "Epoch [10/500], Step [22/26], Loss: 6054.9043\n",
      "Epoch [10/500], Step [23/26], Loss: 3350.6123\n",
      "Epoch [10/500], Step [24/26], Loss: 6228.1084\n",
      "Epoch [10/500], Step [25/26], Loss: 6537.5713\n",
      "Epoch [10/500], Step [26/26], Loss: 4204.1548\n",
      "Epoch [11/500], Step [1/26], Loss: 4898.3662\n",
      "Epoch [11/500], Step [2/26], Loss: 4264.2988\n",
      "Epoch [11/500], Step [3/26], Loss: 3818.3818\n",
      "Epoch [11/500], Step [4/26], Loss: 1967.1764\n",
      "Epoch [11/500], Step [5/26], Loss: 5059.3418\n",
      "Epoch [11/500], Step [6/26], Loss: 5350.4839\n",
      "Epoch [11/500], Step [7/26], Loss: 2532.1782\n",
      "Epoch [11/500], Step [8/26], Loss: 3528.7786\n",
      "Epoch [11/500], Step [9/26], Loss: 4088.3491\n",
      "Epoch [11/500], Step [10/26], Loss: 4612.8271\n",
      "Epoch [11/500], Step [11/26], Loss: 5339.5874\n",
      "Epoch [11/500], Step [12/26], Loss: 5886.2632\n",
      "Epoch [11/500], Step [13/26], Loss: 2906.2197\n",
      "Epoch [11/500], Step [14/26], Loss: 4071.8875\n",
      "Epoch [11/500], Step [15/26], Loss: 7442.3525\n",
      "Epoch [11/500], Step [16/26], Loss: 4978.7334\n",
      "Epoch [11/500], Step [17/26], Loss: 4956.8345\n",
      "Epoch [11/500], Step [18/26], Loss: 5334.4531\n",
      "Epoch [11/500], Step [19/26], Loss: 4949.8652\n",
      "Epoch [11/500], Step [20/26], Loss: 4942.8701\n",
      "Epoch [11/500], Step [21/26], Loss: 3986.9441\n",
      "Epoch [11/500], Step [22/26], Loss: 5015.7236\n",
      "Epoch [11/500], Step [23/26], Loss: 4787.3882\n",
      "Epoch [11/500], Step [24/26], Loss: 5678.5479\n",
      "Epoch [11/500], Step [25/26], Loss: 5574.0459\n",
      "Epoch [11/500], Step [26/26], Loss: 13188.3555\n",
      "Epoch [12/500], Step [1/26], Loss: 3743.4744\n",
      "Epoch [12/500], Step [2/26], Loss: 3367.7109\n",
      "Epoch [12/500], Step [3/26], Loss: 4273.9067\n",
      "Epoch [12/500], Step [4/26], Loss: 5180.5225\n",
      "Epoch [12/500], Step [5/26], Loss: 5766.1963\n",
      "Epoch [12/500], Step [6/26], Loss: 6843.9443\n",
      "Epoch [12/500], Step [7/26], Loss: 6651.3726\n",
      "Epoch [12/500], Step [8/26], Loss: 2767.7524\n",
      "Epoch [12/500], Step [9/26], Loss: 7374.3652\n",
      "Epoch [12/500], Step [10/26], Loss: 3287.7188\n",
      "Epoch [12/500], Step [11/26], Loss: 4984.8667\n",
      "Epoch [12/500], Step [12/26], Loss: 6928.9219\n",
      "Epoch [12/500], Step [13/26], Loss: 2203.9146\n",
      "Epoch [12/500], Step [14/26], Loss: 5992.3003\n",
      "Epoch [12/500], Step [15/26], Loss: 5597.7314\n",
      "Epoch [12/500], Step [16/26], Loss: 2800.1077\n",
      "Epoch [12/500], Step [17/26], Loss: 2888.3682\n",
      "Epoch [12/500], Step [18/26], Loss: 4652.1553\n",
      "Epoch [12/500], Step [19/26], Loss: 4037.4768\n",
      "Epoch [12/500], Step [20/26], Loss: 5309.9912\n",
      "Epoch [12/500], Step [21/26], Loss: 4819.8330\n",
      "Epoch [12/500], Step [22/26], Loss: 3469.6860\n",
      "Epoch [12/500], Step [23/26], Loss: 5839.1816\n",
      "Epoch [12/500], Step [24/26], Loss: 2717.0168\n",
      "Epoch [12/500], Step [25/26], Loss: 4986.0122\n",
      "Epoch [12/500], Step [26/26], Loss: 6446.4419\n",
      "Epoch [13/500], Step [1/26], Loss: 6012.5898\n",
      "Epoch [13/500], Step [2/26], Loss: 2052.9983\n",
      "Epoch [13/500], Step [3/26], Loss: 3400.0068\n",
      "Epoch [13/500], Step [4/26], Loss: 3290.8726\n",
      "Epoch [13/500], Step [5/26], Loss: 3863.8950\n",
      "Epoch [13/500], Step [6/26], Loss: 5825.1699\n",
      "Epoch [13/500], Step [7/26], Loss: 4261.3965\n",
      "Epoch [13/500], Step [8/26], Loss: 4201.1562\n",
      "Epoch [13/500], Step [9/26], Loss: 5151.8911\n",
      "Epoch [13/500], Step [10/26], Loss: 2560.5005\n",
      "Epoch [13/500], Step [11/26], Loss: 3445.2310\n",
      "Epoch [13/500], Step [12/26], Loss: 6950.9937\n",
      "Epoch [13/500], Step [13/26], Loss: 4201.4751\n",
      "Epoch [13/500], Step [14/26], Loss: 4197.0962\n",
      "Epoch [13/500], Step [15/26], Loss: 4337.7432\n",
      "Epoch [13/500], Step [16/26], Loss: 6316.1963\n",
      "Epoch [13/500], Step [17/26], Loss: 6446.6270\n",
      "Epoch [13/500], Step [18/26], Loss: 3116.6841\n",
      "Epoch [13/500], Step [19/26], Loss: 5289.3350\n",
      "Epoch [13/500], Step [20/26], Loss: 1682.2319\n",
      "Epoch [13/500], Step [21/26], Loss: 4371.8179\n",
      "Epoch [13/500], Step [22/26], Loss: 3863.8721\n",
      "Epoch [13/500], Step [23/26], Loss: 3113.9810\n",
      "Epoch [13/500], Step [24/26], Loss: 3067.7969\n",
      "Epoch [13/500], Step [25/26], Loss: 6088.6514\n",
      "Epoch [13/500], Step [26/26], Loss: 3186.6350\n",
      "Epoch [14/500], Step [1/26], Loss: 3227.9688\n",
      "Epoch [14/500], Step [2/26], Loss: 5353.6748\n",
      "Epoch [14/500], Step [3/26], Loss: 3538.2949\n",
      "Epoch [14/500], Step [4/26], Loss: 3186.0691\n",
      "Epoch [14/500], Step [5/26], Loss: 5720.9409\n",
      "Epoch [14/500], Step [6/26], Loss: 6392.5747\n",
      "Epoch [14/500], Step [7/26], Loss: 2366.5686\n",
      "Epoch [14/500], Step [8/26], Loss: 4989.5312\n",
      "Epoch [14/500], Step [9/26], Loss: 6420.8818\n",
      "Epoch [14/500], Step [10/26], Loss: 3339.2551\n",
      "Epoch [14/500], Step [11/26], Loss: 4521.6382\n",
      "Epoch [14/500], Step [12/26], Loss: 4150.1665\n",
      "Epoch [14/500], Step [13/26], Loss: 7472.4287\n",
      "Epoch [14/500], Step [14/26], Loss: 4865.9785\n",
      "Epoch [14/500], Step [15/26], Loss: 6657.6646\n",
      "Epoch [14/500], Step [16/26], Loss: 4526.8242\n",
      "Epoch [14/500], Step [17/26], Loss: 3598.6313\n",
      "Epoch [14/500], Step [18/26], Loss: 7788.7764\n",
      "Epoch [14/500], Step [19/26], Loss: 2456.2231\n",
      "Epoch [14/500], Step [20/26], Loss: 5744.2480\n",
      "Epoch [14/500], Step [21/26], Loss: 2183.9436\n",
      "Epoch [14/500], Step [22/26], Loss: 5818.6523\n",
      "Epoch [14/500], Step [23/26], Loss: 4815.2642\n",
      "Epoch [14/500], Step [24/26], Loss: 3726.6255\n",
      "Epoch [14/500], Step [25/26], Loss: 3812.5088\n",
      "Epoch [14/500], Step [26/26], Loss: 2385.9009\n",
      "Epoch [15/500], Step [1/26], Loss: 5639.2305\n",
      "Epoch [15/500], Step [2/26], Loss: 3222.8127\n",
      "Epoch [15/500], Step [3/26], Loss: 4050.2307\n",
      "Epoch [15/500], Step [4/26], Loss: 5365.6802\n",
      "Epoch [15/500], Step [5/26], Loss: 6377.1045\n",
      "Epoch [15/500], Step [6/26], Loss: 4934.7378\n",
      "Epoch [15/500], Step [7/26], Loss: 3881.3320\n",
      "Epoch [15/500], Step [8/26], Loss: 3383.9419\n",
      "Epoch [15/500], Step [9/26], Loss: 5115.2148\n",
      "Epoch [15/500], Step [10/26], Loss: 3443.5200\n",
      "Epoch [15/500], Step [11/26], Loss: 2638.3572\n",
      "Epoch [15/500], Step [12/26], Loss: 2926.6646\n",
      "Epoch [15/500], Step [13/26], Loss: 6662.7788\n",
      "Epoch [15/500], Step [14/26], Loss: 3832.6992\n",
      "Epoch [15/500], Step [15/26], Loss: 3440.6079\n",
      "Epoch [15/500], Step [16/26], Loss: 3340.8457\n",
      "Epoch [15/500], Step [17/26], Loss: 8614.4834\n",
      "Epoch [15/500], Step [18/26], Loss: 3542.5581\n",
      "Epoch [15/500], Step [19/26], Loss: 3183.8877\n",
      "Epoch [15/500], Step [20/26], Loss: 4196.6367\n",
      "Epoch [15/500], Step [21/26], Loss: 5720.6055\n",
      "Epoch [15/500], Step [22/26], Loss: 5225.8467\n",
      "Epoch [15/500], Step [23/26], Loss: 5654.1460\n",
      "Epoch [15/500], Step [24/26], Loss: 3802.1826\n",
      "Epoch [15/500], Step [25/26], Loss: 4300.5098\n",
      "Epoch [15/500], Step [26/26], Loss: 1359.1847\n",
      "Epoch [16/500], Step [1/26], Loss: 5872.4175\n",
      "Epoch [16/500], Step [2/26], Loss: 1775.3289\n",
      "Epoch [16/500], Step [3/26], Loss: 3742.5801\n",
      "Epoch [16/500], Step [4/26], Loss: 3937.7324\n",
      "Epoch [16/500], Step [5/26], Loss: 2160.4368\n",
      "Epoch [16/500], Step [6/26], Loss: 5074.5703\n",
      "Epoch [16/500], Step [7/26], Loss: 4973.3062\n",
      "Epoch [16/500], Step [8/26], Loss: 3290.5112\n",
      "Epoch [16/500], Step [9/26], Loss: 4268.9785\n",
      "Epoch [16/500], Step [10/26], Loss: 3839.8262\n",
      "Epoch [16/500], Step [11/26], Loss: 4243.9409\n",
      "Epoch [16/500], Step [12/26], Loss: 3548.0479\n",
      "Epoch [16/500], Step [13/26], Loss: 5238.0972\n",
      "Epoch [16/500], Step [14/26], Loss: 4316.2261\n",
      "Epoch [16/500], Step [15/26], Loss: 5958.4946\n",
      "Epoch [16/500], Step [16/26], Loss: 2171.8206\n",
      "Epoch [16/500], Step [17/26], Loss: 6026.2466\n",
      "Epoch [16/500], Step [18/26], Loss: 3081.0532\n",
      "Epoch [16/500], Step [19/26], Loss: 3080.1665\n",
      "Epoch [16/500], Step [20/26], Loss: 3764.2898\n",
      "Epoch [16/500], Step [21/26], Loss: 6617.4951\n",
      "Epoch [16/500], Step [22/26], Loss: 2600.5615\n",
      "Epoch [16/500], Step [23/26], Loss: 8065.1250\n",
      "Epoch [16/500], Step [24/26], Loss: 5529.2900\n",
      "Epoch [16/500], Step [25/26], Loss: 3410.6399\n",
      "Epoch [16/500], Step [26/26], Loss: 2001.6155\n",
      "Epoch [17/500], Step [1/26], Loss: 2063.9873\n",
      "Epoch [17/500], Step [2/26], Loss: 5501.4536\n",
      "Epoch [17/500], Step [3/26], Loss: 4251.9673\n",
      "Epoch [17/500], Step [4/26], Loss: 2004.6754\n",
      "Epoch [17/500], Step [5/26], Loss: 2687.4050\n",
      "Epoch [17/500], Step [6/26], Loss: 3711.7891\n",
      "Epoch [17/500], Step [7/26], Loss: 4616.3462\n",
      "Epoch [17/500], Step [8/26], Loss: 3855.9810\n",
      "Epoch [17/500], Step [9/26], Loss: 3513.4395\n",
      "Epoch [17/500], Step [10/26], Loss: 3529.2676\n",
      "Epoch [17/500], Step [11/26], Loss: 4190.7598\n",
      "Epoch [17/500], Step [12/26], Loss: 3848.0754\n",
      "Epoch [17/500], Step [13/26], Loss: 5125.9312\n",
      "Epoch [17/500], Step [14/26], Loss: 1739.4438\n",
      "Epoch [17/500], Step [15/26], Loss: 3671.8779\n",
      "Epoch [17/500], Step [16/26], Loss: 4000.3752\n",
      "Epoch [17/500], Step [17/26], Loss: 5404.2095\n",
      "Epoch [17/500], Step [18/26], Loss: 3199.7537\n",
      "Epoch [17/500], Step [19/26], Loss: 6602.2583\n",
      "Epoch [17/500], Step [20/26], Loss: 3441.3513\n",
      "Epoch [17/500], Step [21/26], Loss: 4138.7041\n",
      "Epoch [17/500], Step [22/26], Loss: 2865.9497\n",
      "Epoch [17/500], Step [23/26], Loss: 5727.3154\n",
      "Epoch [17/500], Step [24/26], Loss: 3545.2380\n",
      "Epoch [17/500], Step [25/26], Loss: 2032.8638\n",
      "Epoch [17/500], Step [26/26], Loss: 5119.6968\n",
      "Epoch [18/500], Step [1/26], Loss: 2796.5420\n",
      "Epoch [18/500], Step [2/26], Loss: 6514.4795\n",
      "Epoch [18/500], Step [3/26], Loss: 2467.0969\n",
      "Epoch [18/500], Step [4/26], Loss: 2507.5076\n",
      "Epoch [18/500], Step [5/26], Loss: 5128.4639\n",
      "Epoch [18/500], Step [6/26], Loss: 2431.8596\n",
      "Epoch [18/500], Step [7/26], Loss: 1763.4265\n",
      "Epoch [18/500], Step [8/26], Loss: 4976.7725\n",
      "Epoch [18/500], Step [9/26], Loss: 6093.5659\n",
      "Epoch [18/500], Step [10/26], Loss: 2968.0684\n",
      "Epoch [18/500], Step [11/26], Loss: 3313.3967\n",
      "Epoch [18/500], Step [12/26], Loss: 3693.7329\n",
      "Epoch [18/500], Step [13/26], Loss: 5692.3379\n",
      "Epoch [18/500], Step [14/26], Loss: 3539.1707\n",
      "Epoch [18/500], Step [15/26], Loss: 5678.1299\n",
      "Epoch [18/500], Step [16/26], Loss: 3979.1260\n",
      "Epoch [18/500], Step [17/26], Loss: 4483.3267\n",
      "Epoch [18/500], Step [18/26], Loss: 3847.4507\n",
      "Epoch [18/500], Step [19/26], Loss: 3611.0303\n",
      "Epoch [18/500], Step [20/26], Loss: 3124.2896\n",
      "Epoch [18/500], Step [21/26], Loss: 4267.5869\n",
      "Epoch [18/500], Step [22/26], Loss: 5227.5103\n",
      "Epoch [18/500], Step [23/26], Loss: 2482.2708\n",
      "Epoch [18/500], Step [24/26], Loss: 8630.6680\n",
      "Epoch [18/500], Step [25/26], Loss: 2499.3994\n",
      "Epoch [18/500], Step [26/26], Loss: 1348.1501\n",
      "Epoch [19/500], Step [1/26], Loss: 2484.2751\n",
      "Epoch [19/500], Step [2/26], Loss: 2457.5000\n",
      "Epoch [19/500], Step [3/26], Loss: 3171.5454\n",
      "Epoch [19/500], Step [4/26], Loss: 3432.1067\n",
      "Epoch [19/500], Step [5/26], Loss: 4880.9321\n",
      "Epoch [19/500], Step [6/26], Loss: 2797.9119\n",
      "Epoch [19/500], Step [7/26], Loss: 2853.5190\n",
      "Epoch [19/500], Step [8/26], Loss: 5770.0229\n",
      "Epoch [19/500], Step [9/26], Loss: 2287.9348\n",
      "Epoch [19/500], Step [10/26], Loss: 3509.4941\n",
      "Epoch [19/500], Step [11/26], Loss: 4675.4170\n",
      "Epoch [19/500], Step [12/26], Loss: 4831.6816\n",
      "Epoch [19/500], Step [13/26], Loss: 3374.0662\n",
      "Epoch [19/500], Step [14/26], Loss: 1246.5366\n",
      "Epoch [19/500], Step [15/26], Loss: 4273.8989\n",
      "Epoch [19/500], Step [16/26], Loss: 2788.8042\n",
      "Epoch [19/500], Step [17/26], Loss: 2964.3286\n",
      "Epoch [19/500], Step [18/26], Loss: 5569.5630\n",
      "Epoch [19/500], Step [19/26], Loss: 3797.7231\n",
      "Epoch [19/500], Step [20/26], Loss: 2803.9609\n",
      "Epoch [19/500], Step [21/26], Loss: 5128.1055\n",
      "Epoch [19/500], Step [22/26], Loss: 3358.3247\n",
      "Epoch [19/500], Step [23/26], Loss: 4571.9834\n",
      "Epoch [19/500], Step [24/26], Loss: 5788.0190\n",
      "Epoch [19/500], Step [25/26], Loss: 5620.2573\n",
      "Epoch [19/500], Step [26/26], Loss: 715.8751\n",
      "Epoch [20/500], Step [1/26], Loss: 3608.8315\n",
      "Epoch [20/500], Step [2/26], Loss: 3751.6343\n",
      "Epoch [20/500], Step [3/26], Loss: 3255.4412\n",
      "Epoch [20/500], Step [4/26], Loss: 4766.1011\n",
      "Epoch [20/500], Step [5/26], Loss: 2839.8403\n",
      "Epoch [20/500], Step [6/26], Loss: 3596.8379\n",
      "Epoch [20/500], Step [7/26], Loss: 5792.0293\n",
      "Epoch [20/500], Step [8/26], Loss: 2726.2947\n",
      "Epoch [20/500], Step [9/26], Loss: 3079.2588\n",
      "Epoch [20/500], Step [10/26], Loss: 6234.1797\n",
      "Epoch [20/500], Step [11/26], Loss: 2186.9639\n",
      "Epoch [20/500], Step [12/26], Loss: 5740.3711\n",
      "Epoch [20/500], Step [13/26], Loss: 2161.7625\n",
      "Epoch [20/500], Step [14/26], Loss: 3300.2993\n",
      "Epoch [20/500], Step [15/26], Loss: 3830.1006\n",
      "Epoch [20/500], Step [16/26], Loss: 5670.9023\n",
      "Epoch [20/500], Step [17/26], Loss: 4821.8940\n",
      "Epoch [20/500], Step [18/26], Loss: 3469.8179\n",
      "Epoch [20/500], Step [19/26], Loss: 3040.3760\n",
      "Epoch [20/500], Step [20/26], Loss: 1364.9556\n",
      "Epoch [20/500], Step [21/26], Loss: 3638.9404\n",
      "Epoch [20/500], Step [22/26], Loss: 2417.4551\n",
      "Epoch [20/500], Step [23/26], Loss: 1609.8792\n",
      "Epoch [20/500], Step [24/26], Loss: 3806.8979\n",
      "Epoch [20/500], Step [25/26], Loss: 5444.1084\n",
      "Epoch [20/500], Step [26/26], Loss: 1625.1681\n",
      "Epoch [21/500], Step [1/26], Loss: 3985.0757\n",
      "Epoch [21/500], Step [2/26], Loss: 4095.5454\n",
      "Epoch [21/500], Step [3/26], Loss: 3984.6682\n",
      "Epoch [21/500], Step [4/26], Loss: 3260.4431\n",
      "Epoch [21/500], Step [5/26], Loss: 2541.0913\n",
      "Epoch [21/500], Step [6/26], Loss: 3065.8142\n",
      "Epoch [21/500], Step [7/26], Loss: 7025.7979\n",
      "Epoch [21/500], Step [8/26], Loss: 2795.2959\n",
      "Epoch [21/500], Step [9/26], Loss: 3846.3372\n",
      "Epoch [21/500], Step [10/26], Loss: 3668.8521\n",
      "Epoch [21/500], Step [11/26], Loss: 6978.0732\n",
      "Epoch [21/500], Step [12/26], Loss: 3993.8789\n",
      "Epoch [21/500], Step [13/26], Loss: 7061.1455\n",
      "Epoch [21/500], Step [14/26], Loss: 4288.3188\n",
      "Epoch [21/500], Step [15/26], Loss: 2020.6733\n",
      "Epoch [21/500], Step [16/26], Loss: 2280.6196\n",
      "Epoch [21/500], Step [17/26], Loss: 2826.7935\n",
      "Epoch [21/500], Step [18/26], Loss: 2926.4883\n",
      "Epoch [21/500], Step [19/26], Loss: 5656.8042\n",
      "Epoch [21/500], Step [20/26], Loss: 2159.0908\n",
      "Epoch [21/500], Step [21/26], Loss: 6968.7725\n",
      "Epoch [21/500], Step [22/26], Loss: 1414.2985\n",
      "Epoch [21/500], Step [23/26], Loss: 3382.6711\n",
      "Epoch [21/500], Step [24/26], Loss: 1361.9746\n",
      "Epoch [21/500], Step [25/26], Loss: 4000.8284\n",
      "Epoch [21/500], Step [26/26], Loss: 2321.1047\n",
      "Epoch [22/500], Step [1/26], Loss: 4355.0947\n",
      "Epoch [22/500], Step [2/26], Loss: 6166.0244\n",
      "Epoch [22/500], Step [3/26], Loss: 4816.5293\n",
      "Epoch [22/500], Step [4/26], Loss: 3184.2175\n",
      "Epoch [22/500], Step [5/26], Loss: 2275.2339\n",
      "Epoch [22/500], Step [6/26], Loss: 1418.1503\n",
      "Epoch [22/500], Step [7/26], Loss: 1847.2604\n",
      "Epoch [22/500], Step [8/26], Loss: 5472.2852\n",
      "Epoch [22/500], Step [9/26], Loss: 2604.8442\n",
      "Epoch [22/500], Step [10/26], Loss: 3540.4731\n",
      "Epoch [22/500], Step [11/26], Loss: 5472.7183\n",
      "Epoch [22/500], Step [12/26], Loss: 5089.5142\n",
      "Epoch [22/500], Step [13/26], Loss: 5816.3438\n",
      "Epoch [22/500], Step [14/26], Loss: 3175.0806\n",
      "Epoch [22/500], Step [15/26], Loss: 3658.7720\n",
      "Epoch [22/500], Step [16/26], Loss: 2928.8403\n",
      "Epoch [22/500], Step [17/26], Loss: 3529.3318\n",
      "Epoch [22/500], Step [18/26], Loss: 3108.3152\n",
      "Epoch [22/500], Step [19/26], Loss: 3759.6470\n",
      "Epoch [22/500], Step [20/26], Loss: 3733.4575\n",
      "Epoch [22/500], Step [21/26], Loss: 2784.0647\n",
      "Epoch [22/500], Step [22/26], Loss: 2773.2847\n",
      "Epoch [22/500], Step [23/26], Loss: 5714.0474\n",
      "Epoch [22/500], Step [24/26], Loss: 2660.1387\n",
      "Epoch [22/500], Step [25/26], Loss: 2405.5454\n",
      "Epoch [22/500], Step [26/26], Loss: 2302.0088\n",
      "Epoch [23/500], Step [1/26], Loss: 4034.2263\n",
      "Epoch [23/500], Step [2/26], Loss: 4679.3618\n",
      "Epoch [23/500], Step [3/26], Loss: 3558.1255\n",
      "Epoch [23/500], Step [4/26], Loss: 6754.5010\n",
      "Epoch [23/500], Step [5/26], Loss: 2563.9814\n",
      "Epoch [23/500], Step [6/26], Loss: 3774.7422\n",
      "Epoch [23/500], Step [7/26], Loss: 4775.1372\n",
      "Epoch [23/500], Step [8/26], Loss: 1859.1215\n",
      "Epoch [23/500], Step [9/26], Loss: 3985.9578\n",
      "Epoch [23/500], Step [10/26], Loss: 1980.3032\n",
      "Epoch [23/500], Step [11/26], Loss: 2808.6472\n",
      "Epoch [23/500], Step [12/26], Loss: 3648.7849\n",
      "Epoch [23/500], Step [13/26], Loss: 2487.3862\n",
      "Epoch [23/500], Step [14/26], Loss: 2511.4004\n",
      "Epoch [23/500], Step [15/26], Loss: 2924.9985\n",
      "Epoch [23/500], Step [16/26], Loss: 2431.0605\n",
      "Epoch [23/500], Step [17/26], Loss: 6439.8477\n",
      "Epoch [23/500], Step [18/26], Loss: 3924.0940\n",
      "Epoch [23/500], Step [19/26], Loss: 3083.2712\n",
      "Epoch [23/500], Step [20/26], Loss: 3785.9968\n",
      "Epoch [23/500], Step [21/26], Loss: 3452.0190\n",
      "Epoch [23/500], Step [22/26], Loss: 2402.5496\n",
      "Epoch [23/500], Step [23/26], Loss: 3687.2498\n",
      "Epoch [23/500], Step [24/26], Loss: 3957.2356\n",
      "Epoch [23/500], Step [25/26], Loss: 4885.2080\n",
      "Epoch [23/500], Step [26/26], Loss: 10713.0693\n",
      "Epoch [24/500], Step [1/26], Loss: 3018.4646\n",
      "Epoch [24/500], Step [2/26], Loss: 3769.9231\n",
      "Epoch [24/500], Step [3/26], Loss: 3637.7749\n",
      "Epoch [24/500], Step [4/26], Loss: 3922.7466\n",
      "Epoch [24/500], Step [5/26], Loss: 3621.8477\n",
      "Epoch [24/500], Step [6/26], Loss: 4142.4609\n",
      "Epoch [24/500], Step [7/26], Loss: 3157.5962\n",
      "Epoch [24/500], Step [8/26], Loss: 6725.2285\n",
      "Epoch [24/500], Step [9/26], Loss: 3593.2427\n",
      "Epoch [24/500], Step [10/26], Loss: 2825.0046\n",
      "Epoch [24/500], Step [11/26], Loss: 3014.1091\n",
      "Epoch [24/500], Step [12/26], Loss: 1132.0592\n",
      "Epoch [24/500], Step [13/26], Loss: 4616.0264\n",
      "Epoch [24/500], Step [14/26], Loss: 5692.4238\n",
      "Epoch [24/500], Step [15/26], Loss: 4985.1860\n",
      "Epoch [24/500], Step [16/26], Loss: 3076.8765\n",
      "Epoch [24/500], Step [17/26], Loss: 4054.9094\n",
      "Epoch [24/500], Step [18/26], Loss: 2515.2009\n",
      "Epoch [24/500], Step [19/26], Loss: 4441.2002\n",
      "Epoch [24/500], Step [20/26], Loss: 1819.7148\n",
      "Epoch [24/500], Step [21/26], Loss: 2051.8870\n",
      "Epoch [24/500], Step [22/26], Loss: 3692.9473\n",
      "Epoch [24/500], Step [23/26], Loss: 2987.3550\n",
      "Epoch [24/500], Step [24/26], Loss: 3095.9961\n",
      "Epoch [24/500], Step [25/26], Loss: 3406.2578\n",
      "Epoch [24/500], Step [26/26], Loss: 1351.5623\n",
      "Epoch [25/500], Step [1/26], Loss: 3143.2075\n",
      "Epoch [25/500], Step [2/26], Loss: 1607.1335\n",
      "Epoch [25/500], Step [3/26], Loss: 4753.1377\n",
      "Epoch [25/500], Step [4/26], Loss: 4468.7231\n",
      "Epoch [25/500], Step [5/26], Loss: 1090.5691\n",
      "Epoch [25/500], Step [6/26], Loss: 3142.6260\n",
      "Epoch [25/500], Step [7/26], Loss: 3697.5784\n",
      "Epoch [25/500], Step [8/26], Loss: 2930.5532\n",
      "Epoch [25/500], Step [9/26], Loss: 1787.7810\n",
      "Epoch [25/500], Step [10/26], Loss: 4233.7651\n",
      "Epoch [25/500], Step [11/26], Loss: 5991.2070\n",
      "Epoch [25/500], Step [12/26], Loss: 6050.6553\n",
      "Epoch [25/500], Step [13/26], Loss: 4033.1199\n",
      "Epoch [25/500], Step [14/26], Loss: 4455.3911\n",
      "Epoch [25/500], Step [15/26], Loss: 1727.9178\n",
      "Epoch [25/500], Step [16/26], Loss: 5345.6118\n",
      "Epoch [25/500], Step [17/26], Loss: 3727.0254\n",
      "Epoch [25/500], Step [18/26], Loss: 3614.7224\n",
      "Epoch [25/500], Step [19/26], Loss: 2024.2351\n",
      "Epoch [25/500], Step [20/26], Loss: 2645.8862\n",
      "Epoch [25/500], Step [21/26], Loss: 2636.5676\n",
      "Epoch [25/500], Step [22/26], Loss: 2313.5955\n",
      "Epoch [25/500], Step [23/26], Loss: 3652.9082\n",
      "Epoch [25/500], Step [24/26], Loss: 5493.2910\n",
      "Epoch [25/500], Step [25/26], Loss: 8700.6318\n",
      "Epoch [25/500], Step [26/26], Loss: 5773.4429\n",
      "Epoch [26/500], Step [1/26], Loss: 2154.8428\n",
      "Epoch [26/500], Step [2/26], Loss: 3845.0530\n",
      "Epoch [26/500], Step [3/26], Loss: 7984.8867\n",
      "Epoch [26/500], Step [4/26], Loss: 6229.1621\n",
      "Epoch [26/500], Step [5/26], Loss: 3936.8452\n",
      "Epoch [26/500], Step [6/26], Loss: 4814.0850\n",
      "Epoch [26/500], Step [7/26], Loss: 3075.9900\n",
      "Epoch [26/500], Step [8/26], Loss: 2370.7683\n",
      "Epoch [26/500], Step [9/26], Loss: 4217.3970\n",
      "Epoch [26/500], Step [10/26], Loss: 831.5579\n",
      "Epoch [26/500], Step [11/26], Loss: 3713.8059\n",
      "Epoch [26/500], Step [12/26], Loss: 3775.1638\n",
      "Epoch [26/500], Step [13/26], Loss: 1984.8362\n",
      "Epoch [26/500], Step [14/26], Loss: 1876.4528\n",
      "Epoch [26/500], Step [15/26], Loss: 2186.2603\n",
      "Epoch [26/500], Step [16/26], Loss: 3962.8901\n",
      "Epoch [26/500], Step [17/26], Loss: 2041.6471\n",
      "Epoch [26/500], Step [18/26], Loss: 1657.4775\n",
      "Epoch [26/500], Step [19/26], Loss: 3016.3286\n",
      "Epoch [26/500], Step [20/26], Loss: 3175.0374\n",
      "Epoch [26/500], Step [21/26], Loss: 4524.4360\n",
      "Epoch [26/500], Step [22/26], Loss: 5958.4795\n",
      "Epoch [26/500], Step [23/26], Loss: 4268.8818\n",
      "Epoch [26/500], Step [24/26], Loss: 3391.4756\n",
      "Epoch [26/500], Step [25/26], Loss: 3287.6616\n",
      "Epoch [26/500], Step [26/26], Loss: 2035.4362\n",
      "Epoch [27/500], Step [1/26], Loss: 3772.7671\n",
      "Epoch [27/500], Step [2/26], Loss: 3791.8159\n",
      "Epoch [27/500], Step [3/26], Loss: 4340.6768\n",
      "Epoch [27/500], Step [4/26], Loss: 3918.5825\n",
      "Epoch [27/500], Step [5/26], Loss: 2762.6406\n",
      "Epoch [27/500], Step [6/26], Loss: 5265.8667\n",
      "Epoch [27/500], Step [7/26], Loss: 4459.7314\n",
      "Epoch [27/500], Step [8/26], Loss: 3334.8821\n",
      "Epoch [27/500], Step [9/26], Loss: 3636.0400\n",
      "Epoch [27/500], Step [10/26], Loss: 2843.7905\n",
      "Epoch [27/500], Step [11/26], Loss: 2330.1494\n",
      "Epoch [27/500], Step [12/26], Loss: 2294.7896\n",
      "Epoch [27/500], Step [13/26], Loss: 2427.5674\n",
      "Epoch [27/500], Step [14/26], Loss: 3287.0232\n",
      "Epoch [27/500], Step [15/26], Loss: 4203.4146\n",
      "Epoch [27/500], Step [16/26], Loss: 4267.3564\n",
      "Epoch [27/500], Step [17/26], Loss: 3071.0027\n",
      "Epoch [27/500], Step [18/26], Loss: 2716.3154\n",
      "Epoch [27/500], Step [19/26], Loss: 3137.9595\n",
      "Epoch [27/500], Step [20/26], Loss: 2076.1211\n",
      "Epoch [27/500], Step [21/26], Loss: 3881.4028\n",
      "Epoch [27/500], Step [22/26], Loss: 2526.0667\n",
      "Epoch [27/500], Step [23/26], Loss: 5995.5771\n",
      "Epoch [27/500], Step [24/26], Loss: 2386.6401\n",
      "Epoch [27/500], Step [25/26], Loss: 2879.2424\n",
      "Epoch [27/500], Step [26/26], Loss: 1895.9680\n",
      "Epoch [28/500], Step [1/26], Loss: 2636.6135\n",
      "Epoch [28/500], Step [2/26], Loss: 3383.4038\n",
      "Epoch [28/500], Step [3/26], Loss: 2307.3164\n",
      "Epoch [28/500], Step [4/26], Loss: 2526.5083\n",
      "Epoch [28/500], Step [5/26], Loss: 2206.2087\n",
      "Epoch [28/500], Step [6/26], Loss: 2671.2290\n",
      "Epoch [28/500], Step [7/26], Loss: 2805.0659\n",
      "Epoch [28/500], Step [8/26], Loss: 1732.5988\n",
      "Epoch [28/500], Step [9/26], Loss: 3990.0386\n",
      "Epoch [28/500], Step [10/26], Loss: 2451.1172\n",
      "Epoch [28/500], Step [11/26], Loss: 3223.3643\n",
      "Epoch [28/500], Step [12/26], Loss: 2494.8845\n",
      "Epoch [28/500], Step [13/26], Loss: 2703.7522\n",
      "Epoch [28/500], Step [14/26], Loss: 3949.7139\n",
      "Epoch [28/500], Step [15/26], Loss: 3126.5220\n",
      "Epoch [28/500], Step [16/26], Loss: 3431.0461\n",
      "Epoch [28/500], Step [17/26], Loss: 4092.7463\n",
      "Epoch [28/500], Step [18/26], Loss: 3385.1257\n",
      "Epoch [28/500], Step [19/26], Loss: 4746.0801\n",
      "Epoch [28/500], Step [20/26], Loss: 3249.0435\n",
      "Epoch [28/500], Step [21/26], Loss: 2999.2532\n",
      "Epoch [28/500], Step [22/26], Loss: 1672.1331\n",
      "Epoch [28/500], Step [23/26], Loss: 4084.2041\n",
      "Epoch [28/500], Step [24/26], Loss: 5289.2461\n",
      "Epoch [28/500], Step [25/26], Loss: 4234.0552\n",
      "Epoch [28/500], Step [26/26], Loss: 5551.9814\n",
      "Epoch [29/500], Step [1/26], Loss: 3364.0068\n",
      "Epoch [29/500], Step [2/26], Loss: 6292.1699\n",
      "Epoch [29/500], Step [3/26], Loss: 3338.7007\n",
      "Epoch [29/500], Step [4/26], Loss: 4805.7886\n",
      "Epoch [29/500], Step [5/26], Loss: 3591.8616\n",
      "Epoch [29/500], Step [6/26], Loss: 3357.2671\n",
      "Epoch [29/500], Step [7/26], Loss: 2884.0071\n",
      "Epoch [29/500], Step [8/26], Loss: 2320.7424\n",
      "Epoch [29/500], Step [9/26], Loss: 3652.6489\n",
      "Epoch [29/500], Step [10/26], Loss: 5195.5410\n",
      "Epoch [29/500], Step [11/26], Loss: 3227.0420\n",
      "Epoch [29/500], Step [12/26], Loss: 2386.0693\n",
      "Epoch [29/500], Step [13/26], Loss: 3620.0190\n",
      "Epoch [29/500], Step [14/26], Loss: 1423.3221\n",
      "Epoch [29/500], Step [15/26], Loss: 4092.8647\n",
      "Epoch [29/500], Step [16/26], Loss: 2426.7812\n",
      "Epoch [29/500], Step [17/26], Loss: 3936.5386\n",
      "Epoch [29/500], Step [18/26], Loss: 2355.7329\n",
      "Epoch [29/500], Step [19/26], Loss: 3499.9573\n",
      "Epoch [29/500], Step [20/26], Loss: 4578.5361\n",
      "Epoch [29/500], Step [21/26], Loss: 5067.1777\n",
      "Epoch [29/500], Step [22/26], Loss: 2082.5010\n",
      "Epoch [29/500], Step [23/26], Loss: 2440.8848\n",
      "Epoch [29/500], Step [24/26], Loss: 2939.0725\n",
      "Epoch [29/500], Step [25/26], Loss: 1890.0776\n",
      "Epoch [29/500], Step [26/26], Loss: 767.5512\n",
      "Epoch [30/500], Step [1/26], Loss: 5411.7603\n",
      "Epoch [30/500], Step [2/26], Loss: 2098.8318\n",
      "Epoch [30/500], Step [3/26], Loss: 2414.6165\n",
      "Epoch [30/500], Step [4/26], Loss: 2586.0593\n",
      "Epoch [30/500], Step [5/26], Loss: 4090.0227\n",
      "Epoch [30/500], Step [6/26], Loss: 4726.3804\n",
      "Epoch [30/500], Step [7/26], Loss: 1942.2104\n",
      "Epoch [30/500], Step [8/26], Loss: 2585.2637\n",
      "Epoch [30/500], Step [9/26], Loss: 1376.4180\n",
      "Epoch [30/500], Step [10/26], Loss: 1988.0210\n",
      "Epoch [30/500], Step [11/26], Loss: 2484.8701\n",
      "Epoch [30/500], Step [12/26], Loss: 3498.3523\n",
      "Epoch [30/500], Step [13/26], Loss: 2092.2500\n",
      "Epoch [30/500], Step [14/26], Loss: 4677.9980\n",
      "Epoch [30/500], Step [15/26], Loss: 3798.1899\n",
      "Epoch [30/500], Step [16/26], Loss: 2223.7642\n",
      "Epoch [30/500], Step [17/26], Loss: 2939.1416\n",
      "Epoch [30/500], Step [18/26], Loss: 4495.1367\n",
      "Epoch [30/500], Step [19/26], Loss: 2973.5552\n",
      "Epoch [30/500], Step [20/26], Loss: 3812.6270\n",
      "Epoch [30/500], Step [21/26], Loss: 3137.5371\n",
      "Epoch [30/500], Step [22/26], Loss: 1279.2122\n",
      "Epoch [30/500], Step [23/26], Loss: 1991.2842\n",
      "Epoch [30/500], Step [24/26], Loss: 2045.6168\n",
      "Epoch [30/500], Step [25/26], Loss: 1958.0210\n",
      "Epoch [30/500], Step [26/26], Loss: 5058.2383\n",
      "Epoch [31/500], Step [1/26], Loss: 2047.6121\n",
      "Epoch [31/500], Step [2/26], Loss: 1297.3308\n",
      "Epoch [31/500], Step [3/26], Loss: 2486.6709\n",
      "Epoch [31/500], Step [4/26], Loss: 2905.4600\n",
      "Epoch [31/500], Step [5/26], Loss: 1548.0562\n",
      "Epoch [31/500], Step [6/26], Loss: 2402.3491\n",
      "Epoch [31/500], Step [7/26], Loss: 2373.5291\n",
      "Epoch [31/500], Step [8/26], Loss: 3714.9746\n",
      "Epoch [31/500], Step [9/26], Loss: 3419.6841\n",
      "Epoch [31/500], Step [10/26], Loss: 3846.3938\n",
      "Epoch [31/500], Step [11/26], Loss: 3849.3223\n",
      "Epoch [31/500], Step [12/26], Loss: 1641.1641\n",
      "Epoch [31/500], Step [13/26], Loss: 1536.7469\n",
      "Epoch [31/500], Step [14/26], Loss: 4924.7705\n",
      "Epoch [31/500], Step [15/26], Loss: 2741.4658\n",
      "Epoch [31/500], Step [16/26], Loss: 1632.4091\n",
      "Epoch [31/500], Step [17/26], Loss: 2894.7285\n",
      "Epoch [31/500], Step [18/26], Loss: 4811.9727\n",
      "Epoch [31/500], Step [19/26], Loss: 4797.2168\n",
      "Epoch [31/500], Step [20/26], Loss: 6178.7314\n",
      "Epoch [31/500], Step [21/26], Loss: 2523.8735\n",
      "Epoch [31/500], Step [22/26], Loss: 3366.3826\n",
      "Epoch [31/500], Step [23/26], Loss: 3225.0657\n",
      "Epoch [31/500], Step [24/26], Loss: 3184.8723\n",
      "Epoch [31/500], Step [25/26], Loss: 3860.3701\n",
      "Epoch [31/500], Step [26/26], Loss: 1245.2371\n",
      "Epoch [32/500], Step [1/26], Loss: 4051.1440\n",
      "Epoch [32/500], Step [2/26], Loss: 1841.9983\n",
      "Epoch [32/500], Step [3/26], Loss: 2690.8960\n",
      "Epoch [32/500], Step [4/26], Loss: 2697.4346\n",
      "Epoch [32/500], Step [5/26], Loss: 1696.4369\n",
      "Epoch [32/500], Step [6/26], Loss: 4019.4744\n",
      "Epoch [32/500], Step [7/26], Loss: 1718.6498\n",
      "Epoch [32/500], Step [8/26], Loss: 4962.3779\n",
      "Epoch [32/500], Step [9/26], Loss: 3350.5330\n",
      "Epoch [32/500], Step [10/26], Loss: 3931.7808\n",
      "Epoch [32/500], Step [11/26], Loss: 2107.3047\n",
      "Epoch [32/500], Step [12/26], Loss: 3377.7800\n",
      "Epoch [32/500], Step [13/26], Loss: 3400.9106\n",
      "Epoch [32/500], Step [14/26], Loss: 3080.5166\n",
      "Epoch [32/500], Step [15/26], Loss: 5182.1484\n",
      "Epoch [32/500], Step [16/26], Loss: 3034.7717\n",
      "Epoch [32/500], Step [17/26], Loss: 2750.1479\n",
      "Epoch [32/500], Step [18/26], Loss: 1778.7686\n",
      "Epoch [32/500], Step [19/26], Loss: 4684.1035\n",
      "Epoch [32/500], Step [20/26], Loss: 1845.6729\n",
      "Epoch [32/500], Step [21/26], Loss: 3790.8965\n",
      "Epoch [32/500], Step [22/26], Loss: 4291.0869\n",
      "Epoch [32/500], Step [23/26], Loss: 1927.8718\n",
      "Epoch [32/500], Step [24/26], Loss: 1417.0216\n",
      "Epoch [32/500], Step [25/26], Loss: 3678.7212\n",
      "Epoch [32/500], Step [26/26], Loss: 3295.4614\n",
      "Epoch [33/500], Step [1/26], Loss: 1510.6627\n",
      "Epoch [33/500], Step [2/26], Loss: 2181.4634\n",
      "Epoch [33/500], Step [3/26], Loss: 3047.1118\n",
      "Epoch [33/500], Step [4/26], Loss: 3340.2744\n",
      "Epoch [33/500], Step [5/26], Loss: 3806.0332\n",
      "Epoch [33/500], Step [6/26], Loss: 2181.5996\n",
      "Epoch [33/500], Step [7/26], Loss: 3615.5664\n",
      "Epoch [33/500], Step [8/26], Loss: 2800.7993\n",
      "Epoch [33/500], Step [9/26], Loss: 3291.8467\n",
      "Epoch [33/500], Step [10/26], Loss: 4008.5298\n",
      "Epoch [33/500], Step [11/26], Loss: 3076.8743\n",
      "Epoch [33/500], Step [12/26], Loss: 3186.5356\n",
      "Epoch [33/500], Step [13/26], Loss: 2529.7979\n",
      "Epoch [33/500], Step [14/26], Loss: 2727.6931\n",
      "Epoch [33/500], Step [15/26], Loss: 2739.6814\n",
      "Epoch [33/500], Step [16/26], Loss: 3204.9194\n",
      "Epoch [33/500], Step [17/26], Loss: 2359.5825\n",
      "Epoch [33/500], Step [18/26], Loss: 1741.3174\n",
      "Epoch [33/500], Step [19/26], Loss: 3206.2778\n",
      "Epoch [33/500], Step [20/26], Loss: 3220.9971\n",
      "Epoch [33/500], Step [21/26], Loss: 1190.6543\n",
      "Epoch [33/500], Step [22/26], Loss: 2165.4050\n",
      "Epoch [33/500], Step [23/26], Loss: 1289.1323\n",
      "Epoch [33/500], Step [24/26], Loss: 6763.1440\n",
      "Epoch [33/500], Step [25/26], Loss: 3255.9758\n",
      "Epoch [33/500], Step [26/26], Loss: 1262.7029\n",
      "Epoch [34/500], Step [1/26], Loss: 3399.4478\n",
      "Epoch [34/500], Step [2/26], Loss: 2092.6167\n",
      "Epoch [34/500], Step [3/26], Loss: 3483.4150\n",
      "Epoch [34/500], Step [4/26], Loss: 1657.6819\n",
      "Epoch [34/500], Step [5/26], Loss: 1440.3938\n",
      "Epoch [34/500], Step [6/26], Loss: 5013.4175\n",
      "Epoch [34/500], Step [7/26], Loss: 3644.8000\n",
      "Epoch [34/500], Step [8/26], Loss: 3626.1367\n",
      "Epoch [34/500], Step [9/26], Loss: 2225.5732\n",
      "Epoch [34/500], Step [10/26], Loss: 1601.3297\n",
      "Epoch [34/500], Step [11/26], Loss: 2451.6206\n",
      "Epoch [34/500], Step [12/26], Loss: 7981.0908\n",
      "Epoch [34/500], Step [13/26], Loss: 2055.5425\n",
      "Epoch [34/500], Step [14/26], Loss: 2200.4749\n",
      "Epoch [34/500], Step [15/26], Loss: 2150.3530\n",
      "Epoch [34/500], Step [16/26], Loss: 3751.2612\n",
      "Epoch [34/500], Step [17/26], Loss: 3444.4165\n",
      "Epoch [34/500], Step [18/26], Loss: 1910.7700\n",
      "Epoch [34/500], Step [19/26], Loss: 1564.4583\n",
      "Epoch [34/500], Step [20/26], Loss: 1315.2775\n",
      "Epoch [34/500], Step [21/26], Loss: 4713.2271\n",
      "Epoch [34/500], Step [22/26], Loss: 3748.2725\n",
      "Epoch [34/500], Step [23/26], Loss: 1775.8323\n",
      "Epoch [34/500], Step [24/26], Loss: 2552.3706\n",
      "Epoch [34/500], Step [25/26], Loss: 4486.4180\n",
      "Epoch [34/500], Step [26/26], Loss: 453.6397\n",
      "Epoch [35/500], Step [1/26], Loss: 489.1402\n",
      "Epoch [35/500], Step [2/26], Loss: 2123.2095\n",
      "Epoch [35/500], Step [3/26], Loss: 2132.9878\n",
      "Epoch [35/500], Step [4/26], Loss: 2720.4717\n",
      "Epoch [35/500], Step [5/26], Loss: 1241.6824\n",
      "Epoch [35/500], Step [6/26], Loss: 3749.6812\n",
      "Epoch [35/500], Step [7/26], Loss: 3200.1875\n",
      "Epoch [35/500], Step [8/26], Loss: 2450.6772\n",
      "Epoch [35/500], Step [9/26], Loss: 2485.3965\n",
      "Epoch [35/500], Step [10/26], Loss: 1259.8621\n",
      "Epoch [35/500], Step [11/26], Loss: 4454.7070\n",
      "Epoch [35/500], Step [12/26], Loss: 2768.1855\n",
      "Epoch [35/500], Step [13/26], Loss: 4553.4766\n",
      "Epoch [35/500], Step [14/26], Loss: 4649.6699\n",
      "Epoch [35/500], Step [15/26], Loss: 3630.4053\n",
      "Epoch [35/500], Step [16/26], Loss: 2800.8276\n",
      "Epoch [35/500], Step [17/26], Loss: 3066.8269\n",
      "Epoch [35/500], Step [18/26], Loss: 3054.7427\n",
      "Epoch [35/500], Step [19/26], Loss: 3655.3325\n",
      "Epoch [35/500], Step [20/26], Loss: 1937.3580\n",
      "Epoch [35/500], Step [21/26], Loss: 1788.5668\n",
      "Epoch [35/500], Step [22/26], Loss: 2194.0869\n",
      "Epoch [35/500], Step [23/26], Loss: 3880.8667\n",
      "Epoch [35/500], Step [24/26], Loss: 1700.2888\n",
      "Epoch [35/500], Step [25/26], Loss: 3420.6326\n",
      "Epoch [35/500], Step [26/26], Loss: 4438.3296\n",
      "Epoch [36/500], Step [1/26], Loss: 2886.2500\n",
      "Epoch [36/500], Step [2/26], Loss: 2792.3914\n",
      "Epoch [36/500], Step [3/26], Loss: 4010.1074\n",
      "Epoch [36/500], Step [4/26], Loss: 5342.5308\n",
      "Epoch [36/500], Step [5/26], Loss: 3233.5698\n",
      "Epoch [36/500], Step [6/26], Loss: 3320.2581\n",
      "Epoch [36/500], Step [7/26], Loss: 2615.7178\n",
      "Epoch [36/500], Step [8/26], Loss: 2430.1724\n",
      "Epoch [36/500], Step [9/26], Loss: 4135.9180\n",
      "Epoch [36/500], Step [10/26], Loss: 1377.4229\n",
      "Epoch [36/500], Step [11/26], Loss: 2885.0923\n",
      "Epoch [36/500], Step [12/26], Loss: 2465.3574\n",
      "Epoch [36/500], Step [13/26], Loss: 2692.9443\n",
      "Epoch [36/500], Step [14/26], Loss: 2790.0906\n",
      "Epoch [36/500], Step [15/26], Loss: 3250.1116\n",
      "Epoch [36/500], Step [16/26], Loss: 2198.3508\n",
      "Epoch [36/500], Step [17/26], Loss: 4500.9463\n",
      "Epoch [36/500], Step [18/26], Loss: 1178.2806\n",
      "Epoch [36/500], Step [19/26], Loss: 1689.0363\n",
      "Epoch [36/500], Step [20/26], Loss: 3624.0193\n",
      "Epoch [36/500], Step [21/26], Loss: 3970.3999\n",
      "Epoch [36/500], Step [22/26], Loss: 3260.8979\n",
      "Epoch [36/500], Step [23/26], Loss: 2865.3677\n",
      "Epoch [36/500], Step [24/26], Loss: 3631.9297\n",
      "Epoch [36/500], Step [25/26], Loss: 1207.2805\n",
      "Epoch [36/500], Step [26/26], Loss: 2888.0923\n",
      "Epoch [37/500], Step [1/26], Loss: 1597.6494\n",
      "Epoch [37/500], Step [2/26], Loss: 2616.0977\n",
      "Epoch [37/500], Step [3/26], Loss: 2232.4402\n",
      "Epoch [37/500], Step [4/26], Loss: 3059.7363\n",
      "Epoch [37/500], Step [5/26], Loss: 2610.2927\n",
      "Epoch [37/500], Step [6/26], Loss: 2329.2593\n",
      "Epoch [37/500], Step [7/26], Loss: 6339.4932\n",
      "Epoch [37/500], Step [8/26], Loss: 1908.6958\n",
      "Epoch [37/500], Step [9/26], Loss: 4257.9258\n",
      "Epoch [37/500], Step [10/26], Loss: 1491.8088\n",
      "Epoch [37/500], Step [11/26], Loss: 2294.6252\n",
      "Epoch [37/500], Step [12/26], Loss: 4773.6846\n",
      "Epoch [37/500], Step [13/26], Loss: 2856.8848\n",
      "Epoch [37/500], Step [14/26], Loss: 2372.4863\n",
      "Epoch [37/500], Step [15/26], Loss: 2024.9182\n",
      "Epoch [37/500], Step [16/26], Loss: 3312.6694\n",
      "Epoch [37/500], Step [17/26], Loss: 5469.6338\n",
      "Epoch [37/500], Step [18/26], Loss: 1863.1621\n",
      "Epoch [37/500], Step [19/26], Loss: 2679.9043\n",
      "Epoch [37/500], Step [20/26], Loss: 2481.6191\n",
      "Epoch [37/500], Step [21/26], Loss: 2125.9866\n",
      "Epoch [37/500], Step [22/26], Loss: 3880.4688\n",
      "Epoch [37/500], Step [23/26], Loss: 3152.2668\n",
      "Epoch [37/500], Step [24/26], Loss: 2794.3389\n",
      "Epoch [37/500], Step [25/26], Loss: 6094.3115\n",
      "Epoch [37/500], Step [26/26], Loss: 569.2986\n",
      "Epoch [38/500], Step [1/26], Loss: 5227.7407\n",
      "Epoch [38/500], Step [2/26], Loss: 1582.8391\n",
      "Epoch [38/500], Step [3/26], Loss: 4123.5166\n",
      "Epoch [38/500], Step [4/26], Loss: 2085.8169\n",
      "Epoch [38/500], Step [5/26], Loss: 1795.5162\n",
      "Epoch [38/500], Step [6/26], Loss: 1973.6177\n",
      "Epoch [38/500], Step [7/26], Loss: 3685.4907\n",
      "Epoch [38/500], Step [8/26], Loss: 3347.0562\n",
      "Epoch [38/500], Step [9/26], Loss: 1363.0846\n",
      "Epoch [38/500], Step [10/26], Loss: 5012.7881\n",
      "Epoch [38/500], Step [11/26], Loss: 831.3604\n",
      "Epoch [38/500], Step [12/26], Loss: 1727.3364\n",
      "Epoch [38/500], Step [13/26], Loss: 2364.4612\n",
      "Epoch [38/500], Step [14/26], Loss: 2660.1787\n",
      "Epoch [38/500], Step [15/26], Loss: 2199.6641\n",
      "Epoch [38/500], Step [16/26], Loss: 2330.3374\n",
      "Epoch [38/500], Step [17/26], Loss: 2688.8545\n",
      "Epoch [38/500], Step [18/26], Loss: 2204.3938\n",
      "Epoch [38/500], Step [19/26], Loss: 2254.6309\n",
      "Epoch [38/500], Step [20/26], Loss: 2803.0986\n",
      "Epoch [38/500], Step [21/26], Loss: 4047.7568\n",
      "Epoch [38/500], Step [22/26], Loss: 4262.7437\n",
      "Epoch [38/500], Step [23/26], Loss: 2917.8730\n",
      "Epoch [38/500], Step [24/26], Loss: 1592.8663\n",
      "Epoch [38/500], Step [25/26], Loss: 2945.9629\n",
      "Epoch [38/500], Step [26/26], Loss: 1575.1183\n",
      "Epoch [39/500], Step [1/26], Loss: 2038.2000\n",
      "Epoch [39/500], Step [2/26], Loss: 4550.7100\n",
      "Epoch [39/500], Step [3/26], Loss: 2077.1699\n",
      "Epoch [39/500], Step [4/26], Loss: 2124.3538\n",
      "Epoch [39/500], Step [5/26], Loss: 1782.6960\n",
      "Epoch [39/500], Step [6/26], Loss: 3391.4658\n",
      "Epoch [39/500], Step [7/26], Loss: 1561.2094\n",
      "Epoch [39/500], Step [8/26], Loss: 3403.9072\n",
      "Epoch [39/500], Step [9/26], Loss: 2371.3472\n",
      "Epoch [39/500], Step [10/26], Loss: 3525.8030\n",
      "Epoch [39/500], Step [11/26], Loss: 2594.0737\n",
      "Epoch [39/500], Step [12/26], Loss: 3317.7119\n",
      "Epoch [39/500], Step [13/26], Loss: 1919.3855\n",
      "Epoch [39/500], Step [14/26], Loss: 2643.8887\n",
      "Epoch [39/500], Step [15/26], Loss: 1231.0289\n",
      "Epoch [39/500], Step [16/26], Loss: 3387.2461\n",
      "Epoch [39/500], Step [17/26], Loss: 2994.9092\n",
      "Epoch [39/500], Step [18/26], Loss: 2758.0615\n",
      "Epoch [39/500], Step [19/26], Loss: 1185.8440\n",
      "Epoch [39/500], Step [20/26], Loss: 2141.6897\n",
      "Epoch [39/500], Step [21/26], Loss: 2250.2627\n",
      "Epoch [39/500], Step [22/26], Loss: 2562.7998\n",
      "Epoch [39/500], Step [23/26], Loss: 2001.4785\n",
      "Epoch [39/500], Step [24/26], Loss: 4091.3308\n",
      "Epoch [39/500], Step [25/26], Loss: 2218.3550\n",
      "Epoch [39/500], Step [26/26], Loss: 1690.7328\n",
      "Epoch [40/500], Step [1/26], Loss: 1563.9529\n",
      "Epoch [40/500], Step [2/26], Loss: 2239.9377\n",
      "Epoch [40/500], Step [3/26], Loss: 4785.1885\n",
      "Epoch [40/500], Step [4/26], Loss: 3413.1104\n",
      "Epoch [40/500], Step [5/26], Loss: 3377.3093\n",
      "Epoch [40/500], Step [6/26], Loss: 1388.3560\n",
      "Epoch [40/500], Step [7/26], Loss: 892.2371\n",
      "Epoch [40/500], Step [8/26], Loss: 2557.6533\n",
      "Epoch [40/500], Step [9/26], Loss: 1349.5828\n",
      "Epoch [40/500], Step [10/26], Loss: 3075.8047\n",
      "Epoch [40/500], Step [11/26], Loss: 1873.6559\n",
      "Epoch [40/500], Step [12/26], Loss: 5096.9590\n",
      "Epoch [40/500], Step [13/26], Loss: 2221.8271\n",
      "Epoch [40/500], Step [14/26], Loss: 2112.9978\n",
      "Epoch [40/500], Step [15/26], Loss: 3535.4636\n",
      "Epoch [40/500], Step [16/26], Loss: 2152.2920\n",
      "Epoch [40/500], Step [17/26], Loss: 3993.8245\n",
      "Epoch [40/500], Step [18/26], Loss: 1826.4871\n",
      "Epoch [40/500], Step [19/26], Loss: 3530.7834\n",
      "Epoch [40/500], Step [20/26], Loss: 2775.0449\n",
      "Epoch [40/500], Step [21/26], Loss: 4181.6543\n",
      "Epoch [40/500], Step [22/26], Loss: 2687.4185\n",
      "Epoch [40/500], Step [23/26], Loss: 1836.8237\n",
      "Epoch [40/500], Step [24/26], Loss: 2460.6404\n",
      "Epoch [40/500], Step [25/26], Loss: 3418.5801\n",
      "Epoch [40/500], Step [26/26], Loss: 5715.8765\n",
      "Epoch [41/500], Step [1/26], Loss: 1412.6641\n",
      "Epoch [41/500], Step [2/26], Loss: 1110.7556\n",
      "Epoch [41/500], Step [3/26], Loss: 2500.1270\n",
      "Epoch [41/500], Step [4/26], Loss: 3489.0681\n",
      "Epoch [41/500], Step [5/26], Loss: 2522.4485\n",
      "Epoch [41/500], Step [6/26], Loss: 5549.8105\n",
      "Epoch [41/500], Step [7/26], Loss: 2384.1094\n",
      "Epoch [41/500], Step [8/26], Loss: 3263.1160\n",
      "Epoch [41/500], Step [9/26], Loss: 1166.9054\n",
      "Epoch [41/500], Step [10/26], Loss: 1827.2468\n",
      "Epoch [41/500], Step [11/26], Loss: 2985.1914\n",
      "Epoch [41/500], Step [12/26], Loss: 1665.3053\n",
      "Epoch [41/500], Step [13/26], Loss: 2368.0623\n",
      "Epoch [41/500], Step [14/26], Loss: 2278.9897\n",
      "Epoch [41/500], Step [15/26], Loss: 1885.3055\n",
      "Epoch [41/500], Step [16/26], Loss: 2194.1794\n",
      "Epoch [41/500], Step [17/26], Loss: 1945.6227\n",
      "Epoch [41/500], Step [18/26], Loss: 3390.1223\n",
      "Epoch [41/500], Step [19/26], Loss: 4862.0566\n",
      "Epoch [41/500], Step [20/26], Loss: 2793.7839\n",
      "Epoch [41/500], Step [21/26], Loss: 1847.5627\n",
      "Epoch [41/500], Step [22/26], Loss: 2855.6248\n",
      "Epoch [41/500], Step [23/26], Loss: 3906.7471\n",
      "Epoch [41/500], Step [24/26], Loss: 2217.2317\n",
      "Epoch [41/500], Step [25/26], Loss: 834.9451\n",
      "Epoch [41/500], Step [26/26], Loss: 1855.5002\n",
      "Epoch [42/500], Step [1/26], Loss: 723.3115\n",
      "Epoch [42/500], Step [2/26], Loss: 4635.6299\n",
      "Epoch [42/500], Step [3/26], Loss: 3437.5625\n",
      "Epoch [42/500], Step [4/26], Loss: 998.0807\n",
      "Epoch [42/500], Step [5/26], Loss: 4991.3735\n",
      "Epoch [42/500], Step [6/26], Loss: 2069.1426\n",
      "Epoch [42/500], Step [7/26], Loss: 2771.1572\n",
      "Epoch [42/500], Step [8/26], Loss: 1996.3926\n",
      "Epoch [42/500], Step [9/26], Loss: 1892.8706\n",
      "Epoch [42/500], Step [10/26], Loss: 5182.1045\n",
      "Epoch [42/500], Step [11/26], Loss: 1029.8425\n",
      "Epoch [42/500], Step [12/26], Loss: 1643.4631\n",
      "Epoch [42/500], Step [13/26], Loss: 2568.6829\n",
      "Epoch [42/500], Step [14/26], Loss: 3182.7893\n",
      "Epoch [42/500], Step [15/26], Loss: 1676.4731\n",
      "Epoch [42/500], Step [16/26], Loss: 1060.2700\n",
      "Epoch [42/500], Step [17/26], Loss: 2343.4939\n",
      "Epoch [42/500], Step [18/26], Loss: 979.2529\n",
      "Epoch [42/500], Step [19/26], Loss: 1688.9109\n",
      "Epoch [42/500], Step [20/26], Loss: 4128.7314\n",
      "Epoch [42/500], Step [21/26], Loss: 1168.0092\n",
      "Epoch [42/500], Step [22/26], Loss: 3827.8428\n",
      "Epoch [42/500], Step [23/26], Loss: 6036.7002\n",
      "Epoch [42/500], Step [24/26], Loss: 2512.4766\n",
      "Epoch [42/500], Step [25/26], Loss: 2721.3730\n",
      "Epoch [42/500], Step [26/26], Loss: 332.7497\n",
      "Epoch [43/500], Step [1/26], Loss: 1884.8784\n",
      "Epoch [43/500], Step [2/26], Loss: 3315.1848\n",
      "Epoch [43/500], Step [3/26], Loss: 1859.9535\n",
      "Epoch [43/500], Step [4/26], Loss: 2131.4084\n",
      "Epoch [43/500], Step [5/26], Loss: 3184.6978\n",
      "Epoch [43/500], Step [6/26], Loss: 1196.6677\n",
      "Epoch [43/500], Step [7/26], Loss: 2088.9385\n",
      "Epoch [43/500], Step [8/26], Loss: 1970.9290\n",
      "Epoch [43/500], Step [9/26], Loss: 1307.0547\n",
      "Epoch [43/500], Step [10/26], Loss: 3250.2085\n",
      "Epoch [43/500], Step [11/26], Loss: 3095.3901\n",
      "Epoch [43/500], Step [12/26], Loss: 3297.2271\n",
      "Epoch [43/500], Step [13/26], Loss: 5585.7388\n",
      "Epoch [43/500], Step [14/26], Loss: 796.5264\n",
      "Epoch [43/500], Step [15/26], Loss: 2647.5674\n",
      "Epoch [43/500], Step [16/26], Loss: 5102.3247\n",
      "Epoch [43/500], Step [17/26], Loss: 3152.8506\n",
      "Epoch [43/500], Step [18/26], Loss: 3310.7754\n",
      "Epoch [43/500], Step [19/26], Loss: 2054.1880\n",
      "Epoch [43/500], Step [20/26], Loss: 1636.5742\n",
      "Epoch [43/500], Step [21/26], Loss: 3424.8108\n",
      "Epoch [43/500], Step [22/26], Loss: 2656.6143\n",
      "Epoch [43/500], Step [23/26], Loss: 1783.3817\n",
      "Epoch [43/500], Step [24/26], Loss: 794.8516\n",
      "Epoch [43/500], Step [25/26], Loss: 2195.4478\n",
      "Epoch [43/500], Step [26/26], Loss: 1792.8798\n",
      "Epoch [44/500], Step [1/26], Loss: 1576.7495\n",
      "Epoch [44/500], Step [2/26], Loss: 3310.3347\n",
      "Epoch [44/500], Step [3/26], Loss: 3398.6057\n",
      "Epoch [44/500], Step [4/26], Loss: 1531.2075\n",
      "Epoch [44/500], Step [5/26], Loss: 2672.0288\n",
      "Epoch [44/500], Step [6/26], Loss: 1989.9243\n",
      "Epoch [44/500], Step [7/26], Loss: 1830.5365\n",
      "Epoch [44/500], Step [8/26], Loss: 2640.6821\n",
      "Epoch [44/500], Step [9/26], Loss: 2145.7859\n",
      "Epoch [44/500], Step [10/26], Loss: 3162.9016\n",
      "Epoch [44/500], Step [11/26], Loss: 2103.4600\n",
      "Epoch [44/500], Step [12/26], Loss: 2451.0774\n",
      "Epoch [44/500], Step [13/26], Loss: 2485.8330\n",
      "Epoch [44/500], Step [14/26], Loss: 2559.5334\n",
      "Epoch [44/500], Step [15/26], Loss: 1715.1831\n",
      "Epoch [44/500], Step [16/26], Loss: 3244.9756\n",
      "Epoch [44/500], Step [17/26], Loss: 2735.0627\n",
      "Epoch [44/500], Step [18/26], Loss: 3165.3491\n",
      "Epoch [44/500], Step [19/26], Loss: 1270.8782\n",
      "Epoch [44/500], Step [20/26], Loss: 1607.4587\n",
      "Epoch [44/500], Step [21/26], Loss: 2587.9458\n",
      "Epoch [44/500], Step [22/26], Loss: 2627.9727\n",
      "Epoch [44/500], Step [23/26], Loss: 1640.1525\n",
      "Epoch [44/500], Step [24/26], Loss: 2779.9343\n",
      "Epoch [44/500], Step [25/26], Loss: 2332.7637\n",
      "Epoch [44/500], Step [26/26], Loss: 246.1474\n",
      "Epoch [45/500], Step [1/26], Loss: 2813.2964\n",
      "Epoch [45/500], Step [2/26], Loss: 874.9146\n",
      "Epoch [45/500], Step [3/26], Loss: 1891.4066\n",
      "Epoch [45/500], Step [4/26], Loss: 2179.8853\n",
      "Epoch [45/500], Step [5/26], Loss: 2543.1799\n",
      "Epoch [45/500], Step [6/26], Loss: 1593.5243\n",
      "Epoch [45/500], Step [7/26], Loss: 1961.5320\n",
      "Epoch [45/500], Step [8/26], Loss: 1650.7372\n",
      "Epoch [45/500], Step [9/26], Loss: 3041.8594\n",
      "Epoch [45/500], Step [10/26], Loss: 3316.3274\n",
      "Epoch [45/500], Step [11/26], Loss: 1709.3284\n",
      "Epoch [45/500], Step [12/26], Loss: 2396.4360\n",
      "Epoch [45/500], Step [13/26], Loss: 2102.4541\n",
      "Epoch [45/500], Step [14/26], Loss: 4060.3960\n",
      "Epoch [45/500], Step [15/26], Loss: 2233.1621\n",
      "Epoch [45/500], Step [16/26], Loss: 2173.1956\n",
      "Epoch [45/500], Step [17/26], Loss: 3538.1030\n",
      "Epoch [45/500], Step [18/26], Loss: 2317.3818\n",
      "Epoch [45/500], Step [19/26], Loss: 3635.7461\n",
      "Epoch [45/500], Step [20/26], Loss: 2677.2021\n",
      "Epoch [45/500], Step [21/26], Loss: 1776.3052\n",
      "Epoch [45/500], Step [22/26], Loss: 2149.4646\n",
      "Epoch [45/500], Step [23/26], Loss: 1051.4443\n",
      "Epoch [45/500], Step [24/26], Loss: 4060.1831\n",
      "Epoch [45/500], Step [25/26], Loss: 1522.2393\n",
      "Epoch [45/500], Step [26/26], Loss: 1462.6605\n",
      "Epoch [46/500], Step [1/26], Loss: 1410.9298\n",
      "Epoch [46/500], Step [2/26], Loss: 2743.6433\n",
      "Epoch [46/500], Step [3/26], Loss: 799.9994\n",
      "Epoch [46/500], Step [4/26], Loss: 2101.1899\n",
      "Epoch [46/500], Step [5/26], Loss: 1635.0127\n",
      "Epoch [46/500], Step [6/26], Loss: 2229.1304\n",
      "Epoch [46/500], Step [7/26], Loss: 4629.0371\n",
      "Epoch [46/500], Step [8/26], Loss: 3969.3130\n",
      "Epoch [46/500], Step [9/26], Loss: 970.4099\n",
      "Epoch [46/500], Step [10/26], Loss: 600.6959\n",
      "Epoch [46/500], Step [11/26], Loss: 2383.0476\n",
      "Epoch [46/500], Step [12/26], Loss: 3395.7344\n",
      "Epoch [46/500], Step [13/26], Loss: 2582.2817\n",
      "Epoch [46/500], Step [14/26], Loss: 1961.5717\n",
      "Epoch [46/500], Step [15/26], Loss: 1285.3042\n",
      "Epoch [46/500], Step [16/26], Loss: 4145.6440\n",
      "Epoch [46/500], Step [17/26], Loss: 1896.8469\n",
      "Epoch [46/500], Step [18/26], Loss: 2585.8594\n",
      "Epoch [46/500], Step [19/26], Loss: 2316.2090\n",
      "Epoch [46/500], Step [20/26], Loss: 4365.5166\n",
      "Epoch [46/500], Step [21/26], Loss: 1625.3871\n",
      "Epoch [46/500], Step [22/26], Loss: 1573.6466\n",
      "Epoch [46/500], Step [23/26], Loss: 1290.8271\n",
      "Epoch [46/500], Step [24/26], Loss: 1926.3531\n",
      "Epoch [46/500], Step [25/26], Loss: 3950.0034\n",
      "Epoch [46/500], Step [26/26], Loss: 1547.4690\n",
      "Epoch [47/500], Step [1/26], Loss: 5233.9258\n",
      "Epoch [47/500], Step [2/26], Loss: 3371.0376\n",
      "Epoch [47/500], Step [3/26], Loss: 1681.8113\n",
      "Epoch [47/500], Step [4/26], Loss: 2264.3354\n",
      "Epoch [47/500], Step [5/26], Loss: 3192.2461\n",
      "Epoch [47/500], Step [6/26], Loss: 2656.7012\n",
      "Epoch [47/500], Step [7/26], Loss: 2738.6992\n",
      "Epoch [47/500], Step [8/26], Loss: 2883.9900\n",
      "Epoch [47/500], Step [9/26], Loss: 2723.7581\n",
      "Epoch [47/500], Step [10/26], Loss: 1056.2698\n",
      "Epoch [47/500], Step [11/26], Loss: 2030.5751\n",
      "Epoch [47/500], Step [12/26], Loss: 2182.2993\n",
      "Epoch [47/500], Step [13/26], Loss: 2675.1201\n",
      "Epoch [47/500], Step [14/26], Loss: 3457.4346\n",
      "Epoch [47/500], Step [15/26], Loss: 2392.8857\n",
      "Epoch [47/500], Step [16/26], Loss: 2395.9832\n",
      "Epoch [47/500], Step [17/26], Loss: 1400.1921\n",
      "Epoch [47/500], Step [18/26], Loss: 1163.4696\n",
      "Epoch [47/500], Step [19/26], Loss: 3040.6025\n",
      "Epoch [47/500], Step [20/26], Loss: 1991.0198\n",
      "Epoch [47/500], Step [21/26], Loss: 2171.4087\n",
      "Epoch [47/500], Step [22/26], Loss: 964.3105\n",
      "Epoch [47/500], Step [23/26], Loss: 3126.1445\n",
      "Epoch [47/500], Step [24/26], Loss: 1311.3132\n",
      "Epoch [47/500], Step [25/26], Loss: 1435.5648\n",
      "Epoch [47/500], Step [26/26], Loss: 568.3516\n",
      "Epoch [48/500], Step [1/26], Loss: 1282.6082\n",
      "Epoch [48/500], Step [2/26], Loss: 1935.5559\n",
      "Epoch [48/500], Step [3/26], Loss: 3261.1592\n",
      "Epoch [48/500], Step [4/26], Loss: 4250.8394\n",
      "Epoch [48/500], Step [5/26], Loss: 2585.9482\n",
      "Epoch [48/500], Step [6/26], Loss: 1431.7744\n",
      "Epoch [48/500], Step [7/26], Loss: 2343.0684\n",
      "Epoch [48/500], Step [8/26], Loss: 2059.2231\n",
      "Epoch [48/500], Step [9/26], Loss: 1113.3373\n",
      "Epoch [48/500], Step [10/26], Loss: 1923.0293\n",
      "Epoch [48/500], Step [11/26], Loss: 1935.9712\n",
      "Epoch [48/500], Step [12/26], Loss: 2097.9617\n",
      "Epoch [48/500], Step [13/26], Loss: 1444.8503\n",
      "Epoch [48/500], Step [14/26], Loss: 2738.2563\n",
      "Epoch [48/500], Step [15/26], Loss: 1569.9219\n",
      "Epoch [48/500], Step [16/26], Loss: 1349.4485\n",
      "Epoch [48/500], Step [17/26], Loss: 3078.8674\n",
      "Epoch [48/500], Step [18/26], Loss: 3788.8442\n",
      "Epoch [48/500], Step [19/26], Loss: 2055.7017\n",
      "Epoch [48/500], Step [20/26], Loss: 1713.6597\n",
      "Epoch [48/500], Step [21/26], Loss: 3460.6663\n",
      "Epoch [48/500], Step [22/26], Loss: 1236.6521\n",
      "Epoch [48/500], Step [23/26], Loss: 826.3965\n",
      "Epoch [48/500], Step [24/26], Loss: 4174.1797\n",
      "Epoch [48/500], Step [25/26], Loss: 1232.0383\n",
      "Epoch [48/500], Step [26/26], Loss: 1298.6047\n",
      "Epoch [49/500], Step [1/26], Loss: 3973.8535\n",
      "Epoch [49/500], Step [2/26], Loss: 2430.9370\n",
      "Epoch [49/500], Step [3/26], Loss: 1396.5842\n",
      "Epoch [49/500], Step [4/26], Loss: 1864.1821\n",
      "Epoch [49/500], Step [5/26], Loss: 3227.6782\n",
      "Epoch [49/500], Step [6/26], Loss: 3832.0571\n",
      "Epoch [49/500], Step [7/26], Loss: 1796.0603\n",
      "Epoch [49/500], Step [8/26], Loss: 1773.3008\n",
      "Epoch [49/500], Step [9/26], Loss: 1895.1587\n",
      "Epoch [49/500], Step [10/26], Loss: 3217.1448\n",
      "Epoch [49/500], Step [11/26], Loss: 2182.2515\n",
      "Epoch [49/500], Step [12/26], Loss: 1080.8994\n",
      "Epoch [49/500], Step [13/26], Loss: 1898.8616\n",
      "Epoch [49/500], Step [14/26], Loss: 2098.5376\n",
      "Epoch [49/500], Step [15/26], Loss: 2343.4702\n",
      "Epoch [49/500], Step [16/26], Loss: 2942.9404\n",
      "Epoch [49/500], Step [17/26], Loss: 1275.9724\n",
      "Epoch [49/500], Step [18/26], Loss: 1968.1658\n",
      "Epoch [49/500], Step [19/26], Loss: 1107.8738\n",
      "Epoch [49/500], Step [20/26], Loss: 1708.0964\n",
      "Epoch [49/500], Step [21/26], Loss: 919.3391\n",
      "Epoch [49/500], Step [22/26], Loss: 1562.7805\n",
      "Epoch [49/500], Step [23/26], Loss: 831.1305\n",
      "Epoch [49/500], Step [24/26], Loss: 2759.7202\n",
      "Epoch [49/500], Step [25/26], Loss: 1753.9163\n",
      "Epoch [49/500], Step [26/26], Loss: 361.1060\n",
      "Epoch [50/500], Step [1/26], Loss: 2068.5020\n",
      "Epoch [50/500], Step [2/26], Loss: 530.2212\n",
      "Epoch [50/500], Step [3/26], Loss: 2023.7256\n",
      "Epoch [50/500], Step [4/26], Loss: 1764.8184\n",
      "Epoch [50/500], Step [5/26], Loss: 2062.9883\n",
      "Epoch [50/500], Step [6/26], Loss: 3232.4990\n",
      "Epoch [50/500], Step [7/26], Loss: 1653.1149\n",
      "Epoch [50/500], Step [8/26], Loss: 1187.6792\n",
      "Epoch [50/500], Step [9/26], Loss: 3012.0356\n",
      "Epoch [50/500], Step [10/26], Loss: 3133.0776\n",
      "Epoch [50/500], Step [11/26], Loss: 990.0692\n",
      "Epoch [50/500], Step [12/26], Loss: 2747.9126\n",
      "Epoch [50/500], Step [13/26], Loss: 1734.6544\n",
      "Epoch [50/500], Step [14/26], Loss: 2804.6729\n",
      "Epoch [50/500], Step [15/26], Loss: 2449.6409\n",
      "Epoch [50/500], Step [16/26], Loss: 1502.1825\n",
      "Epoch [50/500], Step [17/26], Loss: 1963.2900\n",
      "Epoch [50/500], Step [18/26], Loss: 2161.1321\n",
      "Epoch [50/500], Step [19/26], Loss: 3160.0779\n",
      "Epoch [50/500], Step [20/26], Loss: 2066.7974\n",
      "Epoch [50/500], Step [21/26], Loss: 1789.3887\n",
      "Epoch [50/500], Step [22/26], Loss: 2860.1616\n",
      "Epoch [50/500], Step [23/26], Loss: 2017.1415\n",
      "Epoch [50/500], Step [24/26], Loss: 2252.0010\n",
      "Epoch [50/500], Step [25/26], Loss: 1367.2681\n",
      "Epoch [50/500], Step [26/26], Loss: 5026.5049\n",
      "Epoch [51/500], Step [1/26], Loss: 1175.0383\n",
      "Epoch [51/500], Step [2/26], Loss: 4022.6104\n",
      "Epoch [51/500], Step [3/26], Loss: 2782.4360\n",
      "Epoch [51/500], Step [4/26], Loss: 2004.9641\n",
      "Epoch [51/500], Step [5/26], Loss: 1835.9648\n",
      "Epoch [51/500], Step [6/26], Loss: 3325.3635\n",
      "Epoch [51/500], Step [7/26], Loss: 1297.9263\n",
      "Epoch [51/500], Step [8/26], Loss: 841.4163\n",
      "Epoch [51/500], Step [9/26], Loss: 1627.4576\n",
      "Epoch [51/500], Step [10/26], Loss: 2171.5747\n",
      "Epoch [51/500], Step [11/26], Loss: 817.0745\n",
      "Epoch [51/500], Step [12/26], Loss: 2085.3254\n",
      "Epoch [51/500], Step [13/26], Loss: 2479.3726\n",
      "Epoch [51/500], Step [14/26], Loss: 1918.9625\n",
      "Epoch [51/500], Step [15/26], Loss: 2530.6379\n",
      "Epoch [51/500], Step [16/26], Loss: 1824.5032\n",
      "Epoch [51/500], Step [17/26], Loss: 1965.0289\n",
      "Epoch [51/500], Step [18/26], Loss: 2086.2969\n",
      "Epoch [51/500], Step [19/26], Loss: 1689.8541\n",
      "Epoch [51/500], Step [20/26], Loss: 1132.5117\n",
      "Epoch [51/500], Step [21/26], Loss: 2291.4688\n",
      "Epoch [51/500], Step [22/26], Loss: 1586.2615\n",
      "Epoch [51/500], Step [23/26], Loss: 1320.4741\n",
      "Epoch [51/500], Step [24/26], Loss: 3361.5923\n",
      "Epoch [51/500], Step [25/26], Loss: 3484.6025\n",
      "Epoch [51/500], Step [26/26], Loss: 774.5270\n",
      "Epoch [52/500], Step [1/26], Loss: 1638.4666\n",
      "Epoch [52/500], Step [2/26], Loss: 2513.6504\n",
      "Epoch [52/500], Step [3/26], Loss: 1316.9412\n",
      "Epoch [52/500], Step [4/26], Loss: 3253.5984\n",
      "Epoch [52/500], Step [5/26], Loss: 2576.5515\n",
      "Epoch [52/500], Step [6/26], Loss: 2219.0347\n",
      "Epoch [52/500], Step [7/26], Loss: 1495.7195\n",
      "Epoch [52/500], Step [8/26], Loss: 1906.8121\n",
      "Epoch [52/500], Step [9/26], Loss: 1464.1884\n",
      "Epoch [52/500], Step [10/26], Loss: 2422.8130\n",
      "Epoch [52/500], Step [11/26], Loss: 568.1694\n",
      "Epoch [52/500], Step [12/26], Loss: 3315.5767\n",
      "Epoch [52/500], Step [13/26], Loss: 841.5347\n",
      "Epoch [52/500], Step [14/26], Loss: 1909.8195\n",
      "Epoch [52/500], Step [15/26], Loss: 1454.6021\n",
      "Epoch [52/500], Step [16/26], Loss: 1151.8445\n",
      "Epoch [52/500], Step [17/26], Loss: 2004.5972\n",
      "Epoch [52/500], Step [18/26], Loss: 1810.0835\n",
      "Epoch [52/500], Step [19/26], Loss: 1189.1423\n",
      "Epoch [52/500], Step [20/26], Loss: 3854.8447\n",
      "Epoch [52/500], Step [21/26], Loss: 1735.8389\n",
      "Epoch [52/500], Step [22/26], Loss: 2296.1934\n",
      "Epoch [52/500], Step [23/26], Loss: 1333.8081\n",
      "Epoch [52/500], Step [24/26], Loss: 2220.6372\n",
      "Epoch [52/500], Step [25/26], Loss: 1851.2289\n",
      "Epoch [52/500], Step [26/26], Loss: 1823.1598\n",
      "Epoch [53/500], Step [1/26], Loss: 2347.7358\n",
      "Epoch [53/500], Step [2/26], Loss: 979.7751\n",
      "Epoch [53/500], Step [3/26], Loss: 2139.3262\n",
      "Epoch [53/500], Step [4/26], Loss: 2857.9395\n",
      "Epoch [53/500], Step [5/26], Loss: 1844.2695\n",
      "Epoch [53/500], Step [6/26], Loss: 2283.9185\n",
      "Epoch [53/500], Step [7/26], Loss: 2385.3647\n",
      "Epoch [53/500], Step [8/26], Loss: 2644.2634\n",
      "Epoch [53/500], Step [9/26], Loss: 1752.6387\n",
      "Epoch [53/500], Step [10/26], Loss: 1180.3118\n",
      "Epoch [53/500], Step [11/26], Loss: 971.0566\n",
      "Epoch [53/500], Step [12/26], Loss: 1986.7825\n",
      "Epoch [53/500], Step [13/26], Loss: 2141.0679\n",
      "Epoch [53/500], Step [14/26], Loss: 1332.9314\n",
      "Epoch [53/500], Step [15/26], Loss: 1578.5707\n",
      "Epoch [53/500], Step [16/26], Loss: 2420.4548\n",
      "Epoch [53/500], Step [17/26], Loss: 1890.6560\n",
      "Epoch [53/500], Step [18/26], Loss: 2157.1208\n",
      "Epoch [53/500], Step [19/26], Loss: 2890.8174\n",
      "Epoch [53/500], Step [20/26], Loss: 2083.1973\n",
      "Epoch [53/500], Step [21/26], Loss: 1389.1724\n",
      "Epoch [53/500], Step [22/26], Loss: 2184.2703\n",
      "Epoch [53/500], Step [23/26], Loss: 873.9072\n",
      "Epoch [53/500], Step [24/26], Loss: 3373.7244\n",
      "Epoch [53/500], Step [25/26], Loss: 2764.3630\n",
      "Epoch [53/500], Step [26/26], Loss: 4160.4819\n",
      "Epoch [54/500], Step [1/26], Loss: 1883.6454\n",
      "Epoch [54/500], Step [2/26], Loss: 2171.8931\n",
      "Epoch [54/500], Step [3/26], Loss: 3445.1143\n",
      "Epoch [54/500], Step [4/26], Loss: 4286.5498\n",
      "Epoch [54/500], Step [5/26], Loss: 1695.2699\n",
      "Epoch [54/500], Step [6/26], Loss: 1665.3049\n",
      "Epoch [54/500], Step [7/26], Loss: 2727.1453\n",
      "Epoch [54/500], Step [8/26], Loss: 876.8048\n",
      "Epoch [54/500], Step [9/26], Loss: 2279.7012\n",
      "Epoch [54/500], Step [10/26], Loss: 2894.0889\n",
      "Epoch [54/500], Step [11/26], Loss: 2089.8821\n",
      "Epoch [54/500], Step [12/26], Loss: 2564.2842\n",
      "Epoch [54/500], Step [13/26], Loss: 1852.6060\n",
      "Epoch [54/500], Step [14/26], Loss: 2813.0107\n",
      "Epoch [54/500], Step [15/26], Loss: 1625.5969\n",
      "Epoch [54/500], Step [16/26], Loss: 3527.9702\n",
      "Epoch [54/500], Step [17/26], Loss: 3471.8699\n",
      "Epoch [54/500], Step [18/26], Loss: 3065.6882\n",
      "Epoch [54/500], Step [19/26], Loss: 1129.6221\n",
      "Epoch [54/500], Step [20/26], Loss: 2894.1443\n",
      "Epoch [54/500], Step [21/26], Loss: 2348.1655\n",
      "Epoch [54/500], Step [22/26], Loss: 2958.3113\n",
      "Epoch [54/500], Step [23/26], Loss: 2793.2363\n",
      "Epoch [54/500], Step [24/26], Loss: 1538.5857\n",
      "Epoch [54/500], Step [25/26], Loss: 2277.4414\n",
      "Epoch [54/500], Step [26/26], Loss: 1950.4711\n",
      "Epoch [55/500], Step [1/26], Loss: 2000.0933\n",
      "Epoch [55/500], Step [2/26], Loss: 2447.3567\n",
      "Epoch [55/500], Step [3/26], Loss: 1091.2349\n",
      "Epoch [55/500], Step [4/26], Loss: 2007.9353\n",
      "Epoch [55/500], Step [5/26], Loss: 2894.3848\n",
      "Epoch [55/500], Step [6/26], Loss: 2035.7485\n",
      "Epoch [55/500], Step [7/26], Loss: 1129.0825\n",
      "Epoch [55/500], Step [8/26], Loss: 1839.1554\n",
      "Epoch [55/500], Step [9/26], Loss: 2425.3557\n",
      "Epoch [55/500], Step [10/26], Loss: 1088.7677\n",
      "Epoch [55/500], Step [11/26], Loss: 2161.2329\n",
      "Epoch [55/500], Step [12/26], Loss: 2523.5142\n",
      "Epoch [55/500], Step [13/26], Loss: 2812.0273\n",
      "Epoch [55/500], Step [14/26], Loss: 1613.7168\n",
      "Epoch [55/500], Step [15/26], Loss: 3088.7271\n",
      "Epoch [55/500], Step [16/26], Loss: 767.2050\n",
      "Epoch [55/500], Step [17/26], Loss: 2463.3262\n",
      "Epoch [55/500], Step [18/26], Loss: 1420.2834\n",
      "Epoch [55/500], Step [19/26], Loss: 4002.5911\n",
      "Epoch [55/500], Step [20/26], Loss: 1995.1008\n",
      "Epoch [55/500], Step [21/26], Loss: 2461.7354\n",
      "Epoch [55/500], Step [22/26], Loss: 1253.7290\n",
      "Epoch [55/500], Step [23/26], Loss: 1075.7120\n",
      "Epoch [55/500], Step [24/26], Loss: 1550.7212\n",
      "Epoch [55/500], Step [25/26], Loss: 2484.5142\n",
      "Epoch [55/500], Step [26/26], Loss: 1385.6235\n",
      "Epoch [56/500], Step [1/26], Loss: 6222.9326\n",
      "Epoch [56/500], Step [2/26], Loss: 3908.7144\n",
      "Epoch [56/500], Step [3/26], Loss: 2300.3647\n",
      "Epoch [56/500], Step [4/26], Loss: 2175.3193\n",
      "Epoch [56/500], Step [5/26], Loss: 4562.8696\n",
      "Epoch [56/500], Step [6/26], Loss: 4203.3945\n",
      "Epoch [56/500], Step [7/26], Loss: 1554.8154\n",
      "Epoch [56/500], Step [8/26], Loss: 2715.0957\n",
      "Epoch [56/500], Step [9/26], Loss: 1687.7009\n",
      "Epoch [56/500], Step [10/26], Loss: 1763.7134\n",
      "Epoch [56/500], Step [11/26], Loss: 1971.3772\n",
      "Epoch [56/500], Step [12/26], Loss: 3810.3020\n",
      "Epoch [56/500], Step [13/26], Loss: 4803.7764\n",
      "Epoch [56/500], Step [14/26], Loss: 1316.0486\n",
      "Epoch [56/500], Step [15/26], Loss: 2993.5664\n",
      "Epoch [56/500], Step [16/26], Loss: 2104.0142\n",
      "Epoch [56/500], Step [17/26], Loss: 2218.3376\n",
      "Epoch [56/500], Step [18/26], Loss: 1437.7271\n",
      "Epoch [56/500], Step [19/26], Loss: 1341.6700\n",
      "Epoch [56/500], Step [20/26], Loss: 2188.6577\n",
      "Epoch [56/500], Step [21/26], Loss: 2101.7827\n",
      "Epoch [56/500], Step [22/26], Loss: 1864.6357\n",
      "Epoch [56/500], Step [23/26], Loss: 3198.1038\n",
      "Epoch [56/500], Step [24/26], Loss: 1324.6157\n",
      "Epoch [56/500], Step [25/26], Loss: 931.3400\n",
      "Epoch [56/500], Step [26/26], Loss: 2071.7981\n",
      "Epoch [57/500], Step [1/26], Loss: 1283.4966\n",
      "Epoch [57/500], Step [2/26], Loss: 1679.4186\n",
      "Epoch [57/500], Step [3/26], Loss: 1505.3553\n",
      "Epoch [57/500], Step [4/26], Loss: 3263.6538\n",
      "Epoch [57/500], Step [5/26], Loss: 1248.5852\n",
      "Epoch [57/500], Step [6/26], Loss: 2528.6831\n",
      "Epoch [57/500], Step [7/26], Loss: 2784.7568\n",
      "Epoch [57/500], Step [8/26], Loss: 825.8253\n",
      "Epoch [57/500], Step [9/26], Loss: 1961.0354\n",
      "Epoch [57/500], Step [10/26], Loss: 1440.0007\n",
      "Epoch [57/500], Step [11/26], Loss: 1467.9900\n",
      "Epoch [57/500], Step [12/26], Loss: 2858.6182\n",
      "Epoch [57/500], Step [13/26], Loss: 1824.3582\n",
      "Epoch [57/500], Step [14/26], Loss: 2565.2559\n",
      "Epoch [57/500], Step [15/26], Loss: 1667.2944\n",
      "Epoch [57/500], Step [16/26], Loss: 1278.5273\n",
      "Epoch [57/500], Step [17/26], Loss: 1155.6506\n",
      "Epoch [57/500], Step [18/26], Loss: 420.7800\n",
      "Epoch [57/500], Step [19/26], Loss: 1175.7552\n",
      "Epoch [57/500], Step [20/26], Loss: 1820.1331\n",
      "Epoch [57/500], Step [21/26], Loss: 2096.6101\n",
      "Epoch [57/500], Step [22/26], Loss: 1820.7036\n",
      "Epoch [57/500], Step [23/26], Loss: 1549.2577\n",
      "Epoch [57/500], Step [24/26], Loss: 2495.0596\n",
      "Epoch [57/500], Step [25/26], Loss: 3548.3396\n",
      "Epoch [57/500], Step [26/26], Loss: 1364.5103\n",
      "Epoch [58/500], Step [1/26], Loss: 2399.9067\n",
      "Epoch [58/500], Step [2/26], Loss: 2314.3560\n",
      "Epoch [58/500], Step [3/26], Loss: 1772.6943\n",
      "Epoch [58/500], Step [4/26], Loss: 2315.0967\n",
      "Epoch [58/500], Step [5/26], Loss: 989.7778\n",
      "Epoch [58/500], Step [6/26], Loss: 2339.8752\n",
      "Epoch [58/500], Step [7/26], Loss: 1720.7289\n",
      "Epoch [58/500], Step [8/26], Loss: 876.7584\n",
      "Epoch [58/500], Step [9/26], Loss: 2043.4347\n",
      "Epoch [58/500], Step [10/26], Loss: 1337.7228\n",
      "Epoch [58/500], Step [11/26], Loss: 1603.2108\n",
      "Epoch [58/500], Step [12/26], Loss: 1808.7651\n",
      "Epoch [58/500], Step [13/26], Loss: 2345.4487\n",
      "Epoch [58/500], Step [14/26], Loss: 1427.7014\n",
      "Epoch [58/500], Step [15/26], Loss: 4953.7061\n",
      "Epoch [58/500], Step [16/26], Loss: 1698.9026\n",
      "Epoch [58/500], Step [17/26], Loss: 811.6556\n",
      "Epoch [58/500], Step [18/26], Loss: 1124.7330\n",
      "Epoch [58/500], Step [19/26], Loss: 3101.9368\n",
      "Epoch [58/500], Step [20/26], Loss: 885.5137\n",
      "Epoch [58/500], Step [21/26], Loss: 2063.7405\n",
      "Epoch [58/500], Step [22/26], Loss: 1029.2905\n",
      "Epoch [58/500], Step [23/26], Loss: 1995.5818\n",
      "Epoch [58/500], Step [24/26], Loss: 1148.1240\n",
      "Epoch [58/500], Step [25/26], Loss: 2098.5820\n",
      "Epoch [58/500], Step [26/26], Loss: 1239.0356\n",
      "Epoch [59/500], Step [1/26], Loss: 1812.7819\n",
      "Epoch [59/500], Step [2/26], Loss: 879.4283\n",
      "Epoch [59/500], Step [3/26], Loss: 2517.7429\n",
      "Epoch [59/500], Step [4/26], Loss: 1490.5439\n",
      "Epoch [59/500], Step [5/26], Loss: 1712.6147\n",
      "Epoch [59/500], Step [6/26], Loss: 1892.2356\n",
      "Epoch [59/500], Step [7/26], Loss: 984.8346\n",
      "Epoch [59/500], Step [8/26], Loss: 1495.3052\n",
      "Epoch [59/500], Step [9/26], Loss: 2745.2656\n",
      "Epoch [59/500], Step [10/26], Loss: 1108.6405\n",
      "Epoch [59/500], Step [11/26], Loss: 2067.6140\n",
      "Epoch [59/500], Step [12/26], Loss: 3015.2217\n",
      "Epoch [59/500], Step [13/26], Loss: 1518.7711\n",
      "Epoch [59/500], Step [14/26], Loss: 1966.4910\n",
      "Epoch [59/500], Step [15/26], Loss: 1716.8114\n",
      "Epoch [59/500], Step [16/26], Loss: 2033.1808\n",
      "Epoch [59/500], Step [17/26], Loss: 2639.3064\n",
      "Epoch [59/500], Step [18/26], Loss: 1272.9392\n",
      "Epoch [59/500], Step [19/26], Loss: 3031.4609\n",
      "Epoch [59/500], Step [20/26], Loss: 1819.7908\n",
      "Epoch [59/500], Step [21/26], Loss: 700.8721\n",
      "Epoch [59/500], Step [22/26], Loss: 1499.0072\n",
      "Epoch [59/500], Step [23/26], Loss: 1183.0770\n",
      "Epoch [59/500], Step [24/26], Loss: 1016.5854\n",
      "Epoch [59/500], Step [25/26], Loss: 993.4312\n",
      "Epoch [59/500], Step [26/26], Loss: 978.1843\n",
      "Epoch [60/500], Step [1/26], Loss: 1784.8999\n",
      "Epoch [60/500], Step [2/26], Loss: 1571.3347\n",
      "Epoch [60/500], Step [3/26], Loss: 1728.6917\n",
      "Epoch [60/500], Step [4/26], Loss: 1668.1162\n",
      "Epoch [60/500], Step [5/26], Loss: 1060.5635\n",
      "Epoch [60/500], Step [6/26], Loss: 2182.2280\n",
      "Epoch [60/500], Step [7/26], Loss: 568.8699\n",
      "Epoch [60/500], Step [8/26], Loss: 1397.7682\n",
      "Epoch [60/500], Step [9/26], Loss: 2519.5005\n",
      "Epoch [60/500], Step [10/26], Loss: 2167.7053\n",
      "Epoch [60/500], Step [11/26], Loss: 1443.8003\n",
      "Epoch [60/500], Step [12/26], Loss: 1529.0356\n",
      "Epoch [60/500], Step [13/26], Loss: 1475.7892\n",
      "Epoch [60/500], Step [14/26], Loss: 1102.5822\n",
      "Epoch [60/500], Step [15/26], Loss: 1125.2200\n",
      "Epoch [60/500], Step [16/26], Loss: 2235.6409\n",
      "Epoch [60/500], Step [17/26], Loss: 1506.9003\n",
      "Epoch [60/500], Step [18/26], Loss: 1639.0409\n",
      "Epoch [60/500], Step [19/26], Loss: 3357.6931\n",
      "Epoch [60/500], Step [20/26], Loss: 2519.4167\n",
      "Epoch [60/500], Step [21/26], Loss: 1075.7627\n",
      "Epoch [60/500], Step [22/26], Loss: 1734.4780\n",
      "Epoch [60/500], Step [23/26], Loss: 1544.0723\n",
      "Epoch [60/500], Step [24/26], Loss: 980.7990\n",
      "Epoch [60/500], Step [25/26], Loss: 1642.8706\n",
      "Epoch [60/500], Step [26/26], Loss: 436.5822\n",
      "Epoch [61/500], Step [1/26], Loss: 1518.7369\n",
      "Epoch [61/500], Step [2/26], Loss: 3024.5571\n",
      "Epoch [61/500], Step [3/26], Loss: 1315.1610\n",
      "Epoch [61/500], Step [4/26], Loss: 2220.6389\n",
      "Epoch [61/500], Step [5/26], Loss: 2160.6211\n",
      "Epoch [61/500], Step [6/26], Loss: 3154.2063\n",
      "Epoch [61/500], Step [7/26], Loss: 1440.4423\n",
      "Epoch [61/500], Step [8/26], Loss: 1858.9730\n",
      "Epoch [61/500], Step [9/26], Loss: 1288.0651\n",
      "Epoch [61/500], Step [10/26], Loss: 760.1115\n",
      "Epoch [61/500], Step [11/26], Loss: 1060.2966\n",
      "Epoch [61/500], Step [12/26], Loss: 2152.5569\n",
      "Epoch [61/500], Step [13/26], Loss: 641.3555\n",
      "Epoch [61/500], Step [14/26], Loss: 2011.1666\n",
      "Epoch [61/500], Step [15/26], Loss: 580.6008\n",
      "Epoch [61/500], Step [16/26], Loss: 900.4796\n",
      "Epoch [61/500], Step [17/26], Loss: 1084.1903\n",
      "Epoch [61/500], Step [18/26], Loss: 2251.2822\n",
      "Epoch [61/500], Step [19/26], Loss: 768.8954\n",
      "Epoch [61/500], Step [20/26], Loss: 3395.1118\n",
      "Epoch [61/500], Step [21/26], Loss: 1583.2162\n",
      "Epoch [61/500], Step [22/26], Loss: 811.9811\n",
      "Epoch [61/500], Step [23/26], Loss: 1781.5032\n",
      "Epoch [61/500], Step [24/26], Loss: 1594.5950\n",
      "Epoch [61/500], Step [25/26], Loss: 1027.0718\n",
      "Epoch [61/500], Step [26/26], Loss: 528.7610\n",
      "Epoch [62/500], Step [1/26], Loss: 618.5245\n",
      "Epoch [62/500], Step [2/26], Loss: 1209.2627\n",
      "Epoch [62/500], Step [3/26], Loss: 1321.5436\n",
      "Epoch [62/500], Step [4/26], Loss: 1708.0347\n",
      "Epoch [62/500], Step [5/26], Loss: 2550.9597\n",
      "Epoch [62/500], Step [6/26], Loss: 1745.1157\n",
      "Epoch [62/500], Step [7/26], Loss: 2415.5220\n",
      "Epoch [62/500], Step [8/26], Loss: 1088.7849\n",
      "Epoch [62/500], Step [9/26], Loss: 1599.3986\n",
      "Epoch [62/500], Step [10/26], Loss: 2691.5239\n",
      "Epoch [62/500], Step [11/26], Loss: 2055.7854\n",
      "Epoch [62/500], Step [12/26], Loss: 1259.2034\n",
      "Epoch [62/500], Step [13/26], Loss: 2037.1510\n",
      "Epoch [62/500], Step [14/26], Loss: 938.0659\n",
      "Epoch [62/500], Step [15/26], Loss: 2005.4358\n",
      "Epoch [62/500], Step [16/26], Loss: 825.1417\n",
      "Epoch [62/500], Step [17/26], Loss: 2572.3721\n",
      "Epoch [62/500], Step [18/26], Loss: 678.5303\n",
      "Epoch [62/500], Step [19/26], Loss: 2484.9644\n",
      "Epoch [62/500], Step [20/26], Loss: 1787.8655\n",
      "Epoch [62/500], Step [21/26], Loss: 2352.6287\n",
      "Epoch [62/500], Step [22/26], Loss: 523.2753\n",
      "Epoch [62/500], Step [23/26], Loss: 834.4023\n",
      "Epoch [62/500], Step [24/26], Loss: 2054.4272\n",
      "Epoch [62/500], Step [25/26], Loss: 896.8002\n",
      "Epoch [62/500], Step [26/26], Loss: 733.5795\n",
      "Epoch [63/500], Step [1/26], Loss: 1724.0776\n",
      "Epoch [63/500], Step [2/26], Loss: 834.0063\n",
      "Epoch [63/500], Step [3/26], Loss: 1337.2551\n",
      "Epoch [63/500], Step [4/26], Loss: 1067.7969\n",
      "Epoch [63/500], Step [5/26], Loss: 1814.6401\n",
      "Epoch [63/500], Step [6/26], Loss: 1585.0876\n",
      "Epoch [63/500], Step [7/26], Loss: 941.3851\n",
      "Epoch [63/500], Step [8/26], Loss: 1354.3893\n",
      "Epoch [63/500], Step [9/26], Loss: 1697.5305\n",
      "Epoch [63/500], Step [10/26], Loss: 2269.3555\n",
      "Epoch [63/500], Step [11/26], Loss: 2212.4971\n",
      "Epoch [63/500], Step [12/26], Loss: 533.9196\n",
      "Epoch [63/500], Step [13/26], Loss: 3580.3169\n",
      "Epoch [63/500], Step [14/26], Loss: 2596.3201\n",
      "Epoch [63/500], Step [15/26], Loss: 2117.5933\n",
      "Epoch [63/500], Step [16/26], Loss: 1477.3328\n",
      "Epoch [63/500], Step [17/26], Loss: 1319.6133\n",
      "Epoch [63/500], Step [18/26], Loss: 1118.0178\n",
      "Epoch [63/500], Step [19/26], Loss: 1877.8032\n",
      "Epoch [63/500], Step [20/26], Loss: 1709.4097\n",
      "Epoch [63/500], Step [21/26], Loss: 2117.3032\n",
      "Epoch [63/500], Step [22/26], Loss: 1281.9116\n",
      "Epoch [63/500], Step [23/26], Loss: 1144.3881\n",
      "Epoch [63/500], Step [24/26], Loss: 1068.9639\n",
      "Epoch [63/500], Step [25/26], Loss: 2508.8840\n",
      "Epoch [63/500], Step [26/26], Loss: 2253.7263\n",
      "Epoch [64/500], Step [1/26], Loss: 638.3447\n",
      "Epoch [64/500], Step [2/26], Loss: 2063.1401\n",
      "Epoch [64/500], Step [3/26], Loss: 1775.9102\n",
      "Epoch [64/500], Step [4/26], Loss: 907.4010\n",
      "Epoch [64/500], Step [5/26], Loss: 1287.4900\n",
      "Epoch [64/500], Step [6/26], Loss: 2047.8518\n",
      "Epoch [64/500], Step [7/26], Loss: 1564.3658\n",
      "Epoch [64/500], Step [8/26], Loss: 616.5556\n",
      "Epoch [64/500], Step [9/26], Loss: 828.1146\n",
      "Epoch [64/500], Step [10/26], Loss: 2872.6787\n",
      "Epoch [64/500], Step [11/26], Loss: 1885.4424\n",
      "Epoch [64/500], Step [12/26], Loss: 699.2201\n",
      "Epoch [64/500], Step [13/26], Loss: 2169.3926\n",
      "Epoch [64/500], Step [14/26], Loss: 2122.2051\n",
      "Epoch [64/500], Step [15/26], Loss: 1508.8953\n",
      "Epoch [64/500], Step [16/26], Loss: 1647.5088\n",
      "Epoch [64/500], Step [17/26], Loss: 1453.2133\n",
      "Epoch [64/500], Step [18/26], Loss: 2438.4551\n",
      "Epoch [64/500], Step [19/26], Loss: 1529.4561\n",
      "Epoch [64/500], Step [20/26], Loss: 719.7121\n",
      "Epoch [64/500], Step [21/26], Loss: 1526.3926\n",
      "Epoch [64/500], Step [22/26], Loss: 1638.1855\n",
      "Epoch [64/500], Step [23/26], Loss: 751.4448\n",
      "Epoch [64/500], Step [24/26], Loss: 986.5749\n",
      "Epoch [64/500], Step [25/26], Loss: 1195.7389\n",
      "Epoch [64/500], Step [26/26], Loss: 9366.5547\n",
      "Epoch [65/500], Step [1/26], Loss: 571.1664\n",
      "Epoch [65/500], Step [2/26], Loss: 562.9227\n",
      "Epoch [65/500], Step [3/26], Loss: 1110.7853\n",
      "Epoch [65/500], Step [4/26], Loss: 3011.8359\n",
      "Epoch [65/500], Step [5/26], Loss: 1626.9054\n",
      "Epoch [65/500], Step [6/26], Loss: 892.4214\n",
      "Epoch [65/500], Step [7/26], Loss: 2220.3647\n",
      "Epoch [65/500], Step [8/26], Loss: 1634.3932\n",
      "Epoch [65/500], Step [9/26], Loss: 1253.2317\n",
      "Epoch [65/500], Step [10/26], Loss: 1880.5492\n",
      "Epoch [65/500], Step [11/26], Loss: 2048.1145\n",
      "Epoch [65/500], Step [12/26], Loss: 1577.6440\n",
      "Epoch [65/500], Step [13/26], Loss: 2296.5344\n",
      "Epoch [65/500], Step [14/26], Loss: 1809.7800\n",
      "Epoch [65/500], Step [15/26], Loss: 3071.8345\n",
      "Epoch [65/500], Step [16/26], Loss: 1392.8477\n",
      "Epoch [65/500], Step [17/26], Loss: 2292.0671\n",
      "Epoch [65/500], Step [18/26], Loss: 1579.5220\n",
      "Epoch [65/500], Step [19/26], Loss: 741.6104\n",
      "Epoch [65/500], Step [20/26], Loss: 2295.3103\n",
      "Epoch [65/500], Step [21/26], Loss: 2071.9373\n",
      "Epoch [65/500], Step [22/26], Loss: 822.3510\n",
      "Epoch [65/500], Step [23/26], Loss: 1880.0214\n",
      "Epoch [65/500], Step [24/26], Loss: 1052.8425\n",
      "Epoch [65/500], Step [25/26], Loss: 2313.1514\n",
      "Epoch [65/500], Step [26/26], Loss: 822.9022\n",
      "Epoch [66/500], Step [1/26], Loss: 986.9148\n",
      "Epoch [66/500], Step [2/26], Loss: 884.5310\n",
      "Epoch [66/500], Step [3/26], Loss: 1782.3938\n",
      "Epoch [66/500], Step [4/26], Loss: 1121.3621\n",
      "Epoch [66/500], Step [5/26], Loss: 843.6001\n",
      "Epoch [66/500], Step [6/26], Loss: 653.0951\n",
      "Epoch [66/500], Step [7/26], Loss: 1960.2352\n",
      "Epoch [66/500], Step [8/26], Loss: 1133.4009\n",
      "Epoch [66/500], Step [9/26], Loss: 2925.2935\n",
      "Epoch [66/500], Step [10/26], Loss: 2280.9712\n",
      "Epoch [66/500], Step [11/26], Loss: 2117.3506\n",
      "Epoch [66/500], Step [12/26], Loss: 1586.4622\n",
      "Epoch [66/500], Step [13/26], Loss: 1495.3013\n",
      "Epoch [66/500], Step [14/26], Loss: 1201.0265\n",
      "Epoch [66/500], Step [15/26], Loss: 1327.3530\n",
      "Epoch [66/500], Step [16/26], Loss: 1108.3059\n",
      "Epoch [66/500], Step [17/26], Loss: 1394.9393\n",
      "Epoch [66/500], Step [18/26], Loss: 997.2506\n",
      "Epoch [66/500], Step [19/26], Loss: 1850.7920\n",
      "Epoch [66/500], Step [20/26], Loss: 842.7076\n",
      "Epoch [66/500], Step [21/26], Loss: 1585.5986\n",
      "Epoch [66/500], Step [22/26], Loss: 1334.4731\n",
      "Epoch [66/500], Step [23/26], Loss: 2141.5823\n",
      "Epoch [66/500], Step [24/26], Loss: 1154.2413\n",
      "Epoch [66/500], Step [25/26], Loss: 2919.5542\n",
      "Epoch [66/500], Step [26/26], Loss: 756.0375\n",
      "Epoch [67/500], Step [1/26], Loss: 1492.6659\n",
      "Epoch [67/500], Step [2/26], Loss: 2084.0361\n",
      "Epoch [67/500], Step [3/26], Loss: 899.3511\n",
      "Epoch [67/500], Step [4/26], Loss: 904.6573\n",
      "Epoch [67/500], Step [5/26], Loss: 1084.4275\n",
      "Epoch [67/500], Step [6/26], Loss: 1044.0554\n",
      "Epoch [67/500], Step [7/26], Loss: 3130.5732\n",
      "Epoch [67/500], Step [8/26], Loss: 1352.8931\n",
      "Epoch [67/500], Step [9/26], Loss: 345.9218\n",
      "Epoch [67/500], Step [10/26], Loss: 1243.3762\n",
      "Epoch [67/500], Step [11/26], Loss: 1697.6331\n",
      "Epoch [67/500], Step [12/26], Loss: 692.1808\n",
      "Epoch [67/500], Step [13/26], Loss: 1142.1278\n",
      "Epoch [67/500], Step [14/26], Loss: 954.3176\n",
      "Epoch [67/500], Step [15/26], Loss: 1536.8281\n",
      "Epoch [67/500], Step [16/26], Loss: 1596.4596\n",
      "Epoch [67/500], Step [17/26], Loss: 930.8519\n",
      "Epoch [67/500], Step [18/26], Loss: 1473.5186\n",
      "Epoch [67/500], Step [19/26], Loss: 3560.5415\n",
      "Epoch [67/500], Step [20/26], Loss: 1604.2485\n",
      "Epoch [67/500], Step [21/26], Loss: 919.7726\n",
      "Epoch [67/500], Step [22/26], Loss: 1492.0400\n",
      "Epoch [67/500], Step [23/26], Loss: 1228.5339\n",
      "Epoch [67/500], Step [24/26], Loss: 1056.6709\n",
      "Epoch [67/500], Step [25/26], Loss: 1490.4237\n",
      "Epoch [67/500], Step [26/26], Loss: 4604.6816\n",
      "Epoch [68/500], Step [1/26], Loss: 2264.1558\n",
      "Epoch [68/500], Step [2/26], Loss: 1234.2019\n",
      "Epoch [68/500], Step [3/26], Loss: 1325.3918\n",
      "Epoch [68/500], Step [4/26], Loss: 2085.0225\n",
      "Epoch [68/500], Step [5/26], Loss: 754.1718\n",
      "Epoch [68/500], Step [6/26], Loss: 1690.4749\n",
      "Epoch [68/500], Step [7/26], Loss: 993.5773\n",
      "Epoch [68/500], Step [8/26], Loss: 492.6389\n",
      "Epoch [68/500], Step [9/26], Loss: 923.1880\n",
      "Epoch [68/500], Step [10/26], Loss: 1420.3784\n",
      "Epoch [68/500], Step [11/26], Loss: 1549.9846\n",
      "Epoch [68/500], Step [12/26], Loss: 789.5812\n",
      "Epoch [68/500], Step [13/26], Loss: 2138.8174\n",
      "Epoch [68/500], Step [14/26], Loss: 945.4421\n",
      "Epoch [68/500], Step [15/26], Loss: 1788.4111\n",
      "Epoch [68/500], Step [16/26], Loss: 2005.0386\n",
      "Epoch [68/500], Step [17/26], Loss: 575.5268\n",
      "Epoch [68/500], Step [18/26], Loss: 555.3574\n",
      "Epoch [68/500], Step [19/26], Loss: 1380.5959\n",
      "Epoch [68/500], Step [20/26], Loss: 1389.5311\n",
      "Epoch [68/500], Step [21/26], Loss: 1552.0361\n",
      "Epoch [68/500], Step [22/26], Loss: 2477.7285\n",
      "Epoch [68/500], Step [23/26], Loss: 2570.0432\n",
      "Epoch [68/500], Step [24/26], Loss: 1299.1472\n",
      "Epoch [68/500], Step [25/26], Loss: 1343.3083\n",
      "Epoch [68/500], Step [26/26], Loss: 1858.6702\n",
      "Epoch [69/500], Step [1/26], Loss: 2542.2197\n",
      "Epoch [69/500], Step [2/26], Loss: 1004.3931\n",
      "Epoch [69/500], Step [3/26], Loss: 813.1208\n",
      "Epoch [69/500], Step [4/26], Loss: 1175.8831\n",
      "Epoch [69/500], Step [5/26], Loss: 1176.6477\n",
      "Epoch [69/500], Step [6/26], Loss: 1092.5441\n",
      "Epoch [69/500], Step [7/26], Loss: 2277.2007\n",
      "Epoch [69/500], Step [8/26], Loss: 900.6946\n",
      "Epoch [69/500], Step [9/26], Loss: 1454.6504\n",
      "Epoch [69/500], Step [10/26], Loss: 2353.9863\n",
      "Epoch [69/500], Step [11/26], Loss: 562.0577\n",
      "Epoch [69/500], Step [12/26], Loss: 2491.5515\n",
      "Epoch [69/500], Step [13/26], Loss: 993.8542\n",
      "Epoch [69/500], Step [14/26], Loss: 754.9266\n",
      "Epoch [69/500], Step [15/26], Loss: 567.1469\n",
      "Epoch [69/500], Step [16/26], Loss: 1325.1074\n",
      "Epoch [69/500], Step [17/26], Loss: 517.2788\n",
      "Epoch [69/500], Step [18/26], Loss: 1441.1074\n",
      "Epoch [69/500], Step [19/26], Loss: 2239.5957\n",
      "Epoch [69/500], Step [20/26], Loss: 1930.9814\n",
      "Epoch [69/500], Step [21/26], Loss: 2080.0715\n",
      "Epoch [69/500], Step [22/26], Loss: 1449.6597\n",
      "Epoch [69/500], Step [23/26], Loss: 1359.3339\n",
      "Epoch [69/500], Step [24/26], Loss: 1932.4897\n",
      "Epoch [69/500], Step [25/26], Loss: 2277.2837\n",
      "Epoch [69/500], Step [26/26], Loss: 986.6681\n",
      "Epoch [70/500], Step [1/26], Loss: 2395.8442\n",
      "Epoch [70/500], Step [2/26], Loss: 497.3240\n",
      "Epoch [70/500], Step [3/26], Loss: 959.1536\n",
      "Epoch [70/500], Step [4/26], Loss: 1459.4482\n",
      "Epoch [70/500], Step [5/26], Loss: 869.8388\n",
      "Epoch [70/500], Step [6/26], Loss: 1400.9716\n",
      "Epoch [70/500], Step [7/26], Loss: 746.6001\n",
      "Epoch [70/500], Step [8/26], Loss: 1858.5383\n",
      "Epoch [70/500], Step [9/26], Loss: 1886.0808\n",
      "Epoch [70/500], Step [10/26], Loss: 1724.9475\n",
      "Epoch [70/500], Step [11/26], Loss: 750.4118\n",
      "Epoch [70/500], Step [12/26], Loss: 2864.8135\n",
      "Epoch [70/500], Step [13/26], Loss: 1524.7079\n",
      "Epoch [70/500], Step [14/26], Loss: 972.0495\n",
      "Epoch [70/500], Step [15/26], Loss: 644.2577\n",
      "Epoch [70/500], Step [16/26], Loss: 1703.9142\n",
      "Epoch [70/500], Step [17/26], Loss: 1170.2003\n",
      "Epoch [70/500], Step [18/26], Loss: 2087.8540\n",
      "Epoch [70/500], Step [19/26], Loss: 2594.1406\n",
      "Epoch [70/500], Step [20/26], Loss: 1244.2068\n",
      "Epoch [70/500], Step [21/26], Loss: 1642.0023\n",
      "Epoch [70/500], Step [22/26], Loss: 1487.7689\n",
      "Epoch [70/500], Step [23/26], Loss: 3028.4741\n",
      "Epoch [70/500], Step [24/26], Loss: 863.8215\n",
      "Epoch [70/500], Step [25/26], Loss: 843.4061\n",
      "Epoch [70/500], Step [26/26], Loss: 405.9661\n",
      "Epoch [71/500], Step [1/26], Loss: 1065.9514\n",
      "Epoch [71/500], Step [2/26], Loss: 971.3719\n",
      "Epoch [71/500], Step [3/26], Loss: 1226.2523\n",
      "Epoch [71/500], Step [4/26], Loss: 1561.5198\n",
      "Epoch [71/500], Step [5/26], Loss: 3424.8130\n",
      "Epoch [71/500], Step [6/26], Loss: 1017.2993\n",
      "Epoch [71/500], Step [7/26], Loss: 2369.9678\n",
      "Epoch [71/500], Step [8/26], Loss: 1431.2668\n",
      "Epoch [71/500], Step [9/26], Loss: 1061.7147\n",
      "Epoch [71/500], Step [10/26], Loss: 503.1641\n",
      "Epoch [71/500], Step [11/26], Loss: 794.5256\n",
      "Epoch [71/500], Step [12/26], Loss: 1369.7222\n",
      "Epoch [71/500], Step [13/26], Loss: 871.7734\n",
      "Epoch [71/500], Step [14/26], Loss: 1913.3218\n",
      "Epoch [71/500], Step [15/26], Loss: 1076.7178\n",
      "Epoch [71/500], Step [16/26], Loss: 568.5803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     78\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 80\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_steps\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m         \u001b[39m# if (i + 1) % 10 == 0:\u001b[39;00m\n\u001b[0;32m     83\u001b[0m         \u001b[39m#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}\")\u001b[39;00m\n\u001b[0;32m     85\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mcnn_model_500images.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 147 * 147, 256)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(256, 2) # predicting two variables (alfa x and betax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.pattern = r\"epsnx([\\d.-]+)_alfax([\\d.-]+)_betax([\\d.-]+)_epsny([\\d.-]+)_alfay([\\d.-]+)_betay([\\d.-]+)_epsnz([\\d.-]+)_alfaz([\\d.-]+)_betaz([\\d.-]+)\\.png\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_files[index]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "        matches = re.search(self.pattern, image_name)\n",
    "        variables = [float(matches.group(i)) for i in range(1, 10) if matches.group(i)]\n",
    "        alfa_x = variables[1] # alfax value\n",
    "        beta_x = variables[2] # betax value\n",
    "        return image, torch.tensor([alfa_x, beta_x]) # returns alfa x and betax as the labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "image_dir = \"train\"\n",
    "dataset = CustomDataset(image_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_steps = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # if (i + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_model_500images.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for prediction 1:\n",
      "Mean Absolute Error: 0.7092\n",
      "Mean Absolute Percentage Error: 51.98%\n",
      "Symmetric Mean Absolute Percentage Error: 41.23%\n",
      "Root Mean Square Error: 0.8632\n",
      "\n",
      "Metrics for prediction 2:\n",
      "Mean Absolute Error: 42.4921\n",
      "Mean Absolute Percentage Error: 19.54%\n",
      "Symmetric Mean Absolute Percentage Error: 17.13%\n",
      "Root Mean Square Error: 60.1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_dir):\n",
    "    model.eval()\n",
    "    test_dataset = CustomDataset(test_dir)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    total_mae = torch.zeros(2, device=device)   # Added device argument\n",
    "    total_mape = torch.zeros(2, device=device)  # Added device argument\n",
    "    total_smape = torch.zeros(2, device=device) # Added device argument\n",
    "    total_mse = torch.zeros(2, device=device)   # Added device argument\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            absolute_error = torch.abs(outputs - labels)  # Absolute error\n",
    "            total_mae += absolute_error.sum(dim=0)\n",
    "\n",
    "            non_zero_mask = torch.abs(labels) > 1e-8  # Only calculate percentage for non-zero labels\n",
    "            percentage_error = (absolute_error / torch.abs(labels)) * 100  # Percentage error\n",
    "            total_mape += (percentage_error * non_zero_mask).sum(dim=0)\n",
    "\n",
    "            smape = 200.0 * torch.abs(outputs - labels) / (torch.abs(outputs) + torch.abs(labels) + torch.finfo(torch.float32).eps)\n",
    "            total_smape += smape.sum(dim=0)\n",
    "            \n",
    "            mse = (outputs - labels) ** 2  # Mean Square Error\n",
    "            total_mse += mse.sum(dim=0)\n",
    "\n",
    "            total_count += labels.size(0)  # number of samples\n",
    "\n",
    "    mae = total_mae / total_count\n",
    "    mape = total_mape / total_count\n",
    "    smape = total_smape / total_count\n",
    "    rmse = torch.sqrt(total_mse / total_count)\n",
    "\n",
    "    return mae.cpu().numpy(), mape.cpu().numpy(), smape.cpu().numpy(), rmse.cpu().numpy()  # Moved tensors to CPU and converted to numpy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"cnn_model_500images.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "mae, mape, smape, rmse = evaluate_model(model, \"test\")\n",
    "for i in range(2):\n",
    "    print(f\"Metrics for prediction {i+1}:\")\n",
    "    print(f\"Mean Absolute Error: {mae[i]:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape[i]:.2f}%\")\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error: {smape[i]:.2f}%\")\n",
    "    print(f\"Root Mean Square Error: {rmse[i]:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted variables: [0.011563804000616074, 281.4032897949219]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"cnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# using cv2 is gross but i like the transforms more\n",
    "image_path = \"test\\epsnx0.10_alfax-0.00_betax282.89_epsny0.10_alfay-0.55_betay170.00_epsnz5.00_alfaz0.10_betaz10.00.png\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = transforms.ToTensor()(image)\n",
    "image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_variables = output.squeeze().tolist()\n",
    "\n",
    "print(\"Predicted variables:\", predicted_variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
